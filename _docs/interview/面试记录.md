# **阿里国际一面：**

​	继上一次2020年秋招第一次面试......

8.13 14:00~14:40

电话面试，手机开免提，面试官反馈有嗡嗡的噪音，找了耳机戴上好多了

1、自我介绍（很重要，好好准备下！！！）

## 2、项目亮点、体现你能力的地方？（支支吾吾，说得很凌乱，得好好提前准备下！！！）

> 往构建分布式、高保障、高可用系统方面回答，缓存一致性、高可用保障等
>
> 有构建中大型平台的项目经验，熟悉分布式高并发场景下的常见问题和解决方案，限流、熔断降级、缓存一致性等。

## 3、简历上写的任务判断逻辑和业务逻辑解耦，任务分发逻辑和业务逻辑解耦，解释一下。

## 4、数据库和缓存怎么更新的？

> 解释了，实际业务场景中，T+1日更，且在凌晨业务流量比较低的情况下更新，在DAG任务先更新DB，删除缓存的任务作为后置任务。（Cache Aside，旁路缓存模式）

## 5、在更新数据库和缓存过程中，是否有流量打进来，数据库和缓存的一致性问题怎么保证？

> 解释了，实际业务场景中，T+1日更，且在凌晨业务流量比较低的情况下更新，在DAG任务先更新DB，删除缓存的任务作为后置任务。（Cache Aside，旁路缓存模式）删除缓存作为DAG中的一个节点，可以失败重试，重试达到一定次数之后，发送告警，必要的时候进行人工补偿。

## 6、实际项目中，删除缓存怎么实现的？

> 回答：在数据删除缓存数据任务中，通过jetcache的@CacheInvalidte注解
>
> 批量删除缓存方法：
>
> https://blog.csdn.net/applebomb/article/details/136985787
>
> https://github.com/alibaba/jetcache/issues/38（旧的方式，通过unwrap操作，后来新版本支持Cache API批量删除）
>
> https://github.com/alibaba/jetcache/issues/827（新版本，支持通过Cache API批量删除）
>
> ![image-20240824224711765](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240824224711765.png)
>
> ![image-20240824224803376](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240824224803376.png)
>
> ![image-20240824223343774](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240824223343774.png)

## 7、分级保障机制什么意思？

> 针对不同业务场景的重要性保障等级，不同的流量防护、缓存配置、熔断降级策略和告警形式。

## 8、鉴权怎么做的？限流是怎么做的？如果某个用户短时间突然有大量突发流量，该怎么办？

> 我理解其实就是问：**怎么做流量隔离，使得一个业务场景的流量异常，不影响系统中其他场景的服务?**
>
> 开始介绍鉴权是通过给不同业务场景分派app_id实现的，面试官理解成每个用户请求过来都带着app_id过来请求，
>
> 解释说用户不会直接请求我们的系统，我们提供比较底层的能力输出给app，对于用户的异常流量等行为，传导不到我们系统这一层，我们也不会处理，应该在app端层面去处理。
>
> https://developer.aliyun.com/article/1259616
>
> https://help.aliyun.com/zh/ahas/use-cases/use-msha-to-implement-business-traffic-isolation
>
> > 通义千问：
> >
> > 流量隔离是一种常见的技术手段，用于确保一个业务场景的流量异常不会影响到系统中其他场景的服务。实现流量隔离可以通过多种方法，包括但不限于资源隔离、服务隔离、请求隔离等。下面详细介绍几种实现流量隔离的技术方案：
> >
> > ### 1. 资源隔离
> >
> > 资源隔离是指在物理或逻辑上将不同业务场景所需的资源分开，确保一个业务场景的流量激增不会影响到其他业务场景。具体措施包括：
> >
> > - **容器化**：使用Docker容器或Kubernetes等容器技术，为每个业务场景分配独立的容器环境，确保资源隔离。
> > - **微服务架构**：将业务分解为独立的服务，每个服务拥有自己的资源，比如独立的数据库、缓存等。
> > - **资源配额**：为不同的业务场景设置资源配额，比如CPU、内存、磁盘空间等，确保超出配额时不会影响其他业务。
> >
> > ### 2. 服务隔离
> >
> > 服务隔离是指在逻辑上将不同的业务场景的服务隔离开来，确保一个服务出现问题时不会影响到其他服务。具体措施包括：
> >
> > - **微服务架构**：将不同的业务场景实现为独立的微服务，每个服务独立部署和运行。
> > - **API网关**：使用API网关作为统一的入口，它可以进行服务路由、限流、熔断等操作，确保异常服务不会影响到其他服务。
> > - **服务熔断**：当检测到某个服务出现异常时，自动断开与该服务的连接，防止故障扩散。
> >
> > ### 3. 请求隔离
> >
> > 请求隔离是指在处理请求的过程中进行隔离，确保一个业务场景的请求异常不会影响到其他业务场景。具体措施包括：
> >
> > - **请求限流**：对每个业务场景的请求进行限流，当请求量超过阈值时拒绝新的请求。
> > - **请求排队**：当请求量过大时，可以将请求放入队列中等待处理，避免对系统造成过大压力。
> > - **请求降级**：当系统负载过高时，可以主动降级部分服务，关闭非核心功能，确保核心服务的正常运行。
> >
> > ### 4. 线程池隔离
> >
> > 线程池隔离是指为不同的业务场景配置独立的线程池，确保一个业务场景的线程池满载不会影响到其他业务场景。具体措施包括：
> >
> > - **独立线程池**：为每个业务场景配置独立的线程池，这样即使某个线程池满了也不会影响其他线程池。
> > - **线程池监控**：监控线程池的状态，当线程池满载时采取相应措施，比如拒绝新的任务。
> >
> > ### 5. 数据库隔离
> >
> > 数据库隔离是指为不同的业务场景配置独立的数据库实例，确保一个业务场景的数据库负载不会影响到其他业务场景。具体措施包括：
> >
> > - **读写分离**：为每个业务场景配置独立的读写分离策略，确保读操作和写操作不会相互影响。
> > - **分库分表**：将数据分布在多个数据库和表中，减少单一数据库的压力。
> > - **数据库配额**：为不同的业务场景设置数据库连接数配额，确保超出配额时不会影响其他业务。
> >
> > ### 6. 监控与报警
> >
> > 监控与报警是指定期监控系统的各项指标，当发现异常时能够及时发出警报，采取相应的措施。具体措施包括：
> >
> > - **性能监控**：监控系统的CPU、内存、磁盘I/O等关键性能指标。
> > - **错误日志**：记录错误和异常信息，便于事后分析。
> > - **报警机制**：当监控到异常情况时，自动发送报警通知。
> >
> > ### 示例
> >
> > 假设你正在设计一个包含多个业务场景的电商系统，可以考虑以下策略：
> >
> > 1. **微服务架构**：将系统拆分为商品服务、订单服务、支付服务等多个微服务。
> > 2. **API网关**：作为前端和后端服务之间的入口，可以集中处理认证、限流、熔断等逻辑。
> > 3. **服务熔断**：为每个微服务实现熔断机制，当某个服务响应时间过长或失败率过高时，自动断开与该服务的连接。
> > 4. **请求限流**：在API网关层实现限流策略，防止系统过载。
> > 5. **独立线程池**：为每个微服务配置独立的线程池，确保一个服务的线程池满载不会影响其他服务。
> > 6. **读写分离**：为数据库配置读写分离，主数据库负责写操作，从数据库负责读操作。
> > 7. **监控与报警**：实时监控系统的关键性能指标，当发现异常时及时发出警报。
> >
> > ### 总结
> >
> > 通过上述方法和技术，可以有效地实现流量隔离，确保一个业务场景的流量异常不会影响到系统中其他场景的服务。需要注意的是，具体的实现细节和技术选型会根据项目需求和团队经验有所差异。
>
> 

## 9、不同的业务场景的重要性保障等级不同，其他业务场景突发业务流量压力或异常，怎么保证手机银行app场景下的应用的高可用不受影响？怎么做流量隔离的？实现多个场景相互不受影响？

> 回答：实际上将最高保障等级手机银行app场景单独拆分成了一个应用，其他场景一个应用，集群分开部署，直接从物理上进行隔离。面试官可能想问，都在一个应用中的话，怎么做流量隔离。
>
> 见第8点
>
> https://developer.aliyun.com/article/1259616
>
> https://help.aliyun.com/zh/ahas/use-cases/use-msha-to-implement-business-traffic-isolation

## 10、先删除缓存，再更新数据库和先更新数据库再删除缓存，有什么区别？

> 在缓存失效，读写并发的情况下都可能出现缓存和数据库不一致的情况出现。
>
> 先删缓存再更新数据库：A读请求，B写请求，执行时序：B删除缓存，A查缓存不存在查db查到旧数据，A将旧数据重建到缓存，B将新数据写入db。出现缓存旧数据，数据库新数据的不一致
>
> 先更新数据库再删除缓存：A读请求，B写请求，执行时序：A读缓存miss未命中查数据库查到旧数据，B将新数据更新至数据库，B删除缓存，A将从db读到的旧数据重建到缓存。出现缓存旧数据，数据库新数据的不一致。
>
> 写缓存比写数据库快很多，所以先更新数据库再删除缓存，出现不一致的概率很小，实际项目中采用先更新数据库，再删除缓存的Cache Aside旁路缓存策略。解决先删缓存再更新数据库的不一致问题，可以采用延时双删，即删缓存，更新db，再删缓存。

## 11、两者都可能出现不一致的问题，那种方式能最大程度保证db和Redis的一致吗？

> 说了三点，按123的顺序给面试官解释，说了比较多，最后面试官说最重要的就是第三点，直接说出来就行了。
>
> 1、缓存刚好失效；2、读写请求并发打过来；3、更新缓存比更新数据库要快很多；
>
> 具体过程：A读请求和B写请求同时打过来，时序上：①A查缓存不存在，查数据库；②B更新数据库；③B删除缓存；④A将数据库中查到的旧数据重建到缓存中。此时缓存中旧数据，数据库中新数据，db和cache不一致。
>
> 最重要的点：A重建缓存的写操作比B更新数据库的操作要快很多，发生上述情况的可能性很小，能在很大程度上保证一致性。

12、简历上招行独有、自主设计、行业领先的财富管理量化平台，每太看懂，能解释下吗？

> 主要体现在业务层面，结合招行实际一线零售业务场景，赋能总分支行一线客户经理

## 13、本地缓存都有哪些？

> 又想起一个问题hhhh，回答了Guava、Caffiene、LinkedHashMapCache（还有EhCache，这个知道有印象，但是没答出来）
>
> Guava、Caffiene、EhCache、
>
> https://www.cnblogs.com/imyjy/p/15695160.html



## [主流缓存框架调研 - Guava/Caffeine/EhCache/JetCache](https://www.cnblogs.com/imyjy/p/15695160.html)

> https://www.cnblogs.com/imyjy/p/15695160.html    



反问面试官：

> 1、阿里内部有使用jetcache框架吗，实际项目中使用多级缓存的场景怎么做的？
>
> ​	没有使用jetcache，基于阿里Tair，结合实际业务场景自己实现多级缓存的
>
> 2、实际项目中有使用延时双删吗？（问了这个问题反手问我，延时双删是什么。。。。）
>
> ​	具体问题，实际场景，具体分析。目前没用过，先更新缓存，再异步删除缓存。
>
> ​	一致性要求比较高的情况下，会加锁，更新完成之前，不允许读。
>
> 3、什么时候，怎么直到面试结果？
>
> ​	两三天电话打过来就继续二面三面，五天还没有打过来大概率挂了。



# 快手-Java开发工程师（杭州）-【支付中台】一面

8.14 15:00~16:10，手撕算法，字符串排列，有思路（回溯），代码写出来，测试案例没通过。。。。。

1、自我介绍；

问了很多关于项目背景、用途、业务层面的东西

## 怎么实现MySQL自动主从切换和故障转移的？什么原理

> MySQL MGR：
>
> https://blog.csdn.net/NICK_53/article/details/130332519
>
> ![image-20240825191758250](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825191758250.png)
>
> https://www.bilibili.com/video/BV1eV4y1D7fs/?spm_id_from=333.337.search-card.all.click&vd_source=d88527641a2c5d74436ce88743d2fd55
>
> https://www.bilibili.com/video/BV193411W7bF/?spm_id_from=333.788&vd_source=d88527641a2c5d74436ce88743d2fd55
>
> 
>
> ![image-20240825204458811](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204458811.png)
>
> ![image-20240825204521191](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204521191.png)
>
> ![image-20240825204621769](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204621769.png)
>
> ![image-20240825204425413](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204425413.png)
>
> ![image-20240825205549978](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825205549978.png)

## QPS多少？



## 说说项目中Sentinel的使用情况



## jetCache怎么保证缓存一致性的？

> 多级缓存一致性：更新或失效缓存的时候，基于Redis Pub/Sub，broadcastChannel和syncLocal，将当前节点缓存变更事件发布到指定频道，集群中的节点监听到事件触发本地jvm缓存失效，这样新的请求打过来就直接读redis新数据然后重建本地缓存，保证多级缓存的一致性。

## 怎么保证缓存击穿和缓存雪崩的？

> 缓存击穿：@CachePenetrationProtect
>
> @CachePenetrationProtect注解：
>
> 当缓存访问未命中的情况下，对并发进行的加载行为进行保护。
> 当前版本实现的是单JVM内的保护，即同一个JVM中同一个key只有一个线程去加载，其它线程等待结果。
>
> >- **CacheAside模式**的缓存由于本身有淘汰策略，**在数据失效后，缓存组件会直接访问数据库尝试重建缓存**。
> >- **在大规模分布式热点的情况下，一旦热点数据失效，会有大量的请求同时尝试重建缓存，这不但会导致资源浪费，更加危险的是会造成数据库瞬时极大的压力**。
> >- JetCache通过注解**@CachePenetrationProtect实现了JVM内存锁级的击穿保护**，**使并发重建的请求限制到可控范围**。( **如果数据利用率高还可以使用@CacheRefresh的方式来实现基于分布式锁的缓存重建能力** ，保证一个刷新周期内，只有一个节点重建缓存)
>
> 缓存雪崩：通过设置不同的缓存失效时间、@CacheRefresh主动刷新缓存，避免缓存集体失效。

## 2、用的JDK几？垃圾回收器用的什么？垃圾回收的原理？

> JDK8，
>
> 数据侧ETL使用：默认：年轻代-Parallel Scavenge垃圾回收器，老年代-Parallel Old垃圾回收器
>
> 数字化赋能侧：G1，面向服务端的垃圾回收器，追求吞吐量和停顿时间的平衡
>
> ## 5.年轻代-Parallel Scavenge垃圾回收器
>
> Parallel Scavenge是JDK8默认的年轻代垃圾回收器，专注于提升系统的**吞吐量**。它采用**多线程**并行回收的方式，旨在最大限度地提高应用程序在垃圾回收过程中的执行效率。Parallel Scavenge具备自动调整堆内存大小的特点，可以根据应用程序的需求动态调整内存分配，以获得更好的性能。Parallel Scavenge主要针对**年轻代**进行管理，采用**复制算法**进行垃圾回收。在多线程并行回收的机制下，Parallel Scavenge能够显著提高垃圾回收的效率，从而提高系统的吞吐量。
>
> ![image](https://ucc.alicdn.com/pic/developer-ecology/juazsikdwqz3w_6f6e0da597c04e35aae2918c5c2445ab.png)
>
> Parallel Scavenge的优点在于其**高吞吐量**以及手动可控的特点。**为了提高吞吐量，虚拟机会动态调整堆的参数，以满足应用程序的性能需求**。这种**自动调整内存大小的功能有助于提高系统的适应性和性能**。Parallel Scavenge也存在一些缺点，由于其**关注的是系统的吞吐量，而不是单次的停顿时间**，因此可能会在某些情况下导致较长的停顿时间。
>
> **适用场景：**Parallel Scavenge主要适用于后台任务，这些任务不需要与用户频繁交互，并且容易产生大量的对象。例如，**大数据的处理、大文件的导出**等任务就非常适合使用Parallel Scavenge垃圾回收器。在这些场景下，Parallel Scavenge能够通过**提高系统吞吐量来提高任务的执行效率**。
>
> Parallel Scavenge允许手动设置最大暂停时间和吞吐量。Oracle官方建议在使用这个组合时，不要设置堆内存的最大值，**垃圾回收器会根据最大暂停时间和吞吐量自动调整内存大小**。
>
> - 最大暂停时间：-XX:MaxGCPauseMillis=n设置每次垃圾回收时的最大停顿毫秒数。
> - 吞吐量：-XX:GCTimeRatio=n设置吞吐量为n（用户线程执行时间 = n/n + 1）。
> - 自动调整内存大小：-XX:+UseAdaptiveSizePolicy设置可以让垃圾回收器根据吞吐量和最大停顿的毫秒数自动调整内存大小。
>
> ```dockerfile
> -XX:MaxGCPauseMillis=n
> -XX:GCTimeRatio=n
> -XX:+UseAdaptiveSizePolicy
> ```
>
> ## 6.老年代-Parallel Old垃圾回收器
>
> Parallel Old垃圾回收器是为Parallel Scavenge收集器设计的老年代版本，专门用于管理**老年代**内存区域。它利用**多线程**并发收集技术，旨在提高老年代垃圾回收的效率和吞吐量。Parallel Old采用**标记-整理算法**对老年代进行管理。在多线程并发收集的机制下，Parallel Old能够充分利用多核CPU的计算能力，提高垃圾回收的效率。
>
> ![image](https://ucc.alicdn.com/pic/developer-ecology/juazsikdwqz3w_80302a1eeed64d07a5d48ccd3186060a.png)
>
> Parallel Old的优点在于其并发收集的特性，使得在多核CPU环境下表现出较高的效率。通过并发收集，Parallel Old能够降低垃圾回收对应用程序的影响，提高系统的吞吐量。Parallel Old也存在一些缺点，由于其采用标记-整理算法，垃圾回收过程中可能需要暂停用户线程，导致较长的停顿时间。此外，Parallel Old可能不太适合内存容量非常大的系统，因为在大内存环境下，垃圾回收的开销可能会增加。
>
> **适用场景：**Parallel Old通常与Parallel Scavenge收集器配套使用。通过使用**-XX:+UseParallelGC**或**-XX:+UseParallelOldGC**参数，可以启用Parallel Scavenge和Parallel Old的组合。**这种组合适用于后台任务和大数据处理的场景，其中应用程序可以容忍较长的停顿时间，并希望通过提高吞吐量来提高任务的执行效率**。通过使用Parallel Scavenge和Parallel Old的组合，应用程序可以**在多核CPU环境下获得更好的性能表现**。
>
> ```
> -XX:+UseParallelGC
> -XX:+UseParallelOldGC
> ```
>
> 
>
> 数据侧ETL：使用的JDK8默认的垃圾回收器，新生代Parallel Scavenge + 老年代Parallel Old
>
> 数字化赋能侧：G1

## 索引，联合索引的索引结构是什么样子的？非叶子节点数据怎么存储的

> https://segmentfault.com/a/1190000023617757
>
> 联合索引的索引结构与单个列的索引结构类似，都是 B+树结构。B+树的特点是：
>
> 1. 非叶子节点只存储索引列的值和指向子节点的指针，不存储实际的数据记录。
> 2. 叶子节点包含了索引列的值以及对应的数据记录的指针（在聚簇索引中是完整的数据记录，在非聚簇索引中是主键值或数据行的地址）。
>
> 对于联合索引，其叶子节点存储的是多个索引列的值以及对应的数据记录指针。例如，如果有一个联合索引是 `(col1, col2, col3)`，那么叶子节点中会按照 `col1`、`col2`、`col3` 的顺序依次存储这些列的值以及对应的数据记录指针。
>
> 非叶子节点则存储的是联合索引列的部分值（通常是索引列值的范围）以及指向子节点的指针，用于快速定位到对应的叶子节点。
>
> 通过联合索引，可以提高根据多个列进行查询时的查询效率。例如，对于查询条件中使用了联合索引的前导列（如 `WHERE col1 =? AND col2 =?` 中，`col1` 是前导列）的查询，能够利用到该联合索引进行优化。但需要注意的是，如果查询条件中没有使用联合索引的前导列，或者查询条件中列的顺序与联合索引的列顺序不一致（**最左匹配原则**），可能无法充分利用联合索引的优势。
>
> ![image-20240816093101225](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240816093101225.png)

## 有没有遇到慢查询？怎么优化？

> 加索引，应该先执行explain查看执行计划，分析什么原因，没加索引？还是索引失效？
>
> 其实可以写一个，由于索引前缀区分度不够，反转过来简历索引，进行优化
>
> https://blog.csdn.net/2401_85648342/article/details/139818951
>
> 慢查询是指在数据库中执行时间超过一定阈值的查询操作。这些查询操作消耗过多的系统资源，影响数据库性能，甚至导致用户体验下降。下面详细介绍慢查询的定义、识别方法、常见原因以及优化策略。
>
> ### 慢查询的定义
>
> 慢查询的具体定义因不同组织和场景而异，但一般而言，如果一个查询的执行时间超过预设的时间阈值，则认为该查询为慢查询。这个阈值通常是可配置的，并且默认值可能因数据库系统而异。例如，在 MySQL 中，可以通过 `long_query_time` 参数来配置慢查询的时间阈值，其默认值通常为 10 秒。
>
> ### 慢查询的识别
>
> 为了识别慢查询，数据库管理系统通常提供了慢查询日志功能。通过开启慢查询日志，可以记录执行时间超过设定阈值的所有查询。例如，在 MySQL 中，可以通过以下步骤开启慢查询日志：
>
> 1. **查看配置**:
>    - 使用 `SHOW VARIABLES LIKE '%slow%'` 查看慢查询相关配置。
>    - 使用 `SHOW VARIABLES LIKE 'long%'` 查看慢查询时间阈值的配置。
>
> 2. **开启慢查询日志**:
>    - 设置 `slow_query_log` 为 1 以开启慢查询日志。
>    - 设置 `long_query_time` 为希望的阈值（单位：秒）。
>    - 设置 `slow_query_log_file` 指定慢查询日志文件的路径。
>
> 3. **重启数据库服务**:
>    - 为了使更改生效，可能需要重启数据库服务。
>
> ### 慢查询的常见原因
>
> 慢查询的出现可能由以下多种因素造成：
>
> 1. **索引不当**:
>    - 缺少索引、索引选择不当或索引未得到充分利用都可能导致慢查询。
>    - 某些情况下，即使存在索引，如果使用不当（例如在 WHERE 子句中使用函数或不正确的比较操作），也可能导致查询无法利用索引。
>
> 2. **复杂的查询逻辑**:
>    - 使用 JOIN 操作连接多个表、使用子查询等都可能增加查询的复杂度和执行时间。
>
> 3. **数据量过大**:
>    - 当表中的数据量非常大时，查询性能可能受到影响。
>
> 4. **硬件资源不足**:
>    - CPU、内存、磁盘 I/O 等资源受限也可能导致查询性能下降。
>
> 5. **并发问题**:
>    - 高并发环境下的锁定和等待也可能导致查询变慢。
>
> ### 慢查询的优化策略
>
> 优化慢查询通常涉及以下几个方面：
>
> 1. **建立合适的索引**:
>    - 针对经常进行搜索、排序和分组操作的字段，建立合适的索引可以显著提高查询性能。
>    - 例如，使用 `CREATE INDEX` 语句创建索引。
>
> 2. **优化查询语句**:
>    - 使用 `EXPLAIN` 关键字模拟查询优化器执行查询 SQL，分析执行计划来找出瓶颈。
>    - 简化查询，减少不必要的子查询和连接。
>
> 3. **分析查询执行计划**:
>    - 使用 EXPLAIN 命令来分析查询语句的执行计划，找到查询语句的瓶颈所在。
>
> 4. **定期审查和重构慢查询**:
>    - 定期审查慢查询日志，分析和重构慢查询。
>
> 5. **考虑存储引擎和硬件优化**:
>    - 考虑使用不同的存储引擎（如 InnoDB 与 MyISAM）。
>    - 升级硬件资源（如增加内存、使用更快的磁盘驱动器）。
>
> 6. **使用存储过程**:
>    - 将复杂的查询逻辑封装在存储过程中，可以提高执行效率。
>
> 7. **避免使用 SELECT ***:
>    - 明确指定需要检索的列，避免使用 `SELECT *`，以减少不必要的数据传输。
>
> 8. **分页查询的优化**:
>    - 对分页查询进行优化，例如使用 LIMIT 子句结合 OFFSET。
>
> 9. **合理使用连接**:
>    - 优化 JOIN 操作，尽量减少连接的数量。
>
> 10. **子查询的优化**:
>     - 尽量避免使用嵌套的子查询，考虑使用 JOIN 代替。
>
> ### 总结
>
> 慢查询会对数据库性能产生负面影响，通过开启慢查询日志、分析慢查询日志中的信息，并采取相应的优化措施，可以有效地改善数据库性能。优化措施通常包括建立合适的索引、优化查询语句、分析执行计划等。通过这些方法，可以提高查询速度，减少资源消耗，并最终提升系统的整体性能。

## MySQL支持的隔离级别？默认隔离级别？

> 事务的隔离级别定了一个事务可能受其他事务影响的程度，MySQL 支持的四种隔离级别分别是：读未提交（Read  Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable  Read）和串行化（Serializable）。
>
> #### [什么是读未提交？](https://javabetter.cn/sidebar/sanfene/mysql.html#什么是读未提交)
>
> 读未提交是最低的隔离级别，在这个级别，当前事务可以读取未被其他事务提交的数据，以至于会出现“脏读”、“不可重复读”和“幻读”的问题。
>
> #### [什么是读已提交？](https://javabetter.cn/sidebar/sanfene/mysql.html#什么是读已提交)
>
> 当前事务只能读取已经被其他事务提交的数据，可以避免“脏读”现象。但不可重复读和幻读问题仍然存在。
>
> #### [什么是可重复读？](https://javabetter.cn/sidebar/sanfene/mysql.html#什么是可重复读)
>
> 确保在同一事务中多次读取相同记录的结果是一致的，即使其他事务对这条记录进行了修改，也不会影响到当前事务。
>
> 可重复读是 MySQL 默认的隔离级别，避免了“脏读”和“不可重复读”，但可能会出现幻读。
>
> #### [什么是串行化？](https://javabetter.cn/sidebar/sanfene/mysql.html#什么是串行化)
>
> 最高的隔离级别，通过强制事务串行执行来避免并发问题，可以解决“脏读”、“不可重复读”和“幻读”问题。
>
> 但会导致大量的超时和锁竞争问题。

## 可重复读怎么实现的

> https://xie.infoq.cn/article/803b1813d4141946ff141e528
>
> https://xiaolincoding.com/mysql/transaction/phantom.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%B9%BB%E8%AF%BB
>
> #### [读未提交是如何实现的？](https://javabetter.cn/sidebar/sanfene/mysql.html#读未提交是如何实现的)
>
> 不提供任何锁机制来保护读取的数据，允许读取未提交的数据（即脏读）。
>
> #### [读已提交&可重复读是如何实现的？](https://javabetter.cn/sidebar/sanfene/mysql.html#读已提交-可重复读是如何实现的)
>
> 读已提交和可重复读通过 MVCC 机制中的 ReadView 来实现。
>
> - READ COMMITTED：每次读取数据前都生成一个 ReadView，保证每次读操作都是最新的数据。
> - REPEATABLE READ：只在第一次读操作时生成一个 ReadView，后续读操作都使用这个 ReadView，保证事务内读取的数据是一致的。
>
> #### [串行化是如何实现的？](https://javabetter.cn/sidebar/sanfene/mysql.html#串行化是如何实现的)
>
> 事务在读操作时，必须先加表级共享锁，直到事务结束才释放；事务在写操作时，必须先加表级排他锁，直到事务结束才释放。
>
> **面试官**：事务中的隔离性是如何保证的呢？(你解释一下MVCC)
>
> **候选人**：事务的隔离性是由锁和mvcc实现的。
>
> 其中mvcc的意思是多版本并发控制。指维护一个数据的多个版本，使得读写操作没有冲突，它的底层实现主要是分为了三个部分，第一个是隐藏字段，第二个是undo log日志，第三个是readView读视图
>
> 隐藏字段是指：在mysql中给每个表都设置了隐藏字段，有一个是trx_id(事务id)，记录每一次操作的事务id，是自增的；另一个字段是roll_pointer(回滚指针)，指向上一个版本的事务版本记录地址
>
> undo log主要的作用是记录回滚日志，存储老版本数据，在内部会形成一个版本链，在多个事务并行操作某一行记录，记录不同事务修改数据的版本，通过roll_pointer指针形成一个链表
>
> readView解决的是一个事务查询选择版本的问题，在内部定义了一些匹配规则和当前的一些事务id判断该访问那个版本的数据，不同的隔离级别快照读是不一样的，最终的访问的结果不一样。如果是rc隔离级别，每一次执行快照读时生成ReadView，如果是rr隔离级别仅在事务中第一次执行快照读时生成ReadView，后续复用。

## 可重复读解决幻读问题了吗？

> https://xie.infoq.cn/article/803b1813d4141946ff141e528
>
> **https://xiaolincoding.com/mysql/transaction/phantom.html#%E5%BD%93%E5%89%8D%E8%AF%BB%E6%98%AF%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E7%9A%84**
>
> ## 6.可重复读解决幻读问题了吗？
>
> 首先说一下什么是幻读。当同一个查询在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题。例如，如果 SELECT 执行了两次，但第二次返回了第一次没有返回的行，则该行是“幻像”行。
>
> 可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，**后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的**，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。
>
> MySQL 里除了普通查询是快照读，其他都是**当前读**，比如 update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。
>
> **针对当前读是仍存在幻读问题的**，因为行锁并不影响其他事务的插入操作。因此 **Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了间隙锁**，通过 **记录锁+间隙锁形成 next-key lock（临键锁）**的方式解决了幻读。
>
> 但是可重复读隔离级别下仍然没彻底解决幻读问题，举两个例子：
>
> - 对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。
> - 对于当前读，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。
>
> 所以，**MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。**
>
> 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

## MVCC

>  MVCC 是多版本并发控制（Multi-Version Concurrency Control）的简称，主要用来解决数据库并发问题。
>
> 在支持 MVCC 的数据库中，当多个用户同时访问数据时，每个用户都可以看到一个在某一时间点之前的数据库快照，并且能够无阻塞地执行查询和修改操作，而不会相互干扰。
>
> 在传统的锁机制中，如果一个事务正在写数据，那么其他事务必须等待写事务完成才能读数据，MVCC 允许读操作访问数据的一个旧版本快照，同时写操作创建一个新的版本，这样读写操作就可以并行进行，不必等待对方完成。
>
> 在 MySQL 中，特别是 InnoDB 存储引擎，MVCC 是通过版本链和 ReadView 机制来实现的。
>
> #### [什么是版本链？](https://javabetter.cn/sidebar/sanfene/mysql.html#什么是版本链)
>
> 在 InnoDB 中，每一行数据都有两个隐藏的列：一个是 DB_TRX_ID，另一个是 DB_ROLL_PTR。
>
> - `DB_TRX_ID`，保存创建这个版本的事务 ID。
> - `DB_ROLL_PTR`，指向 undo 日志记录的指针，这个记录包含了该行的前一个版本的信息。通过这个指针，可以访问到该行数据的历史版本。
>
> ![img](https://cdn.tobebetterjavaer.com/stutymore/mysql-20240415084347.png)
>
> 假设有一张`hero`表，表中有一行记录 name 为张三，city 为帝都，插入这行记录的事务 id 是 80。此时，`DB_TRX_ID`的值就是 80，`DB_ROLL_PTR`的值就是指向这条 insert undo 日志的指针。
>
> ![三分恶面渣逆袭：表记录](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mysql-80ebc2b3-ae63-417d-9307-f6a7811f7965.jpg)三分恶面渣逆袭：表记录
>
> 接下来，如果有两个`DB_TRX_ID`分别为`100`、`200`的事务对这条记录进行了`update`操作，那么这条记录的版本链就会变成下面这样：
>
> ![三分恶面渣逆袭：update 操作](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mysql-bf4ff00d-01bd-4170-a17b-6919f7873ea4.jpg)三分恶面渣逆袭：update 操作
>
> 当事务更新一行数据时，InnoDB 不会直接覆盖原有数据，而是创建一个新的数据版本，并更新 DB_TRX_ID 和 DB_ROLL_PTR，使得它们指向前一个版本和相关的 undo 日志。这样，老版本的数据不会丢失，可以通过版本链找到。
>
> 由于 undo 日志会记录每一次的 update，并且新插入的行数据会记录上一条 undo 日志的指针，所以可以通过这个指针找到上一条记录，这样就形成了一个版本链。
>
> ![三分恶面渣逆袭：版本链](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/mysql-765b3d83-14eb-4b56-8940-9d60bfaf1737.jpg)
>
> ​											版本链
>
> #### [说说什么是 ReadView？](https://javabetter.cn/sidebar/sanfene/mysql.html#说说什么是-readview)
>
> ReadView（读视图）是 InnoDB 为了实现一致性读（Consistent Read）而创建的数据结构，它用于确定在特定事务中哪些版本的行记录是可见的。
>
> ReadView 主要用来处理隔离级别为"可重复读"（REPEATABLE READ）和"读已提交"（READ COMMITTED）的情况。因为在这两个隔离级别下，事务在读取数据时，需要保证读取到的数据是一致的，即读取到的数据是在事务开始时的一个快照。
>
> #### [可重复读和读已提交在 ReadView 上的区别是什么？](https://javabetter.cn/sidebar/sanfene/mysql.html#可重复读和读已提交在-readview-上的区别是什么)
>
> 可重复读（REPEATABLE READ）和读已提交（READ COMMITTED）的区别在于生成 ReadView 的时机不同。
>
> - 可重复读：在第一次读取数据时生成一个 ReadView，这个 ReadView 会一直保持到事务结束，这样可以保证在事务中多次读取同一行数据时，读取到的数据是一致的。
> - 读已提交：每次读取数据前都生成一个 ReadView，这样就能保证每次读取的数据都是最新的。

## 说一下你知道的锁？

> 锁是一种同步机制，用于控制多个线程或进程对共享资源的访问，以防止并发访问导致的数据不一致或竞争条件等问题。下面详细介绍几种常见的锁类型及其特点：
>
> ### 1. 互斥锁（Mutex）
>
> - **定义**:
>   - 互斥锁是最基本的锁类型，它允许多个线程中的一个独占访问共享资源。
>   - 一旦一个线程获得了互斥锁，其他试图获取该锁的线程将会阻塞，直到锁被释放。
>
> - **用途**:
>   - 保护临界区，确保一次只有一个线程可以访问共享资源。
>   - 适用于需要独占访问资源的情况。
>
> ### 2. 读写锁（ReadWriteLock）
>
> - **定义**:
>   - 读写锁允许多个线程同时进行读操作，但只允许一个线程进行写操作。
>   - 当多个线程同时读取共享资源时，读写锁可以显著提高并发性能。
>
> - **用途**:
>   - 在读操作远多于写操作的场景中提高并发效率。
>   - 适用于需要区分读写操作的场景。
>
> ### 3. 自旋锁（Spinlock）
>
> - **定义**:
>   - 自旋锁是一种简单的锁机制**，线程在尝试获取锁失败时会继续循环尝试获取锁，而不是进入阻塞状态**。
>   - **自旋锁适合于锁持有时间较短的情况，以减少线程上下文切换的开销**。
>
> - **用途**:
>   - 在锁持有时间很短且线程争用不严重的情况下使用，以减少线程切换的成本。
>   - 适用于锁持有时间短、争用不大的场景。
>
> ### 4. 信号量（Semaphore）
>
> - **定义**:
>   - 信号量是一个计数器，用于控制多个线程对共享资源的访问。
>   - 信号量可以控制多个线程同时访问资源的数量。
>
> - **用途**:
>   - 用于限制可以同时访问某个资源的线程数量，例如文件句柄或数据库连接。
>   - 适用于需要限制资源访问数量的场景。
>
> ### 5. 重入锁（Reentrant Lock）
>
> - **定义**:
>   - 重入锁允许同一个线程多次获取同一把锁，而不会导致死锁。
>   - 通常与条件变量一起使用，以实现更复杂的同步逻辑。
>
> - **用途**:
>   - 当一个线程需要多次进入临界区时使用。
>   - 适用于需要多次进入临界区的场景。
>
> ### 6. 乐观锁（Optimistic Locking）
>
> - **定义**:
>   - 乐观锁假设并发冲突较少，因此在读取数据时不加锁，而是在更新数据时检查数据版本是否发生变化。
>   - 通常通过版本号或时间戳来实现。
>
> - **用途**:
>   - 在并发冲突较少的情况下提高性能。
>   - 适用于并发冲突较少的场景。
>
> ### 7. 悲观锁（Pessimistic Locking）
>
> - **定义**:
>   - 悲观锁假设并发冲突频繁发生，因此在读取数据时就加锁。
>   - 通常用于数据库事务处理，确保数据的一致性。
>
> - **用途**:
>   - 在并发冲突较多的情况下使用，以确保数据一致性。
>   - 适用于需要严格数据一致性的场景。
>
> ### 8. 分布式锁
>
> - **定义**:
>   - 分布式锁用于协调分布式系统中的多个节点对共享资源的访问。
>   - 可以基于消息队列、ZooKeeper 或 Redis 等实现。
>
> - **用途**:
>   - 在分布式系统中实现资源共享的同步。
>   - 适用于分布式系统的资源共享场景。
>
> ### 9. 公平锁（Fair Lock）
>
> - **定义**:
>   - 公平锁按照请求锁的顺序来授予锁，以确保公平性。
>   - 公平锁可以避免某些线程饥饿。
>
> - **用途**:
>   - 当需要保证线程公平访问资源时使用。
>   - 适用于需要公平性的场景。
>
> ### 10. 非公平锁（Unfair Lock）
>
> - **定义**:
>   - 非公平锁不按照请求锁的顺序来授予锁，而是尽可能快速地授予锁。
>   - 非公平锁可以提高性能，但可能导致某些线程饥饿。
>
> - **用途**:
>   - 当性能比公平性更重要时使用。
>   - 适用于性能优先的场景。
>
> ### 11. Java 中的锁
>
> 在 Java 中，锁可以分为以下几类：
>
> - **synchronized 锁**:
>   - 通过 `synchronized` 关键字实现的锁，作用于对象或者类。
>   - 每个对象都有一个与之关联的锁。
>   - 适用于 Java 中的基本同步控制。
>
> - **ReentrantLock**:
>   - 是 Java 提供的一种可重入的互斥锁，它具有与 `synchronized` 相似的语义和功能。
>   - 重入锁可以重复进入由它所保护的临界区，而不会死锁。
>   - 适用于需要更高级控制的场景。
>
> - **读写锁（ReadWriteLock）**:
>   - 也是 Java 提供的一种锁机制，它分为读锁和写锁。
>   - 适用于读多写少的场景。
>
> - **偏向锁/轻量级锁/重量级锁**:
>   - 这三种锁特指 `synchronized` 锁的状态，通过在对象头中的 mark word 来表明锁的状态。
>   - 偏向锁、轻量级锁和重量级锁是为了适应不同的并发情况而设计的。
>   - 适用于不同级别的并发控制。
>
> ### 总结
>
> 锁是用于解决并发访问问题的重要工具。选择合适的锁类型取决于具体的应用场景、并发模型以及对性能的要求。了解不同类型的锁及其特点有助于开发者更好地设计并发程序。

## 悲观锁和乐观锁？分别适用于什么场景下？

> https://segmentfault.com/a/1190000023866733
>
> 悲观锁和乐观锁是两种常见的并发控制机制，用于处理多线程或多进程环境下数据的一致性和并发问题。下面分别介绍这两种锁的特性及其适用场景。
>
> ### 悲观锁
>
> 悲观锁假设最坏的情况会发生，即认为数据总是处于冲突状态。因此，在访问数据时，悲观锁会先获取锁，确保在操作期间数据不会被其他线程或进程修改。一旦操作完成，锁会被释放。
>
> #### 特点
>
> - **加锁**：在操作数据前，悲观锁会先获取锁。
> - **简单易用**：悲观锁的使用较为简单直观。
> - **性能开销**：悲观锁可能会增加系统的锁管理开销，降低系统的并发性能。
> - **容易引发死锁**：由于悲观锁是一种保守的锁策略，可能会导致事务之间相互等待，从而引发死锁问题。
>
> #### 适用场景
>
> - **写入操作频繁**：悲观锁适合用于写入操作比较频繁的场景，或者并发写入多、临界区代码复杂、竞争激烈等场景。
> - **并发冲突多**：当预期并发冲突较多时，悲观锁可以有效避免数据不一致的问题。
> - **数据一致性要求高**：当数据的一致性非常重要时，悲观锁可以提供更强的一致性保证。
>
> ### 乐观锁
>
> 乐观锁假设最好的情况会发生，即认为数据不会经常发生冲突。因此，默认情况下不加锁，只有在更新数据时才检查数据是否被其他线程或进程修改过。如果数据未被修改，则更新数据；如果数据已被修改，则放弃操作或重试。
>
> #### 特点
>
> - **不加锁**：在操作数据时不加锁，只有在更新数据时才进行版本检查。
> - **性能好**：由于不加锁，乐观锁的性能通常比悲观锁好，特别是在读取频繁的场景下。
> - **不能解决脏读**：乐观锁不能解决脏读的问题，即读取到其他未提交事务的数据。
> - **实现方式**：通常通过版本号机制或CAS（Compare and Swap）机制实现。
>
> #### 适用场景
>
> - **读取操作频繁**：乐观锁适合用于读取操作比较频繁的场景，或者虽然读写都很多，但是并发并不激烈的场景。
> - **并发冲突少**：当预期并发冲突较少时，乐观锁可以提高系统的吞吐量。
> - **数据一致性要求相对较低**：当数据的一致性要求不是非常高时，乐观锁可以提供较好的性能。
>
> ### 总结
>
> - **悲观锁**适用于写入操作频繁、并发冲突多、数据一致性要求高的场景。
> - **乐观锁**适用于读取操作频繁、并发冲突少、数据一致性要求相对较低的场景。
>
> 选择悲观锁还是乐观锁取决于具体的业务场景和需求。一般来说，如果数据的一致性非常重要，且写操作频繁，可以选择悲观锁；如果读操作远多于写操作，且并发冲突较少，可以选择乐观锁。在实际应用中，还需要考虑性能、系统设计等因素来做出最佳选择。



## 公平锁和非公平锁

> 公平锁和非公平锁是两种不同的锁实现策略，它们主要区别在于线程获取锁的顺序和方式。下面详细解释这两种锁的区别：
>
> ### 1. 公平锁（Fair Lock）
>
> - **定义**:
>   - 公平锁遵循**先来先服务**的原则，即按照线程请求锁的顺序来分配锁。
>   - 当一个线程请求锁时，它会加入等待队列，只有前面的线程释放锁后，它才能获得锁。
>
> - **优点**:
>   - 确保线程公平地获得锁，**减少了线程饥饿的问题**。
>   - 有助于**避免某些线程一直等待，而无法获得锁**。
>
> - **缺点**:
>   - 实现相对复杂，因为需要维护等待队列。
>   - 可能会导致较高的系统开销，因为每次获取锁都需要检查等待队列。
>   - 性能通常不如非公平锁。
>
> - **应用场景**:
>   - 当需要保证线程公平访问资源时使用。
>   - 适用于线程间的公平性比性能更重要的场景。
>
> ### 2. 非公平锁（Unfair Lock）
>
> - **定义**:
>   - 非公平锁并**不按照线程请求锁的顺序来分配锁，而是尽可能地快速分配锁**。
>   - 当一个线程请求锁时，它可能会立即获得锁，即使有其他线程已经在等待。
>
> - **优点**:
>   - **实现简单**，因为不需要维护复杂的等待队列。
>   - **性能通常优于公平锁**，因为它**减少了检查等待队列的开销**。
>
> - **缺点**:
>   - 可能会导致**线程饥饿**，即某些线程可能长时间得不到锁。
>   - 不适合需要保证公平性的场景。
>
> - **应用场景**:
>   - 当性能比公平性更重要时使用。
>   - 适用于线程间的公平性不是主要关注点的场景。
>
> ### 示例
>
> 假设有一个锁，线程 A 正在持有锁，线程 B 和 C 正在等待获取锁。
>
> - **公平锁**:
>   - 当线程 A 释放锁时，线程 B 会获得锁，因为它是等待队列中的第一个线程。
>   - 线程 C 需要在 B 释放锁之后才能获得锁。
>
> - **非公平锁**:
>   - 当线程 A 释放锁时，**线程 B 或 C 都有可能获得锁，这取决于哪个线程最先尝试获取锁**。
>   - 即使线程 B 已经等待了一段时间，如果线程 C 刚好尝试获取锁，它也有可能获得锁。
>
> ### Java 中的实现
>
> 在 Java 中，`java.util.concurrent.locks.ReentrantLock` 类提供了公平锁和非公平锁的实现。你可以通过构造函数指定是否使用公平锁。
>
> #### ReentrantLock 示例
>
> ```java
> import java.util.concurrent.locks.ReentrantLock;
> 
> public class LockExample {
>     public static void main(String[] args) {
>         // 创建一个非公平锁
>         ReentrantLock lock = new ReentrantLock(false); // false 表示非公平锁
> 
>         // 创建一个公平锁
>         ReentrantLock fairLock = new ReentrantLock(true); // true 表示公平锁
> 
>         // 使用锁的示例代码
>         lock.lock();
>         try {
>             // 执行临界区内的代码
>             System.out.println("Locked by thread " + Thread.currentThread().getName());
>         } finally {
>             lock.unlock();
>         }
>     }
> }
> ```
>
> ### 总结
>
> 公平锁和非公平锁各有优势和劣势。**公平锁确保线程能够按照请求顺序获得锁，有助于避免线程饥饿，但可能会影响性能。而非公平锁则牺牲了一定程度的公平性以换取更高的性能**。选择哪种锁取决于你的应用场景和需求。

## 什么是可重入锁？

> 可重入锁（Reentrant Lock）是一种特殊的锁类型，它**允许一个已经持有锁的线程再次获取同一把锁而不导致死锁**。换句话说，**同一个线程可以多次获取同一把锁，并且必须同样次数地释放锁**。这种特性使得可重入锁非常适合用于保护多个嵌套的临界区。
>
> ### 特点
>
> 1. **可重入性**:
>    - 同一个线程可以多次获取同一把锁。
>    - **每次获取锁时，锁的持有计数会增加**。
>    - 必须同样次数地释放锁，**锁的持有计数才会减至零，从而解锁**。
>
> 2. **公平性和非公平性**:
>    - 可重入锁可以是公平的也可以是非公平的。
>    - **公平锁按照请求锁的顺序来分配锁，有助于避免线程饥饿。**
>    - **非公平锁不保证锁的分配顺序，但通常提供更好的性能。**
>
> 3. **条件变量支持**:
>    - 可重入锁通常支持条件变量，这使得线程可以在等待某些条件满足时释放锁，在条件满足时重新获取锁。
>
> 4. **锁的粒度**:
>    - 可重入锁可以应用于不同的粒度，从方法级到细粒度的代码块级。
>
> 5. **高级控制**:
>    - 可重入锁通常提供了比基本的互斥锁更多的控制选项，例如尝试获取锁的超时时间、中断处理等。
>
> ### Java 中的实现
>
> 在 Java 中，`java.util.concurrent.locks.ReentrantLock` 类提供了可重入锁的实现。下面是一个简单的示例，演示如何使用 `ReentrantLock`：
>
> ```java
> import java.util.concurrent.locks.ReentrantLock;
> 
> public class ReentrantLockExample {
>     private final ReentrantLock lock = new ReentrantLock();
> 
>     public void outer() {
>         lock.lock();
>         try {
>             System.out.println("Outer method locked by thread " + Thread.currentThread().getName());
> 
>             inner(); // 调用内部方法，可以再次获取锁
>         } finally {
>             lock.unlock();
>         }
>     }
> 
>     public void inner() {
>         lock.lock();
>         try {
>             System.out.println("Inner method locked by thread " + Thread.currentThread().getName());
>         } finally {
>             lock.unlock();
>         }
>     }
> 
>     public static void main(String[] args) {
>         ReentrantLockExample example = new ReentrantLockExample();
>         example.outer();
>     }
> }
> ```
>
> 在这个示例中，`outer()` 方法获取了锁，并调用了 `inner()` 方法。`inner()` 方法也会尝试获取相同的锁。由于 `ReentrantLock` 支持可重入性，因此这个调用不会导致死锁。
>
> ### 总结
>
> 可重入锁是一种非常有用的同步工具，它允许同一个线程多次获取同一把锁，这对于需要保护多个嵌套临界区的场景非常有用。在 Java 中，`ReentrantLock` 提供了可重入锁的功能，并且可以通过构造函数来指定是否使用公平锁。使用可重入锁可以帮助你构建更复杂、更灵活的并发控制逻辑。

## 线程安全的数据结构？  

> JDK已经为大家准备好了一批好用的线程安全容器类，可以大大减少开发工作量，例如HashTable，ConcurrentHashMap，CopyOnWriteArrayList，CopyOnWriteArraySet，ConcurrentLinkedQueue，Vector，**StringBuffer**等。
>
> 1. HashTable
>
> HashTable实现了Map接口，为此其本身也是一个散列表，它存储的内容是基于key-value的键值对映射。
>
> HashTable中的**key、value都不可以为null**；具有无序特性；由于其方法函数都是同步的（采用synchronized修饰），不会出现两个线程同时对数据进行操作的情况，因此保证了线程安全性。
>
> HashTable使用synchronized来修饰方法函数来保证线程安全，但是在多线程运行环境下效率表现非常低下。
>
> 因为当一个线程访问HashTable的同步方法时，其他线程也访问同步方法就会出现阻塞状态。
>
> 比如当一个线程在添加数据时候，另外一个线程即使执行获取其他数据的操作也必须被阻塞，大大降低了程序的运行效率。
>
> 1. ConcurrentHashMap
>
> 我们知道HashMap是线程不安全的，ConcurrentHashMap是HashMap的线程安全版。
>
> 但是与HashTable相比，ConcurrentHashMap不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。
>
> ConcurrentHashMap允许多个修改操作并发运行，其原因在于使用了锁分段技术（**JDK1.8取消了分段锁**）：首先讲Map存放的数据分成一段一段的存储方式，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。这样就保证了每一把锁只是用于锁住一部分数据，那么当多线程访问Map里的不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效提高并发访问效率。
>
> 上述的处理机制明显区别于HashTable是给整体数据分配了一把锁的处理方法。
>
> 为此，在多线程环境下，常用ConcurrentHashMap在需要保证数据安全的场景中去替换HashMap，而不会去使用HashTable，同时在最新版的JDK中已经推荐废弃使用HashTable。
>
> 1. CopyOnWriteArrayList
>
> CopyOnWriteArrayList实现了List接口，提供的数据更新操作都使用了ReentrantLock的lock()方法来加锁，unlock()方法来解锁。
>
> 当增加元素的时候，首先**使用Arrays.copyOf()来拷贝形成新的副本，在副本上增加元素，然后改变原引用指向副本**。**读操作不需要加锁，而写操作类实现中对其进行了加锁**。因此，CopyOnWriteArrayList类是一个线程安全的List接口的实现，在高并发的情况下，可以提供高性能的并发读取，并且保证读取的内容一定是正确的，这对于读操作远远多于写操作的应用非常适合（注意： 如上述更新操作会带来较大的空间与性能开销，如果更新操太过频繁，反而不太合适使用）。
>
> > 1. CopyOnWriteArrayList使用`ReentrantLock`重入锁加锁，保证线程安全；
> > 2. CopyOnWriteArrayList的写操作都要先拷贝一份新数组，在新数组中做修改，修改完了再用新数组替换老数组，所以空间复杂度是O(n)，性能比较低下；
> > 3. CopyOnWriteArrayList的读操作支持随机访问，时间复杂度为O(1)；
> > 4. CopyOnWriteArrayList采用读写分离的思想，读操作不加锁，写操作加锁，且写操作占用较大内存空间，所以适用于读多写少的场合；
> > 5. CopyOnWriteArrayList只保证最终一致性，不保证实时一致性。
>
> 1. CopyOnWriteArraySet
>
> CopyOnWriteArraySet是对CopyOnWriteArrayList使用了装饰模式后的具体实现。所以CopyOnWriteArrayList的实现机理适用于CopyOnWriteArraySet，此处不再赘述。
>
> Java里的List和Set的之间的特性比较结论同样适用于CopyOnWriteArrayList与CopyOnWriteArraySet之间的比较；此外，CopyOnWriteArrayList与CopyOnWriteArraySet都是线程安全的。
>
> 1. ConcurrentLinkedQueue
>
> ConcurrentLinkedQueue可以被看作是一个线程安全的LinkedList，使用了非阻塞算法实现的一个高效、线程安全的并发队列；其本质是一个基于链接节点的无界线程安全队列，它采用先进先出的规则对节点进行排序，当添加一个元素时会添加到队列的尾部；当获取一个元素时，会返回队列头部的元素。
>
> ConcurrentLinkedQueue应该算是在高并发环境中性能最好的队列，没有之一。
>
> 1. Vector
>
> Vector通过数组保存数据，继承了Abstract，实现了List；所以，其本质上是一个队列。
>
> 但是和ArrayList不同，Vector中的操作是线程安全的，它是利用synchronized同步锁机制进行实现，其实现方式与HashTable类似。
>
> 1. StringBuffer（线程安全）与StringBuilder（线程不安全，但性能比StringBuffer好，不存在线程安全问题，优先使用）
>
> https://blog.csdn.net/weixin_45393094/article/details/104526603
>
> 在Java里面，字符串操作应该是最频繁的操作了，为此有必要把StringBuffer与StringBuilder两个方法类比较一下。
>
> 首先，对于频繁的字符串拼接操作，是不推荐采用效率低下的“+”操作的。一般是采用StringBuffer与StringBuilder来实现上述功能。但是，这两者也是有区别的：前者线程安全，后者不是线程安全的。

## ConcurrentHashMap如何实现线程安全的？

> JDK1.7和JDK1.8
>
> > ### [JDK 1.7 和 JDK 1.8 的 ConcurrentHashMap 实现有什么不同？](#jdk-1-7-和-jdk-1-8-的-concurrenthashmap-实现有什么不同)
> >
> > - **线程安全实现方式**：JDK 1.7 采用 `Segment` 分段锁来保证安全， `Segment` 是继承自 `ReentrantLock`。JDK1.8 放弃了 `Segment` 分段锁的设计，采用 `Node + CAS + synchronized` 保证线程安全，锁粒度更细，`synchronized` 只锁定当前链表或红黑二叉树的首节点。
> > - **Hash 碰撞解决方法** : JDK 1.7 采用拉链法，JDK1.8 采用拉链法结合红黑树（链表长度超过一定阈值时，将链表转换为红黑树）。
> > - **并发度**：JDK 1.7 最大并发度是 Segment 的个数，默认是 16。JDK 1.8 最大并发度是 Node 数组的大小，并发度更大。
> >
> > ------
> >
> > 著作权归JavaGuide(javaguide.cn)所有 基于MIT协议 原文链接：https://javaguide.cn/java/collection/java-collection-questions-02.html
>
> #### **数据结构**
>
> 正如最开始的两个结构示意图所示，Java 7 采用 **Segment 分段锁**来实现，而 Java 8 中的 ConcurrentHashMap 使用**数组 + 链表 + 红黑树**，在这一点上它们的差别非常大。
>
> #### **并发度**
>
> Java 7 中，每个 Segment 独立加锁，最大并发个数就是 Segment 的个数，默认是 16。
>
> 但是到了 Java 8 中，**锁粒度更细**，理想情况下 table 数组元素的个数（也就是数组长度）就是其支持并发的最大个数，并发度比之前有提高。
>
> #### **保证并发安全的原理**
>
> Java 7 采用 Segment 分段锁来保证安全，而 Segment 是继承自 ReentrantLock。
>
> Java 8 中放弃了 Segment 的设计，采用 Node + CAS + synchronized 保证线程安全。
>
> #### **遇到 Hash 碰撞**
>
> Java 7 在 Hash 冲突时，会使用**拉链法，也就是链表的形式**。
>
> Java 8 先使用拉链法，**在链表长度超过一定阈值时，将链表转换为红黑树，来提高查找效率**。
>
> #### **查询时间复杂度**
>
> Java 7 遍历链表的时间复杂度是 O(n)，n 为链表长度。
>
> Java 8 如果变成遍历红黑树，那么时间复杂度降低为 O(log(n))，n 为树的节点个数。

## 别的系统怎么调用你们的系统的？

> http

## 项目是单体还是微服务，注册中心用的什么，配置中心怎么下发热更新的，原理是什么？



反问：是否设计核心支付场景？



# 思淘瑞 Stori	一面

2024年8月14日 19:00 ~ 20：30	

面了90min，问了很多业务层面的东西、金工的知识

挑一个你认为最复杂的衍生指标，讲讲怎么算的？

> 讲了夏普比率的计算，无风险年化 / 波动率
>
> 为什么选择30年国债作为无风险年化利率？
>
> 金融产品里面，只有国债最适合作为无风险利率，我猜测应该是因为由国家发行和背书吧。
>
> 

## 实际业务场景中二级缓存怎么设置的？

## JetCache二级缓存，Local、Remote、Both分别在什么场景下设置？

> 本地jvm缓存相比redis缓存，纯内存操作，不需要网络io，速度极快，但受jvm内存容量的限制，且在分布式环境下，本地jvm缓存保证多个节点之间的一致性也是一个问题。
>
> 实际项目中，对接行内B端的系统，流量和并发度没有那么高，使用远程分布式redis缓存即可。
>
> 面向C端手机银行App，高并发场景，我们的有些接口包主机接口的数据和平台数据进行整合返回给手机银行app后端，对于接口响应时间要求比较高，使用二级缓存提高访问速度。在Both 二级缓存模式下，基于Redis Pub/Sub模式，对于任意单节点的缓存操作事件会被广播到集群中的其他节点，其他节点监听到该事件，使得其他所有节点本地jvm缓存失效，保证二级缓存一致性。

## 多级缓存之间的一致性、DB和缓存双写一致性？

> jetcache保证二级缓存一致性见上文，db和缓存双写一致性采用Cache Aside（旁路缓存模式），即先更新db，再删除缓存。

## 缓存的过期策略是什么样的？数据淘汰策略？

> 过期策略：allKeys LRU		https://blog.csdn.net/zhiaidaidai/article/details/134961743
>
> 淘汰策略：惰性删除（CPU友好，内存不友好） or 定期删除（内存友好，CPU不友好）							
>
> https://segmentfault.com/a/1190000041447820

哪一部分是你自己独立开发实现的？

多少人参与开发？开发了多久？

## QPS有多少？

> 

## 数据侧数据量有多少？

理财、保险的估值？

# 蚂蚁-网商银行一面

8.15 15:00 ~ 16:00

## 项目中遇到的最大的挑战是什么？

## 遇到什么问题，怎么排查的？遇到生产问题怎么一般怎么排查的？

> 可以说一个排查问题的过程，有用到开源工具最好 比如阿尔萨斯。没有就redis、sql，往一个自己熟悉的领域说。如果排查过程中上用了什么命令、debug源码什么的 都可以说。实在实在没有，百度一个oom排查过程 mark一下 ... 展开  
>
> 作者：Evan_Wang🌴🌴
> 链接：https://www.nowcoder.com/discuss/353157112958230528
> 来源：牛客网

JAVA项目，隔一段时间就OOM重启

![image-20240816215835950](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240816215835950.png)



## 什么情况下CPU占用率会很高?CPU占用高怎么排查？

> https://www.cnblogs.com/crazymakercircle/p/18240933
>
> - top 命令查看CPU占用率
> - jstack命令查看堆栈信息

jps、jstack、jmap

> https://www.cnblogs.com/baihuitestsoftware/articles/6382733.html*
>
> 在Java开发中，调试是非常重要的环节之一，它帮助开发者找出程序中的错误并进行修复。Java提供了多种工具和命令来辅助调试，下面是一些常用的Java调试工具和命令：
>
> ### 1. JVisualVM (VisualVM)
>
> - **简介**：VisualVM 是一个免费的、强大的Java监控和性能分析工具，它可以帮助开发者监控和分析Java应用程序的性能。
> - **功能**：
>   - CPU和内存分析
>   - 线程分析
>   - 文件I/O监控
>   - 堆转储和非堆转储
>   - 网络监控
> - **使用**：VisualVM 可以作为独立应用程序运行，也可以作为一个插件集成到NetBeans IDE中。
>
> ### 2. JConsole
>
> - **简介**：JConsole 是Java自带的一个监控工具，用于监视和调试运行中的Java应用程序。
> - **功能**：
>   - 内存使用监控
>   - CPU使用率
>   - 线程状态
>   - 类加载信息
>   - 堆转储
> - **使用**：JConsole 可以远程连接到运行中的Java应用程序，并提供图形界面来查看应用程序的状态。
>
> ### 3. jstack
>
> - **简介**：jstack 是一个命令行工具，用于生成运行中的Java进程的线程堆栈跟踪。
> - **功能**：
>   - 显示所有线程的堆栈跟踪
>   - 查看死锁信息
>   - 线程状态监控
> - **使用**：通过命令行输入 `jstack <pid>` 来获取指定Java进程的线程堆栈信息。
>
> ### 4. jmap
>
> - **简介**：jmap 是一个命令行工具，用于生成Java进程的堆转储文件，还可以显示对象和类的信息。
> - **功能**：
>   - 生成堆转储文件
>   - 显示对象和类的统计信息
>   - 查看堆使用情况
> - **使用**：通过命令行输入 `jmap -dump:format=b,file=<filename> <pid>` 来生成堆转储文件。
>
> ### 5. jstat
>
> - **简介**：jstat 是一个命令行工具，用于监视Java进程的运行时统计信息，如内存使用情况。
> - **功能**：
>   - 内存使用情况
>   - GC活动
>   - 类加载信息
> - **使用**：通过命令行输入 `jstat -gc <pid>` 来查看指定Java进程的GC信息。
>
> ### 6. jinfo
>
> - **简介**：jinfo 是一个命令行工具，用于查询和更改Java进程的JVM配置参数。
> - **功能**：
>   - 查询JVM配置
>   - 动态更改JVM参数
> - **使用**：通过命令行输入 `jinfo -flag <option> <pid>` 来查询或更改指定Java进程的JVM参数。
>
> ### 7. jhat
>
> - **简介**：jhat 是一个命令行工具，用于生成堆转储文件并提供一个HTTP服务器来浏览堆转储文件。
> - **功能**：
>   - 生成堆转储文件
>   - 浏览堆转储文件
> - **使用**：通过命令行输入 `jhat <heapdumpfile>` 来启动HTTP服务器并浏览堆转储文件。
>
> ### 8. Eclipse Memory Analyzer (MAT)
>
> - **简介**：Eclipse Memory Analyzer (MAT) 是一个强大的Java内存分析工具，用于分析Java堆转储文件。
> - **功能**：
>   - 内存泄漏检测
>   - 内存使用分析
>   - 对象引用关系图
> - **使用**：将堆转储文件导入MAT，然后使用各种视图和工具来分析内存使用情况。
>
> ### 9. IntelliJ IDEA Debugger
>
> - **简介**：IntelliJ IDEA 提供了一个强大的调试器，用于调试Java应用程序。
> - **功能**：
>   - 断点设置
>   - 单步执行
>   - 变量监视
>   - 远程调试
> - **使用**：在IntelliJ IDEA中设置断点，启动调试会话，使用调试工具栏或菜单来控制程序执行。
>
> ### 10. NetBeans Debugger
>
> - **简介**：NetBeans IDE 自带一个调试器，用于调试Java应用程序。
> - **功能**：
>   - 断点设置
>   - 单步执行
>   - 变量监视
>   - 远程调试
> - **使用**：在NetBeans IDE中设置断点，启动调试会话，使用调试工具栏或菜单来控制程序执行。
>
> ### 示例
>
> 假设你想使用`jstack`工具来查看一个Java进程的线程堆栈信息：
>
> 1. **确定Java进程ID**：
>
>    ```bash
>    ps -ef | grep java
>    ```
>
> 2. **使用jstack获取线程堆栈信息**：
>
>    ```bash
>    jstack <pid>
>    ```
>
>    其中 `<pid>` 是你想要调试的Java进程的ID。
>
> ### 总结
>
> 以上列举的工具涵盖了从基本的命令行工具到更复杂的IDE集成调试工具。选择合适的工具取决于你的具体需求和偏好。在实际开发过程中，你可能需要结合使用多种工具来全面了解和调试Java应用程序。



## OOM怎么排查？

> https://blog.csdn.net/o9109003234/article/details/121917786

## Spring AOP是什么？怎么实现的？Spring使用的是JDK的动态代理还是CgLib的动态代理

> **Spring AOP 默认使用 JDK 动态代理，如果目标对象没有实现接口，则自动切换到 CGLIB 动态代理**。你可以通过配置 `DefaultAdvisorAutoProxyCreator` 的 `setProxyTargetClass` 方法来显式地指定使用哪种代理类型。使用 JDK 动态代理时，性能通常更好，而 CGLIB 动态代理则提供了更多的灵活性，特别是对于没有实现接口的目标对象。
>
> Spring 框架中的 AOP（面向切面编程）模块支持两种类型的动态代理机制：JDK 动态代理和 Cglib 代理。这两种代理机制的选择取决于几个因素，主要是目标对象是否实现了接口。
>
> ### JDK 动态代理
> JDK 动态代理是基于 Java 的 `java.lang.reflect.Proxy` 类和 `java.lang.reflect.InvocationHandler` 接口实现的。它只能为实现了接口的目标对象创建代理。这意味着如果一个类已经实现了某些接口，那么 Spring 就会使用 JDK 动态代理来创建该类的代理对象。
>
> ### Cglib 代理
> Cglib 是一个高性能的字节码生成库，它可以用来为一个未实现接口的类创建子类。Spring 使用 Cglib 创建代理时，不需要目标对象实现特定接口。这种代理方式适用于那些没有实现接口的类。
>
> ### Spring 中如何选择代理类型
> Spring 会根据目标对象是否实现了接口来自动选择合适的代理类型：
>
> - 如果目标对象实现了至少一个接口，则 Spring 使用 JDK 动态代理。
> - 如果目标对象没有实现任何接口，则 Spring 使用 Cglib 代理。
>
> ### 配置选项
> 在 Spring 中，也可以通过配置显式地指定要使用的代理类型。例如，在 XML 配置中，可以使用 `aop:aspectj-autoproxy` 元素的 `proxy-target-class` 属性来控制是否总是使用 Cglib 代理。
>
> ```xml
> <aop:aspectj-autoproxy proxy-target-class="true"/>
> ```
>
> 如果将 `proxy-target-class` 设置为 `true`，那么无论目标对象是否实现了接口，Spring 都会使用 Cglib 代理。
>
> ### 总结
> 在默认情况下，Spring AOP 会根据目标对象是否实现了接口来决定使用 JDK 动态代理还是 Cglib 代理。**如果目标对象实现了接口，那么 Spring 默认使用 JDK 动态代理；如果没有实现接口，则使用 Cglib 代理**。当然，你也可以通过配置来强制使用 Cglib 代理。

## 说一说Java的异常类型？Exception和Error有什么区别？

> Exception 和 Error 都是继承了  Throwable 类，在 Java 中只有 Throwable  类型的实例才可以被抛出（throw）或者捕获（catch），它是异常处理机制的基本组成类型。Exception 和 Error 体现了 Java 平台设计者对不同异常情况的分类。Exception 是程序正常运行中，可以预料的意外情况，可能并且应该被捕获，进行相应处理。Error  是指在正常情况下，不大可能出现的情况，绝大部分的 Error 都会导致程序（比如 JVM  自身）处于非正常的、不可恢复状态。既然是非正常情况，所以不便于也不需要捕获，常见的比如 OutOfMemoryError 之类，都是 Error 的子类。Exception  又分为**受检（checked）异常和不受检（unchecked）异常**，**受检（checked）异常在源代码里必须显式地进行捕获处理，这是编译期检查的一部分**。前面我介绍的不可查的 Error，是 Throwable 不是 Exception。**不受检异常（unchecked）就是所谓的运行时异常**，类似  NullPointerException、ArrayIndexOutOfBoundsException  之类，通常是可以编码避免的逻辑错误，具体根据需要来判断是否需要捕获，并不会在编译期强制要求。
>
> Java 中的异常处理机制是用于管理程序运行过程中可能出现的问题的一种方式。Java 的异常体系主要分为两大类：`Exception` 和 `Error`。这两者都继承自 `Throwable` 类，但它们有着不同的用途和处理方式。
>
> ### Throwable 类
> `Throwable` 是 Java 中所有错误和异常的超类，它定义了一些基本方法，如 `printStackTrace()`、`getMessage()` 等。
>
> ### Exception 类
> `Exception` 类及其子类表示应用程序可能捕获并处理的异常。当这些异常发生时，通常可以通过修改程序逻辑来避免或处理它们。`Exception` 类本身又可以分为两个子类：
> - **检查型异常**（Checked Exceptions）：这些异常在**编译时会被检**查，如果一个方法声明抛出某种检查型异常，那么调用者必须处理这个异常，**要么捕获它，要么在自己的方法签名中声明抛出它**。例如 `IOException`。
> - **运行时异常**（Runtime Exceptions）：这些异常在编译时不会被检查，通常是由编程错误导致的，比如 `NullPointerException` 或 `ArrayIndexOutOfBoundsException`。这类异常通常不需要在方法签名中声明，因为它们通常表明了代码中的错误。
>
> ### Error 类
> `Error` 类及其子类表示严重问题，这些问题通常发生在 JVM 级别上，比如 **`OutOfMemoryError` 或 `StackOverflowError`**。这些问题**很难通过修改应用级别的代码来解决**，并且通常不应该被程序捕获和处理。大多数情况下，如果遇到 `Error`，最好的做法是让程序终止执行。
>
> ### Exception 和 Error 的主要区别
> 1. **处理方式**：
>    - `Exception` 可以被捕获和处理，通常代表了一种可以恢复的情况。
>    - `Error` 一般不需要也不应该被程序捕获，通常代表了一种无法恢复的情况。
>
> 2. **用途**：
>    - `Exception` 通常用于表示程序逻辑错误或者外部条件变化导致的问题。
>    - `Error` 表示系统级错误或资源耗尽等严重情况，这些问题往往不能通过修改代码来解决。
>
> 3. **编译时检查**：
>    - `Exception` 包括了检查型异常和运行时异常。
>    - `Error` 不会被编译器检查。
>
> ### 示例
> 下面是一个简单的例子来说明如何使用 `Exception`：
>
> ```java
> public class Example {
>     public static void main(String[] args) {
>         try {
>             readFile("example.txt");
>         } catch (FileNotFoundException e) {
>             System.out.println("File not found: " + e.getMessage());
>         }
>     }
> 
>     private static void readFile(String filename) throws FileNotFoundException {
>         // 假设这里读取文件失败
>         throw new FileNotFoundException("File does not exist.");
>     }
> }
> ```
>
> 在这个例子中，`FileNotFoundException` 是一个检查型异常，需要在调用方法中处理，**要么被try-catch捕获处理，要么通过方法向上抛出去**。
>
> 总的来说，`Exception` 和 `Error` 在 Java 中扮演着不同的角色，理解它们之间的区别对于编写健壮的应用程序是非常重要的。

## MySQL的事务隔离级别？

> 读未提交（read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
> 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
> 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。这是MySQL InnoDB 引擎的默认隔离级别；
> 串行化（serializable ）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

## mysql一条select语句怎么执行的？

> ![image-20240825172105601](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825172105601.png)
>
> MySQL 8.0开始，删除了查询缓存的整个功能模块。

## MVCC

> `MVCC` 的实现依赖于：**隐藏字段、Read View、undo log**。在内部实现中，`InnoDB` 通过数据行的 `DB_TRX_ID` 和 `Read View` 来判断数据的可见性，如不可见，则通过数据行的 `DB_ROLL_PTR` 找到 `undo log` 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 `Read View` 之前已经提交的修改和该事务本身做的修改
>
> ### [隐藏字段](#隐藏字段)
>
> 在内部，`InnoDB` 存储引擎为每行数据添加了三个 [隐藏字段open in new window](https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html)：
>
> - `DB_TRX_ID（6字节）`：表示最后一次插入或更新该行的事务 id。此外，`delete` 操作在内部被视为更新，只不过会在记录头 `Record header` 中的 `deleted_flag` 字段将其标记为已删除
> - `DB_ROLL_PTR（7字节）` 回滚指针，指向该行的 `undo log` 。如果该行未被更新，则为空
> - `DB_ROW_ID（6字节）`：如果没有设置主键且该表没有唯一非空索引时，`InnoDB` 会使用该 id 来生成聚簇索引
>
> [`Read View`open in new window](https://github.com/facebook/mysql-8.0/blob/8.0/storage/innobase/include/read0types.h#L298) 主要是用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”
>
> ### [undo-log](#undo-log)
>
> `undo log` 主要有两个作用：
>
> - 当事务回滚时用于将数据恢复到修改前的样子
> - 另一个作用是 `MVCC` ，当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过 `undo log` 读取之前的版本数据，以此实现非锁定读
>
> ## [RC 和 RR 隔离级别下 MVCC 的差异](#rc-和-rr-隔离级别下-mvcc-的差异)
>
> 在事务隔离级别 `RC` 和 `RR` （InnoDB 存储引擎的默认事务隔离级别）下，`InnoDB` 存储引擎使用 `MVCC`（非锁定一致性读），但它们生成 `Read View` 的时机却不同
>
> - 在 RC 隔离级别下的 **`每次select`** 查询前都生成一个`Read View` (m_ids 列表)
> - 在 RR 隔离级别下只在事务开始后 **`第一次select`** 数据前生成一个`Read View`（m_ids 列表）
>
> 

## 前端用的什么技术栈？

>基础：JS、HTML、CSS
>
>前端框架：VUE
>
>UI框架：Element-UI
>
>构建工具和包管理器：Webpack、npm
>
>前端技术栈是指开发网页应用或桌面应用时所使用的一系列技术和工具。随着Web技术的发展，前端开发变得越来越重要且复杂，涵盖了从HTML、CSS、JavaScript基础到复杂的框架和库的应用。
>
>以下是一些常见的前端技术及其相关工具：
>
>### 基础技术
>1. **HTML (HyperText Markup Language)**
>   - 用于构建网页的基本结构。
>   - HTML5 引入了许多新特性，如 `<audio>`, `<video>`, `<canvas>` 等标签。
>
>2. **CSS (Cascading Style Sheets)**
>   - 用于控制网页的样式和布局。
>   - CSS3 引入了更多的动画效果、响应式设计的支持等。
>
>3. **JavaScript**
>   - 用于实现网页的交互性和动态功能。
>   - ES6+（ECMAScript 2015 及以后版本）引入了许多新特性，如箭头函数、模块系统等。
>
>### 构建工具和包管理器
>1. **Webpack**
>   - 用于模块打包和资源管理。
>   - 支持热更新、代码拆分等功能。
>
>2. **Gulp/Grunt**
>   - 自动化构建工具。
>   - 用于自动化任务如压缩文件、合并文件等。
>
>3. **npm (Node Package Manager) / Yarn**
>   - 包管理器，用于安装和管理 JavaScript 库和框架。
>
>### 框架和库
>1. **React**
>   - Facebook 开发的 JavaScript 库，用于构建用户界面。
>   - 使用虚拟 DOM 提高性能。
>
>2. **Angular**
>   - Google 推出的完整的前端框架。
>   - 支持依赖注入、双向数据绑定等。
>
>3. **Vue.js**
>   - 轻量级的渐进式框架。
>   - 易于上手，提供了强大的组件系统。
>
>### UI 框架
>1. **Bootstrap**
>   - 响应式设计框架，包含大量的预定义样式和组件。
>
>2. **Material-UI**
>   - 基于 React 的 UI 组件库，遵循 Material Design 规范。
>
>3. **Ant Design**
>   - 面向企业级产品的设计体系，适用于 React。
>
>### 状态管理
>1. **Redux**
>   - 用于管理应用程序的状态容器。
>   - 通常与 React 结合使用。
>
>2. **Vuex**
>   - Vue.js 的状态管理模式和库。
>
>### 测试
>1. **Jest**
>   - 快速可靠的 JavaScript 测试框架。
>   - 用于单元测试、集成测试等。
>
>2. **Mocha**
>   - 流行的 JavaScript 测试框架。
>   - 常用于单元测试。
>
>### 其他工具和技术
>1. **TypeScript**
>   - JavaScript 的超集，提供了静态类型检查。
>   - 可以编译成纯 JavaScript。
>
>2. **Sass/LESS**
>   - CSS 预处理器，可以编写更易于维护的样式表。
>
>3. **ESLint**
>   - 代码质量检查工具，帮助开发者遵循编码标准。
>
>4. **Babel**
>   - 编译器，用于将新的 JavaScript 语法转换为旧版本浏览器可识别的代码。
>
>5. **Git**
>   - 版本控制系统，用于代码管理和团队协作。
>
>### 响应式设计
>1. **Media Queries**
>   - CSS 中的媒体查询，用于实现响应式布局。
>
>2. **Flexbox**
>   - CSS3 的一种布局模式，简化了对齐和空间分配。
>
>3. **Grid Layout**
>   - CSS3 的网格布局，允许创建复杂的二维布局。
>
>### 性能优化
>1. **懒加载 (Lazy Loading)**
>   - 仅在需要时加载某些资源。
>
>2. **代码分割 (Code Splitting)**
>   - 将代码拆分成较小的块，按需加载。
>
>3. **资源压缩**
>   - 减小文件大小，提高加载速度。
>
>4. **Service Workers**
>   - 用于离线访问和预加载资源。
>
>5. **PWA (Progressive Web Apps)**
>   - 进阶的 Web 应用，具有类似原生应用的体验。
>
>前端技术栈非常广泛，不同的项目可能需要不同的技术和工具。了解这些技术和工具的基本原理以及它们如何协同工作对于成为一名优秀的前端开发者至关重要。随着技术的不断演进，保持学习和适应新技术也是非常重要的。

## 讲讲跨域问题？

> https://blog.csdn.net/qq_43353619/article/details/111058749

## 介绍项目中责任链模式的使用？如果接入的指标计算越来越多，责任链越来越长怎么解决？

> https://blog.csdn.net/weixin_42762133/article/details/109628930

代码题

**![image-20240815162634504](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240815162634504.png)**

# 问题

1、多级缓存一致性问题，项目问题没有找到具体解决方案，最后去github一个个issue翻并且提了issue和开源框架作者请教，找到了对应答案，解决了多级缓存一致性问题。

2、缓存实体类字段变更，上线投产，原来的缓存还没有失效，到时命中旧缓存，在反序列化的时候报错

​	解决：给缓存换了个前缀版本上线修复

3、OOM问题

多个任务同时提交执行，大批量数据读取到内存中处理，导致OOM，

```
1、调整JVM参数，调大堆内存，调大元空间大小；
2、手动启停任务，错峰执行；
通过1、2临时调整，进行临时上线投产修复。第二天，数据任务分批次读取到内存进行处理，进行上线调整。
```



# JVM调优

>
>
>https://www.cnblogs.com/three-fighter/p/14644152.html
>
>https://segmentfault.com/a/1190000044547802#item-3-9
>
>https://javaguide.cn/java/jvm/jvm-parameters-intro.html#_4-%E5%A4%84%E7%90%86-oom
>
>JDK自带了很多性能监控工具，我们可以用这些工具来监测系统和排查内存性能问题。
>
>![JDK自带工具](https://gitee.com/sanfene/picgo/raw/master/856154-20201010094123520-1650593645.png)



# 蚂蚁面经

> 蚂蚁集团-财产险创新技术（java工程师）-杭州
> 工作职责
> 负责蚂蚁保险财产险技术研发，例如智能风控、外呼、大模型等技术在业务场景落地，为用户提供智能、专业的科技服务 1、协助保险业务场景梳理业务需求，提供业务规划方案、架构设计方案； 2、负责业务系统的规划设计，制定产品的技术发展路线，完成重要业务模块及核心框架的搭建及编码实现； 3、发现和解决业务系统的技术问题，保证系统的性能和稳定性； 4、协同他人组织跨团队沟通协作，确保系统架构内外设计合理或保障项目质量与进度； 5、搭建算法工程链路，提出合理可行的架构演进和迭代方案，完成智能化业务方案落地。
> 任职要求
> 1、JAVA基础扎实：熟练掌握数据结构、多线程编程，掌握常用的设计模式及具备业务模型抽象能力；熟悉JVM，包括内存模型、类加载机制以及性能优化； 2、3年以上java及WEB应用软件开发经验；熟悉spring mvc、orm框架（ibatis或hibernate）； 3、熟悉常见的中间件、分布式解决方案及其原理：分布式缓存、SOA、消息中间件，负载均衡、连接池等；熟悉蚂蚁sofa技术栈优先； 4、具有一定的需求分析能力，善于捕捉业务需求、架构设计中存在的问题，并给出有效的技术解决方案； 5、具有很强的分析复杂问题和解决复杂问题的能力，有强烈的责任心和使命感，良好的沟通表达能力和团队协作能力； 6、有智能化业务落地经验，看中保险科技业务领域长远发展前景，认同行业发展方向的同路人优先考虑。

> **![image-20240819181126318](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240819181126318.png)**
>
> 人选A：
> 一面：
> 1.数据库，事务特性，各种锁，各种优化
> 2.印象深的一个项目，技术难点，如何解决，如何优化
> 3.有没有架构思路分享，有没有什么方法论分享
> 4.手写加法计算器
> 二面：
> 1.分布式事务相关问题   
> 2.最近做过的项目，难点在哪里，如何解决，还有没有更优方案   
> 3.消息队列用过哪些，原理怎样，如何调优   
> 4.项目合作需要注意什么问题
>
> 人选B：
> 一面：
> 1、除了问项目，问了mysql事务，spring 事务传播，mvcc
> 2、Zk和etcd区别
> 3、雪花算法原理，依赖的数据源可以用那些
> 4、MySQL 分库分表有哪些优缺点，怎么解决这个缺点(一致性哈希)
> 5、Es的索引原理，架构，有哪些性能优化手段，提高写入
> 6、Java基础
> 7、手写lru，还有一个线程题
>
> 人选C：
> 一面：
> 1、 问项目，mysql事务，spring 事务传播，mvcc
> 2、Zk分布式锁，原理
> 3、雪花算法原理，依赖的数据源可以用那些
> 4、MySQL 分库分表有哪些优缺点，怎么解决这个缺点(一致性哈希)
> 5、Es的索引原理，架构，有哪些性能优化手段，提高写入
> 6、Java线程池原理，参数，线程关闭
> 7、算法题，说的层次遍历
> 8、redis分布式锁，是否单线程，zset数据结构
> 二面：
> 主要是聊项目，项目的设计图，架构设计等
>
> 人选D:
> 一面：
>
> 1. synchronized 底层原理
> 2.mgr集群的数据一致性算法
> 3.violate 关键字的底层原理（native方法做了什么）
> 4.concurrenthashmap 的同步机制（jdk1.7和1.8的不同）
>
>  人选E：
> 一面：
> 1、问了数据库索引原理，b树和b➕ 树区别。
> 2、问了redis的淘汰策略， lru实现
> 3、算法题是树的层次遍历
> 4、Redis zset的数据结构
> 5、Maybatis#和$的区别
> 6、Redis锁失效的几种场景和处理方案

> 人选A：
> 一面：
> 1.数据库，事务特性，各种锁，各种优化
> 2.印象深的一个项目，技术难点，如何解决，如何优化
> 3.有没有架构思路分享，有没有什么方法论分享
> 4.手写加法计算器
> 二面：
> 1.分布式事务相关问题   
> 2.最近做过的项目，难点在哪里，如何解决，还有没有更优方案   
> 3.消息队列用过哪些，原理怎样，如何调优   
>
> > RabbitMQ 是一个广泛使用的开源消息代理软件，它基于 AMQP（Advanced Message Queuing Protocol）协议，支持多种消息传递模式，如发布/订阅、工作队列、路由等。RabbitMQ 的设计目标是提供一个可靠、灵活、高性能的消息传递平台。
> >
> > ### RabbitMQ 的工作原理
> >
> > RabbitMQ 的核心组件包括：
> >
> > 1. **Broker**（消息代理）：
> >    - RabbitMQ 服务器负责接收和转发消息。
> >    - Broker 是消息传递的核心。
> >
> > 2. **Exchange**（交换器）：
> >    - Exchange 负责接收来自生产者的消息，并将消息发送给队列。
> >    - 根据不同的类型（如 direct、fanout、topic 等），Exchange 会有不同的路由规则。
> >
> > 3. **Queue**（队列）：
> >    - Queue 是消息的容器，它保存消息直到消费者接收。
> >    - 每个消息都会被放入一个或多个队列中。
> >
> > 4. **Routing Key**（路由键）：
> >    - Routing Key 是一个字符串，用于绑定 Exchange 和 Queue，同时也用于匹配消息的路由。
> >    - 生产者发送消息时可以指定一个 routing key，Exchange 根据这个 key 将消息发送到对应的 Queue。
> >
> > 5. **Binding**（绑定）：
> >    - Binding 是 Exchange 和 Queue 之间的连接，定义了消息如何从 Exchange 路由到 Queue。
> >    - Binding 可以包含 routing key，用于精确地控制消息的流向。
> >
> > ### RabbitMQ 调优策略
> >
> > 调优 RabbitMQ 的目的是提高其性能、可靠性和可伸缩性。以下是一些调优建议：
> >
> > 1. **硬件优化**：
> >    - **内存**：增加内存可以提高缓存性能。
> >    - **CPU**：更多更快的 CPU 可以提高消息处理速度。
> >    - **网络**：高速网络可以减少网络延迟。
> >
> > 2. **配置调整**：
> >    - **磁盘使用**：考虑使用 SSD 以减少 I/O 延迟。
> >    - **文件系统**：使用高性能文件系统，如 ext4 或 XFS。
> >    - **持久化**：根据需要调整消息持久化的频率，以平衡性能和可靠性。
> >
> > 3. **网络配置**：
> >    - **TCP 参数**：调整 TCP 参数以提高网络传输效率。
> >    - **心跳间隔**：合理设置心跳间隔以减少不必要的连接重启。
> >
> > 4. **消息队列优化**：
> >    - **队列预取计数**：通过 `prefetch_count` 控制消费者预取的消息数量。
> >    - **消息确认模式**：使用 `ack` 确认模式以确保消息被正确处理。
> >    - **消息压缩**：对于大消息，可以启用压缩以减少网络带宽使用。
> >
> > 5. **交换器和绑定配置**：
> >    - **选择合适的交换器类型**：根据消息传递模式选择适当的交换器类型。
> >    - **绑定优化**：合理规划绑定关系以减少不必要的消息传递。
> >
> > 6. **监控和诊断**：
> >    - **启用监控**：使用 RabbitMQ 的内置监控工具或第三方工具来监控性能指标。
> >    - **日志分析**：定期分析日志以发现潜在的问题。
> >
> > 7. **集群和高可用性**：
> >    - **集群部署**：部署多个 RabbitMQ 节点以提高可伸缩性和容错能力。
> >    - **镜像队列**：使用镜像队列来实现跨节点的数据复制。
> >
> > 8. **负载均衡**：
> >    - **使用负载均衡器**：在 RabbitMQ 节点之间分配客户端连接以分散负载。
> >
> > 9. **客户端优化**：
> >    - **连接池**：使用连接池管理客户端与 RabbitMQ 的连接。
> >    - **异步处理**：对于密集型操作，使用异步 API 减少阻塞。
> >
> > ### 示例代码
> >
> > 下面是一个使用 Java 的 RabbitMQ 客户端示例，展示如何创建队列、发送消息和接收消息：
> >
> > ```java
> > import com.rabbitmq.client.*;
> > 
> > import java.io.IOException;
> > import java.nio.charset.StandardCharsets;
> > import java.util.concurrent.TimeoutException;
> > 
> > public class SimpleExample {
> > 
> >     private static final String QUEUE_NAME = "simple_queue";
> > 
> >     public static void main(String[] args) throws IOException, TimeoutException {
> >         ConnectionFactory factory = new ConnectionFactory();
> >         factory.setHost("localhost");
> >         Connection connection = factory.newConnection();
> >         Channel channel = connection.createChannel();
> > 
> >         // 创建队列
> >         channel.queueDeclare(QUEUE_NAME, false, false, false, null);
> > 
> >         // 发送消息
> >         String message = "Hello, RabbitMQ!";
> >         channel.basicPublish("", QUEUE_NAME, null, message.getBytes(StandardCharsets.UTF_8));
> > 
> >         // 接收消息
> >         DeliverCallback deliverCallback = (consumerTag, delivery) -> {
> >             String receivedMessage = new String(delivery.getBody(), StandardCharsets.UTF_8);
> >             System.out.println("Received '" + receivedMessage + "'");
> >         };
> >         channel.basicConsume(QUEUE_NAME, true, deliverCallback, consumerTag -> {});
> > 
> >         // 关闭连接
> >         channel.close();
> >         connection.close();
> >     }
> > }
> > ```
> >
> > ### 总结
> >
> > RabbitMQ 是一个功能强大且灵活的消息代理系统，通过合理的配置和调优，可以极大地提高其性能和可靠性。在实际应用中，应该根据具体的应用场景和需求来选择合适的配置和调优策略。
>
> 4.项目合作需要注意什么问题
>
> 人选B：
> 一面：
> 1、除了问项目，问了mysql事务，spring 事务传播，mvcc
>
> > https://www.cnblogs.com/renxiuxing/p/15395798.html
>
> 2、Zk和etcd区别
>
> > https://www.zhaowenyu.com/etcd-doc/best-practices/etcd-vs-zk.html
> >
> > ZooKeeper (ZK) 和 etcd 是两种非常流行的分布式协调服务工具，它们各自具有独特的特性和应用场景。下面是 ZK 和 etcd 的主要区别：
> >
> > ### ZooKeeper (ZK)
> > - **成熟度**：ZooKeeper 是一个成熟的分布式协调服务，由 Apache 基金会维护和支持，已经存在了很长时间，有着广泛的社区支持和丰富的文档资源。
> > - **数据模型**：ZooKeeper 使用类似于文件系统的层次结构来组织数据，每个节点（znode）都可以有子节点，并且每个节点可以包含数据。这使得它非常适合用于实现复杂的分布式场景。
> > - **事务性和顺序性**：ZooKeeper 支持原子性的更新操作，并且可以保证操作的顺序性。这意味着对某个节点的操作会被按顺序执行，这对于实现选举算法和分布式锁非常重要。
> > - **高可用性**：ZooKeeper 使用 Paxos 算法的一个变种来实现数据的一致性和高可用性，即使部分节点失败，集群仍然可以正常工作。
> > - **应用场景**：ZooKeeper 经常被用于解决分布式系统中的一些基础问题，如服务发现、配置管理、命名服务、集群管理、分布式锁、队列管理等。
> >
> > ### etcd
> > - **简洁性**：etcd 是一个相对简单的键值存储系统，专注于提供分布式键值存储服务。它由 CoreOS 开发和维护。
> > - **数据模型**：etcd 使用键值对的形式来存储数据，没有像 ZooKeeper 那样的层次结构。这使得它的使用更为简单直接，但同时也限制了一些复杂的功能。
> > - **一致性**：etcd 使用 Raft 一致性算法来保证数据的一致性。Raft 算法比 Paxos 更容易理解和实现，因此在实践中更容易部署和维护。
> > - **高可用性**：etcd 也支持集群模式，可以容忍一部分成员失败而保持可用。
> > - **应用场景**：etcd 主要用于简单的服务发现、配置管理和状态存储。它是 Kubernetes 等容器管理系统中用来存储集群状态的主要工具之一。
> >
> > ### 总结
> > - **ZooKeeper** 更适合于需要复杂协调逻辑的应用场景，例如需要实现分布式锁、选举等。
> > - **etcd** 则更适合于需要简单键值存储和配置管理的场景，特别是在容器化环境中。
> >
> > ### 选择建议
> > - 如果你需要一个高度可扩展和强大的协调服务，ZooKeeper 可能是一个更好的选择。
> > - 如果你的需求比较简单，并且希望使用一个轻量级且易于部署的工具，那么 etcd 可能更适合你。
> >
> > ### 注意事项
> > - 两者都有一定的学习曲线，特别是 ZooKeeper 的复杂性可能会让初学者感到困惑。
> > - 在选择使用哪一个工具时，还需要考虑到现有团队的技术栈、维护成本以及未来的扩展需求等因素。
> >
> > 希望这些信息对你有所帮助！如果你有任何具体的问题或者需要进一步了解的地方，请随时告诉我。
>
> 3、雪花算法原理，依赖的数据源可以用那些
>
> > 雪花算法（Snowflake ID Generator）是一种**生成全局唯一ID的算法**，最初由 Twitter 开发并用于其大规模分布式系统中。雪花算法的主要特点是能够高效地生成全局唯一的长整型数字，同时这些数字是递增的，这有助于提高数据库的写入性能。
> >
> > ### 雪花算法原理
> > 雪花算法的基本原理是将一个64位的长整型数字分成几个部分，每部分代表不同的含义：
> > - **时间戳**（41位）：表示从某个时间点开始到现在的毫秒数。由于是41位，因此可以支持的时间跨度大约为69年（从2010年到2079年左右）。
> > - **机器标识**（10位）：表示生成该ID的机器ID。由于是10位，因此最多可以支持1024台机器。
> > - **序列号**（12位）：表示同一毫秒内产生的不同ID的序列号。由于是12位，因此每毫秒可以生成的最大ID数量为4096个。
> >
> > ### 雪花算法的工作流程
> > 1. **初始化时间基准**：选择一个固定的时间点作为基准时间，通常选择项目开始的时间或者某个特定日期。
> > 2. **获取当前时间戳**：获取当前时间相对于基准时间的毫秒数。
> > 3. **生成机器ID**：为每台机器分配一个唯一的ID。
> > 4. **生成序列号**：对于每毫秒内生成的每个ID，分配一个唯一的序列号。
> > 5. **拼接生成ID**：将时间戳、机器ID和序列号拼接起来形成最终的64位ID。
> >
> > ### 依赖的数据源
> > 雪花算法依赖于以下几个数据源：
> >
> > 1. **时间源**：通常使用系统时钟作为时间源，但是需要注意的是，系统时钟必须保持准确且不能回拨，否则会导致生成的ID出现问题。
> > 2. **机器ID**：机器ID通常是预先分配的，并且需要保证在分布式环境中全局唯一。机器ID可以基于以下几种数据源来生成：
> >    - **MAC地址**：使用机器的物理地址（MAC地址）的一部分作为机器ID。
> >    - **IP地址**：使用IP地址的一部分作为机器ID。
> >    - **自定义分配**：手动为每台机器分配一个ID，例如通过配置文件或数据库记录。
> > 3. **序列号生成器**：序列号通常是在内存中维护的一个计数器，每次生成ID时递增。序列号可以存储在一个本地变量中，或者在多线程环境下使用原子操作来保证线程安全。
> >
> > ### 实现注意事项
> > - **时钟回拨**：为了避免时钟回拨导致的问题，可以在算法中加入检查机制，如果检测到当前时间小于上一次生成ID的时间，则等待或者抛出异常。
> > - **序列号溢出**：每毫秒内生成的ID数量有限制，如果超过最大序列号，需要等到下一个毫秒才能继续生成新的ID。
> > - **高可用性**：为了保证高可用性，可以部署多个生成节点，并且确保每个节点都能正确生成唯一的ID。
> >
> > ### 示例代码
> > 下面是一个简单的Java实现示例：
> >
> > ```java
> > import java.time.Instant;
> > 
> > public class SnowflakeIdGenerator {
> >     private final long workerId;
> >     private final long sequence;
> >     private long lastTimestamp = -1L;
> > 
> >     private static final long EPOCH = 1288834974657L; // 2010-11-04 00:00:00 UTC
> >     private static final int WORKER_ID_BITS = 10;
> >     private static final int SEQUENCE_BITS = 12;
> >     private static final long MAX_WORKER_ID = ~(-1L << WORKER_ID_BITS);
> >     private static final long MAX_SEQUENCE = ~(-1L << SEQUENCE_BITS);
> >     private static final long WORKER_ID_SHIFT = SEQUENCE_BITS;
> >     private static final long TIMESTAMP_LEFT_SHIFT = SEQUENCE_BITS + WORKER_ID_BITS;
> > 
> >     public SnowflakeIdGenerator(long workerId, long sequence) {
> >         if (workerId > MAX_WORKER_ID || workerId < 0) {
> >             throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0", MAX_WORKER_ID));
> >         }
> >         if (sequence > MAX_SEQUENCE || sequence < 0) {
> >             throw new IllegalArgumentException(String.format("sequence can't be greater than %d or less than 0", MAX_SEQUENCE));
> >         }
> >         this.workerId = workerId;
> >         this.sequence = sequence;
> >     }
> > 
> >     public synchronized long nextId() {
> >         long timestamp = timeGen();
> > 
> >         if (timestamp < lastTimestamp) {
> >             throw new RuntimeException(String.format("Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp));
> >         }
> > 
> >         if (lastTimestamp == timestamp) {
> >             sequence = (sequence + 1) & MAX_SEQUENCE;
> >             if (sequence == 0) {
> >                 timestamp = tilNextMillis(lastTimestamp);
> >             }
> >         } else {
> >             sequence = 0;
> >         }
> > 
> >         lastTimestamp = timestamp;
> > 
> >         return ((timestamp - EPOCH) << TIMESTAMP_LEFT_SHIFT) |
> >                (workerId << WORKER_ID_SHIFT) |
> >                sequence;
> >     }
> > 
> >     protected long tilNextMillis(long lastTimestamp) {
> >         long timestamp = timeGen();
> >         while (timestamp <= lastTimestamp) {
> >             timestamp = timeGen();
> >         }
> >         return timestamp;
> >     }
> > 
> >     protected long timeGen() {
> >         return Instant.now().toEpochMilli();
> >     }
> > }
> > ```
> >
> > 在这个示例中，我们使用了 `Instant.now().toEpochMilli()` 方法来获取当前时间戳，这是一个较为简单的实现。在实际生产环境中，你可能需要考虑更多的细节，比如时钟同步、跨数据中心的ID分配等问题。
> >
> > 希望这些信息对你有所帮助！如果你有任何其他问题或需要进一步的信息，请随时告诉我。
>
> 4、MySQL 分库分表有哪些优缺点，怎么解决这个缺点(一致性哈希)
>
> > MySQL的分库分表结合一致性哈希（Consistent Hashing）是一种常用的水平扩展技术，它可以有效地解决数据增长带来的性能瓶颈问题，并提高系统的可扩展性和可用性。下面我将详细介绍如何使用一致性哈希来进行MySQL的分库分表，以及这种方法的优点和挑战。
> >
> > ### 一致性哈希原理
> > 一致性哈希是一种特殊的哈希算法，**用于解决分布式环境中数据分布和节点变化的问题**。一致性哈希算法的主要特点包括：
> > - **数据分布均匀**：确保数据在多个节点间均匀分布。
> > - **节点变化平滑**：当节点加入或离开时，只有少量的数据需要重新分布，而不会影响到大部分数据。
> >
> > ### MySQL分库分表与一致性哈希
> > 在MySQL的分库分表中使用一致性哈希可以实现以下目标：
> > - **数据分布**：将数据均匀分布到多个数据库实例中。
> > - **节点变化**：当数据库实例增加或减少时，尽量减少数据迁移的数量。
> > - **查询定位**：能够快速定位到数据所在的数据库实例。
> >
> > ### 实现步骤
> > 以下是使用一致性哈希进行MySQL分库分表的一般步骤：
> >
> > 1. **定义分片键**：选择一个合适的字段作为分片键，这个字段应该是数据分布的关键字段，例如用户ID。
> >
> > 2. **一致性哈希环**：
> >    - 创建一个**虚拟的哈希环，将所有数据库实例映射到这个环上**。
> >    - 对于每个数据库实例，可以使用其名称或其他唯一标识符通过一致性哈希算法计算出一个位置。
> >    - 可以为每个数据库实例添加多个虚拟节点（称为虚拟节点复制），以提高数据分布的均匀性。
> >
> > 3. **数据映射**：
> >    - 对于每个需要插入或查询的数据项，使用一致性哈希算法计算出一个位置。
> >    - 根据计算出的位置，找到距离该位置最近的顺时针方向上的数据库实例，即为数据应该存储的位置。
> >
> > 4. **节点变化**：
> >    - 当数据库实例加入或离开时，只需重新计算受影响的虚拟节点的位置，将相关数据迁移到新的位置即可。
> >
> > ### 示例代码
> > 下面是一个简单的Java实现示例，用于演示如何使用一致性哈希算法来决定数据应该存储在哪个数据库实例上：
> >
> > ```java
> > import java.security.MessageDigest;
> > import java.security.NoSuchAlgorithmException;
> > import java.util.Arrays;
> > import java.util.Collections;
> > import java.util.HashMap;
> > import java.util.Map;
> > import java.util.TreeMap;
> > 
> > public class ConsistentHashing {
> > 
> >     private Map<Integer, String> virtualNodes = new TreeMap<>();
> >     private Map<String, Integer> virtualNodeCount = new HashMap<>();
> > 
> >     public void addDatabase(String databaseName, int replicationFactor) {
> >         try {
> >             MessageDigest md = MessageDigest.getInstance("MD5");
> >             byte[] hash = md.digest(databaseName.getBytes());
> >             int digest = ByteBuffer.wrap(hash).getInt();
> >             for (int i = 0; i < replicationFactor; i++) {
> >                 virtualNodes.put(digest + i, databaseName);
> >             }
> >             virtualNodeCount.put(databaseName, replicationFactor);
> >         } catch (NoSuchAlgorithmException e) {
> >             e.printStackTrace();
> >         }
> >     }
> > 
> >     public String getDatabase(String key) {
> >         try {
> >             MessageDigest md = MessageDigest.getInstance("MD5");
> >             byte[] hash = md.digest(key.getBytes());
> >             int digest = ByteBuffer.wrap(hash).getInt();
> >             Map.Entry<Integer, String> entry = virtualNodes.ceilingEntry(digest);
> >             if (entry == null) {
> >                 entry = virtualNodes.firstEntry();
> >             }
> >             return entry.getValue();
> >         } catch (NoSuchAlgorithmException e) {
> >             e.printStackTrace();
> >             return null;
> >         }
> >     }
> > 
> >     public static void main(String[] args) {
> >         ConsistentHashing hashing = new ConsistentHashing();
> >         hashing.addDatabase("db1", 3);
> >         hashing.addDatabase("db2", 3);
> >         hashing.addDatabase("db3", 3);
> > 
> >         System.out.println("Virtual Nodes: " + hashing.virtualNodes);
> > 
> >         String db1 = hashing.getDatabase("user1");
> >         String db2 = hashing.getDatabase("user1000");
> > 
> >         System.out.println("User1 should be stored in: " + db1);
> >         System.out.println("User1000 should be stored in: " + db2);
> >     }
> > }
> > ```
> >
> > ### 说明
> > - 在这个示例中，我们使用了MD5哈希算法来计算虚拟节点和数据项的位置。
> > - 每个数据库实例添加了3个虚拟节点，以提高数据分布的均匀性。
> > - `getDatabase` 方法返回给定数据项应该存储的数据库实例名称。
> >
> > ### 优缺点
> > #### 优点
> > - **数据分布均匀**：一致性哈希可以确保数据在多个数据库实例之间均匀分布。
> > - **平滑扩展**：当添加或移除数据库实例时，只需要重新计算少量的数据位置，减少了数据迁移的开销。
> > - **查询定位快**：通过哈希计算可以快速定位数据所在的数据库实例。
> >
> > #### 缺点
> > - **哈希冲突**：虽然概率很小，但在极端情况下可能会出现哈希冲突。
> > - **数据偏斜**：如果选择的分片键分布不均匀，可能会导致数据偏斜。
> > - **实现复杂度**：一致性哈希算法的实现相比简单的取模分片更加复杂。
> >
> > ### 结论
> > 使用一致性哈希进行MySQL分库分表是一种有效的水平扩展策略，尤其是在需要动态扩展数据库实例的情况下。通过上述实现方法，你可以根据实际需求调整虚拟节点的数量和其他参数，以达到最佳的性能和扩展效果。
> >
> > 
> >
> > 一致性哈希（Consistent Hashing）是一种分布式哈希算法，它解决了**在分布式环境中节点动态变化时，如何高效地重新分布数据的问题**。一致性哈希算法特别适用于构建可扩展的分布式缓存系统、负载均衡方案和分布式存储系统等场景。
> >
> > ### 一致性哈希的基本思想
> >
> > 一致性哈希的主要思想是**将数据和节点映射到一个固定的环形空间（通常是 0 到 2^32-1 的整数空间），并通过哈希函数将数据项和节点映射到这个空间上**。这样，**当数据项或节点发生变化时，只需要局部调整映射关系，而不需要全局重分布数据**。
> >
> > ### 一致性哈希的特点
> >
> > 1. **节点变化的影响最小**：
> >    - 当一个节点加入或离开时，只有位于该节点周围的键受到影响，而不是整个系统中的所有键。
> >
> > 2. **负载均衡**：
> >    - 通过虚拟节点技术可以达到较好的负载均衡效果。
> >
> > 3. **简单高效**：
> >    - 一致性哈希算法相对简单，易于理解和实现。
> >
> > 4. **可预测性**：
> >    - 通过一定的算法设计，可以预测哪些节点将受到影响。
> >
> > ### 一致性哈希的工作流程
> >
> > 1. **哈希环**：
> >    - 创建一个虚拟的哈希环，将所有的节点映射到环上。
> >
> > 2. **数据映射**：
> >    - 使用哈希函数将数据映射到哈希环上，**数据将存储在顺时针方向距离它最近的节点上**。
> >
> > 3. **虚拟节点**：
> >    - 为了提高负载均衡效果，每个物理节点可以对应多个虚拟节点，这样可以更均匀地分布数据。
> >
> > 4. **节点加入和离开**：
> >    - 当新节点加入时，它将接管环上一部分数据。
> >    - 当节点离开时，它所负责的数据将被转移给下一个节点。
> >
> > ### 一致性哈希的实现细节
> >
> > 1. **哈希函数**：
> >    - 选择一个合适的哈希函数，确保数据和节点均匀分布在整个哈希环上。
> >    - **常见的哈希函数有 MD5、SHA-1** 等。
> >
> > 2. **虚拟节点**：
> >    - 通过为每个物理节点创建多个虚拟节点，可以改善负载均衡。
> >    - 每个虚拟节点都有一个唯一的标识符，通常由物理节点标识符加上一个数字组成。
> >
> > 3. **数据迁移**：
> >    - 当节点加入或离开时，只需要将受影响的数据迁移到新的节点，而不需要全局重分布数据。
> >
> > 4. **故障恢复**：
> >    - 可以通过复制数据到多个节点来提高系统的容错能力。
> >
> > ### 总结
> >
> > **一致性哈希算法是一种高效且实用的分布式哈希方案，特别适用于节点动态变化的分布式系统。通过合理的设计和实现，可以有效地解决节点加入或离开时数据重新分布的问题，同时保持较好的负载均衡**。
>
> 5、Es的索引原理，架构，有哪些性能优化手段，提高写入
>
> > Elasticsearch (ES) 是一个分布式的、RESTful风格的搜索和分析引擎，它基于Lucene构建，主要用于全文检索、实时分析和大数据处理。下面我会详细介绍ES的索引原理、架构以及一些性能优化手段，特别是提高写入效率的方法。
> >
> > ### Elasticsearch 索引原理
> >
> > #### 索引过程
> > 1. **文档**：Elasticsearch 中的基本单位是文档（document），文档是JSON格式的数据。
> > 2. **索引**：索引是一组文档的集合，类似于关系数据库中的“表”。
> > 3. **分词**：当文档被索引时，文本内容会被分词器切分成一系列的词条（token）。
> > 4. **倒排索引**：Elasticsearch 使用倒排索引来存储文档中的词条及其出现的位置。倒排索引是以词条为键，文档ID列表为值的映射。
> >
> > #### 存储结构
> > - **分片**（Shard）：每个索引可以被分成多个分片，每个分片都是一个完整的Lucene索引。
> > - **副本**（Replica）：为了提高可靠性和可用性，每个分片可以拥有一个或多个副本。
> > - **主分片**（Primary Shard）：每个索引至少有一个主分片，数据写入时首先写入主分片，然后复制到副本分片。
> >
> > ### Elasticsearch 架构
> >
> > #### 节点和集群
> > - **节点**（Node）：Elasticsearch 的每个服务器实例被称为一个节点。
> > - **集群**（Cluster）：由一个或多个节点组成的集合，共享一组配置文件。
> > - **主节点**（Master Node）：集群中的一个特殊节点，负责集群范围内的元数据操作（例如索引创建和删除）。
> >
> > #### 数据流
> > 1. **写入**：当文档被写入时，数据首先写入主分片，然后复制到副本分片。
> > 2. **刷新**：为了提高写入性能，Elasticsearch 使用缓冲区来暂存写入的数据，定期刷新到磁盘。
> > 3. **合并**：随着新数据的不断写入，Lucene 会产生多个小的段文件。Elasticsearch 定期执行合并操作来减少这些段文件的数量。
> >
> > ### 性能优化手段
> >
> > #### 提高写入效率
> > 1. **批量写入**：将多个文档打包成一个请求发送到Elasticsearch，减少网络往返次数。
> > 2. **使用bulk API**：Elasticsearch 提供了专门的 bulk API 来支持批量写入。
> > 3. **调整索引设置**：
> >    - 减少刷新间隔：默认情况下，Elasticsearch 每隔1秒钟自动刷新一次索引。可以通过设置 `index.refresh_interval` 参数来调整刷新频率。
> >    - 关闭自动刷新：在高吞吐量写入场景下，可以关闭自动刷新，而是手动触发刷新。
> > 4. **使用缓冲区**：通过调整 `indices.memory.min_shard_size` 设置，增加缓冲区大小，以减少磁盘I/O操作。
> > 5. **禁用不必要的功能**：例如关闭source或metadata的存储，减少索引大小。
> > 6. **使用异步写入**：利用Elasticsearch的异步特性，可以在后台异步处理写入操作。
> > 7. **优化分片数量**：合理设置初始分片数量，避免过多分片导致的性能开销。
> >
> > ### 示例代码
> > 下面是一个使用Java客户端进行批量写入的例子：
> >
> > ```java
> > import org.elasticsearch.action.bulk.BulkRequest;
> > import org.elasticsearch.action.bulk.BulkResponse;
> > import org.elasticsearch.client.RequestOptions;
> > import org.elasticsearch.client.RestHighLevelClient;
> > 
> > import java.io.IOException;
> > import java.util.ArrayList;
> > import java.util.List;
> > 
> > public class ElasticsearchBulkWriter {
> >     private RestHighLevelClient client;
> > 
> >     public ElasticsearchBulkWriter(RestHighLevelClient client) {
> >         this.client = client;
> >     }
> > 
> >     public void writeDocuments(List<Map<String, Object>> documents, String indexName) throws IOException {
> >         BulkRequest bulkRequest = new BulkRequest();
> >         for (Map<String, Object> document : documents) {
> >             bulkRequest.add(new IndexRequest(indexName).source(document));
> >         }
> > 
> >         BulkResponse bulkResponse = client.bulk(bulkRequest, RequestOptions.DEFAULT);
> >         if (bulkResponse.hasFailures()) {
> >             System.out.println("Some of the documents failed to be indexed.");
> >         } else {
> >             System.out.println("All documents were indexed successfully.");
> >         }
> >     }
> > 
> >     public static void main(String[] args) {
> >         // 初始化客户端
> >         RestHighLevelClient client = new RestHighLevelClient(...); // 配置客户端
> >         ElasticsearchBulkWriter writer = new ElasticsearchBulkWriter(client);
> > 
> >         // 准备数据
> >         List<Map<String, Object>> documents = new ArrayList<>();
> >         documents.add(Map.of("title", "Document 1", "content", "This is the first document."));
> >         documents.add(Map.of("title", "Document 2", "content", "This is the second document."));
> > 
> >         try {
> >             writer.writeDocuments(documents, "documents-index");
> >         } catch (IOException e) {
> >             e.printStackTrace();
> >         }
> >     }
> > }
> > ```
> >
> > ### 总结
> > Elasticsearch 的索引原理基于倒排索引，并且通过分片和副本机制来提高可伸缩性和可靠性。通过上述性能优化手段，特别是针对写入效率的优化措施，可以显著提高Elasticsearch在高并发写入场景下的性能表现。在实际应用中，还需要根据具体的业务场景和技术要求来调整配置和优化策略。
>
> 6、Java基础
> 7、手写lru，还有一个线程题
>
> > https://leetcode.cn/problems/lru-cache/
> >
> > ```java
> > class LRUCache {
> >     int cap;
> >     LinkedHashMap<Integer, Integer> cache = new LinkedHashMap<>();
> >     public LRUCache(int capacity) { 
> >         this.cap = capacity;
> >     }
> >     
> >     public int get(int key) {
> >         if (!cache.containsKey(key)) {
> >             return -1;
> >         }
> >         // 将 key 变为最近使用
> >         makeRecently(key);
> >         return cache.get(key);
> >     }
> >     
> >     public void put(int key, int val) {
> >         if (cache.containsKey(key)) {
> >             // 修改 key 的值
> >             cache.put(key, val);
> >             // 将 key 变为最近使用
> >             makeRecently(key);
> >             return;
> >         }
> >         
> >         if (cache.size() >= this.cap) {
> >             // 链表头部就是最久未使用的 key
> >             int oldestKey = cache.keySet().iterator().next();
> >             cache.remove(oldestKey);
> >         }
> >         // 将新的 key 添加链表尾部
> >         cache.put(key, val);
> >     }
> >     
> >     private void makeRecently(int key) {
> >         int val = cache.get(key);
> >         // 删除 key，重新插入到队尾
> >         cache.remove(key);
> >         cache.put(key, val);
> >     }
> > }
> > ```
> >
> > 
>
> 人选C：
> 一面：
> 1、 问项目，mysql事务，spring 事务传播，mvcc
> 2、Zk分布式锁，原理
> 3、雪花算法原理，依赖的数据源可以用那些
> 4、MySQL 分库分表有哪些优缺点，怎么解决这个缺点(一致性哈希)
> 5、Es的索引原理，架构，有哪些性能优化手段，提高写入
> 6、Java线程池原理，参数，线程关闭
> 7、算法题，说的层次遍历
>
> > ```java
> > class Solution {
> >     public List<List<Integer>> levelOrder(TreeNode root) {
> >         // 使用队列实现
> >         List<List<Integer>> res = new ArrayList<List<Integer>>();
> >         if(root == null) return res;
> >         Queue<TreeNode> queue = new LinkedList<TreeNode>();
> >         queue.offer(root);
> >         while(!queue.isEmpty()) {
> >             List<Integer> level = new ArrayList<Integer>();
> >             int currentLevelSize = queue.size();
> >             for(int i = 1; i <= currentLevelSize; i++) {
> >                 TreeNode node = queue.poll();
> >                 level.add(node.val);
> >                 if(node.left != null) {
> >                     queue.offer(node.left);
> >                 }
> >                 if(node.right != null) {
> >                     queue.offer(node.right);
> >                 }
> >             }
> >             res.add(level);
> >         }
> >         return res;
> >     }
> > }
> > ```
> >
> > 
>
> 8、redis分布式锁，是否单线程，zset数据结构
>
> > https://xiaolincoding.com/redis/data_struct/command.html#zset
> >
> > Redis 是一个单线程的服务器。这意味着在任何一个给定的时间点，Redis 只能处理一个客户端请求。尽管 Redis 是单线程的，但它通过高效的事件驱动架构和非阻塞性 I/O 操作能够处理大量的并发客户端请求。
> >
> > ### Redis 的单线程特性
> >
> > 1. **事件驱动架构**：
> >    - Redis 使用事件驱动架构来处理客户端连接和请求。它监听客户端的连接请求，并使用非阻塞 I/O 操作来读取和写入数据。
> >
> > 2. **非阻塞 I/O**：
> >    - Redis 利用非阻塞 I/O 机制来处理客户端请求。当 Redis 接收到客户端请求时，它会将请求放入队列，并继续处理其他请求。一旦请求准备好处理，Redis 将从队列中取出请求并执行。
> >
> > 3. **内存操作**：
> >    - Redis 将所有数据存储在内存中，这意味着数据访问非常快。由于数据在内存中，Redis 可以非常快速地执行各种数据结构操作。
> >
> > 4. **多路复用器**：
> >    - Redis 使用多路复用器（如 epoll 或 kqueue）来处理多个客户端连接。多路复用器可以让 Redis 同时监听多个客户端连接，并在有数据到达时通知 Redis。
> >
> > ### Redis 的并发处理能力
> >
> > 尽管 Redis 是单线程的，但它仍然能够高效地处理大量并发请求。以下是 Redis 处理并发的一些关键特性：
> >
> > 1. **非阻塞性 I/O**：
> >    - Redis 使用非阻塞性 I/O，这意味着它可以同时处理多个客户端连接而不会被阻塞。
> >
> > 2. **内存操作**：
> >    - Redis 将所有数据存储在内存中，这意味着数据操作非常快。
> >
> > 3. **事件驱动架构**：
> >    - Redis 的事件驱动架构允许它快速响应客户端请求，并能够高效地处理高并发场景。
> >
> > 4. **异步操作**：
> >    - Redis 支持异步操作，如持久化到磁盘（如 RDB 和 AOF），这些操作不会阻塞主线程。
> >
> > 5. **多线程支持**：
> >    - Redis 6.0 引入了对多线程的支持，以提高某些复杂操作的性能，如字符串和哈希操作。然而，这些多线程操作是针对特定命令的，并且仍然保持了 Redis 的单线程模型不变。
> >
> > ### Redis 的多线程特性
> >
> > 从 Redis 6.0 版本开始，Redis 引入了对多线程的支持，以提高数据密集型操作的性能。这些多线程操作是针对特定命令的，旨在提高 CPU 密集型操作的性能，而 Redis 的主要请求处理仍然是单线程的。
> >
> > 1. **多线程命令**：
> >    - Redis 6.0 支持使用多线程处理某些命令，如字符串操作和哈希操作。
> >    - 这些命令可以通过配置参数 `io-threads-do-reads` 和 `io-threads` 来启用。
> >
> > 2. **命令调度**：
> >    - Redis 会根据命令的类型决定是否使用多线程来执行。例如，某些 CPU 密集型操作可能会被调度到单独的线程中执行。
> >
> > 3. **主处理线程**：
> >    - Redis 仍然保留了一个主处理线程来处理客户端请求和调度命令到多线程中执行。
> >
> > ### 总结
> >
> > - **Redis 是单线程的**：Redis 的核心请求处理是单线程的，这意味着在任何时候只能处理一个客户端请求。
> > - **并发处理能力**：通过高效的事件驱动架构和非阻塞 I/O 操作，Redis 能够处理大量的并发客户端请求。
> > - **多线程支持**：从 Redis 6.0 开始，Redis 引入了对多线程的支持，以提高某些 CPU 密集型操作的性能，但这不影响 Redis 的单线程请求处理模型。
> >
> > 总之，Redis 的单线程模型保证了数据的一致性和简单性，同时通过多线程支持提高了某些操作的性能。
>
> 二面：
> 主要是聊项目，项目的设计图，架构设计等
>
> 人选D:
> 一面：
>
> 1、synchronized 底层原理
>
> > https://xiaolincoding.com/interview/juc.html#synchronized%E5%92%8Creentrantlock%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF
>
> 2、mgr集群的数据一致性算法
>
> > paxos
>
> 3、violate 关键字的底层原理（native方法做了什么）
>
> > ### volatile 关键字
> >
> > `volatile` 是 Java 中的一个关键字，用于声明一个变量在多线程环境下需要保证可见性和禁止指令重排。当一个变量被声明为 `volatile` 时，它具备以下两个特性：
> >
> > 1. **可见性**：当一个线程修改了 `volatile` 变量的值，其他线程能够立即看到这个变化。
> > 2. **禁止指令重排**：编译器和处理器不会对 `volatile` 变量相关的代码进行指令重排。
> >
> > ### volatile 的底层原理
> >
> > `volatile` 变量的底层实现涉及了处理器的内存屏障（memory barrier）和内存一致性模型。
> >
> > 1. **内存屏障**：
> >    - 内存屏障是一种特殊的指令，它可以确保在屏障之前的指令不会被重排到屏障之后。
> >    - 在 `volatile` 变量的读写操作前后，JVM 会插入内存屏障来保证操作的顺序性。
> >    - 这些屏障确保了 `volatile` 变量的读写操作不会被编译器或处理器重排，从而保证了可见性和有序性。
> >
> > 2. **内存一致性模型**：
> >    - 不同的处理器架构有不同的内存一致性模型。
> >    - `volatile` 变量的实现依赖于特定处理器架构的内存一致性模型。
> >    - 例如，在 x86 架构中，`volatile` 变量的读写操作相当于一个内存屏障，确保了内存访问的顺序性。
> >
> > ### native 方法的作用
> >
> > Java 中的 native 方法通常是指那些使用其他语言（如 C 或 C++）编写的代码，这些代码通常被用来实现一些底层的、高性能的操作。在 `volatile` 变量的上下文中，native 方法并不直接与 `volatile` 相关，但它们可以间接地支持 `volatile` 变量的底层实现。
> >
> > ### volatile 和 native 方法的关系
> >
> > `volatile` 变量的底层实现主要依赖于 JVM 的实现，而不是直接依赖于 native 方法。然而，在某些情况下，native 方法可以用来支持与 `volatile` 变量相关的一些操作，例如：
> >
> > 1. **原子类**：
> >    - Java 提供了一系列的原子类（如 `AtomicInteger`、`AtomicLong` 等），这些类内部使用了 native 方法来实现原子操作。
> >    - 这些原子操作通常涉及到对 `volatile` 变量的读写操作，以保证多线程环境下的原子性和可见性。
> >
> > 2. **Unsafe 类**：
> >    - `sun.misc.Unsafe` 类提供了一些 native 方法，用于执行低级别的操作，如直接内存访问。
> >    - `Unsafe` 类中的方法可以用来直接操作 `volatile` 变量，例如 `Unsafe.putOrderedInt` 和 `Unsafe.getAndAddInt` 等方法。
> >
> > ### 示例代码
> >
> > 下面是一个使用 `volatile` 变量和 `Unsafe` 类的简单示例，展示了如何直接操作 `volatile` 变量：
> >
> > ```java
> > import sun.misc.Unsafe;
> > 
> > public class VolatileExample {
> > 
> >     private static final Unsafe UNSAFE;
> >     private static final long VALUE_OFFSET;
> > 
> >     static {
> >         try {
> >             UNSAFE = Unsafe.getUnsafe();
> >             VALUE_OFFSET = UNSAFE.objectFieldOffset
> >                 (VolatileExample.class.getDeclaredField("value"));
> >         } catch (Exception e) {
> >             throw new Error(e);
> >         }
> >     }
> > 
> >     private volatile int value;
> > 
> >     public static void main(String[] args) {
> >         VolatileExample example = new VolatileExample();
> >         example.value = 0;
> > 
> >         // 使用 Unsafe 直接修改 volatile 变量
> >         UNSAFE.putOrderedInt(example, VALUE_OFFSET, 1);
> > 
> >         System.out.println("Value after putOrderedInt: " + example.value);
> >     }
> > }
> > ```
> >
> > ### 总结
> >
> > - **volatile** 关键字用于声明一个变量在多线程环境下需要**保证可见性和禁止指令重排**。
> > - `volatile` 变量的底层实现涉及**内存屏障**和特定处理器架构的**内存一致性模型**。
> > - native 方法可以间接地支持 `volatile` 变量的相关操作，例如通过原子类或 `Unsafe` 类中的方法。
> > - 在 Java 中，`volatile` 变量的实现主要依赖于 JVM 的实现，而不是直接依赖于 native 方法。
>
> 4、concurrenthashmap 的同步机制（jdk1.7和1.8的不同）
>
> > `ConcurrentHashMap` 是 Java 中一个高性能的线程安全的哈希表实现，它在 Java 1.7 和 Java 1.8 中有着不同的内部实现机制。下面分别介绍这两个版本中 `ConcurrentHashMap` 的同步机制。
> >
> > ### Java 1.7 中的 ConcurrentHashMap
> >
> > 在 Java 1.7 中，`ConcurrentHashMap` 使用了一种基于分段锁（segment lock）的实现方式。它将整个哈希表分为多个段（segments），每个段都使用一个锁来保护该段内的元素。这种设计可以减少锁的粒度，从而提高并发性能。
> >
> > 1. **分段锁**：
> >    - `ConcurrentHashMap` 被划分为多个段（segments），每个段都包含了一个哈希表。
> >    - 每个段都有一个锁（ReentrantLock），用于保护该段内的哈希表。
> >    - 这种设计减少了锁的粒度，从而提高了并发性能。
> >
> > 2. **哈希表**：
> >    - 每个段中的哈希表使用类似于 `HashMap` 的实现。
> >    - 当哈希冲突发生时，节点会链接成链表形式。
> >
> > 3. **分割数量**：
> >    - 默认分割数量为 16，可以通过构造函数传入自定义的分割数量。
> >
> > 4. **操作锁**：
> >    - 当需要修改一个段中的元素时，只锁定该段的锁。
> >    - 当需要遍历或查找元素时，可以同时访问多个段，从而提高并发性能。
> >
> > ### Java 1.8 中的 ConcurrentHashMap
> >
> > Java 1.8 中的 `ConcurrentHashMap` 改进了 Java 1.7 中的实现，采用了更加细粒度的锁和更高效的内存利用率。
> >
> > 1. **细粒度锁**：
> >    - Java 1.8 中的 `ConcurrentHashMap` 使用了**更细粒度的锁，每个桶（bucket）都有一个锁**。
> >    - 这意味着**当多个线程访问不同的桶时，它们可以并行操作，从而提高并发性能**。
> >
> > 2. **数据结构**：
> >    - Java 1.8 中的 `ConcurrentHashMap` 使用了树化机制来减少链表长度。
> >    - 当链表长度超过一定阈值时，链表会被转换为红黑树以提高查找性能。
> >    - 当树的节点数小于一定阈值时，红黑树会被转换回链表。
> >
> > 3. **CAS 操作**：
> >    - Java 1.8 中的 `ConcurrentHashMap` 在许多操作中使用了 **CAS（Compare and Swap）操作来实现无锁化**。
> >    - CAS 操作可以减少锁的使用，提高并发性能。
> >
> > 4. **分段锁的移除**：
> >    - Java 1.8 中不再使用分割锁，而是使用了更细粒度的锁来提高性能。
> >
> > ### 示例代码
> >
> > 下面是一个使用 Java 1.8 的 `ConcurrentHashMap` 的简单示例：
> >
> > ```java
> > import java.util.concurrent.ConcurrentHashMap;
> > 
> > public class ConcurrentHashMapExample {
> > 
> >     public static void main(String[] args) {
> >         ConcurrentHashMap<String, Integer> map = new ConcurrentHashMap<>();
> > 
> >         // 添加元素
> >         map.put("one", 1);
> >         map.put("two", 2);
> >         map.put("three", 3);
> > 
> >         // 获取元素
> >         Integer value = map.get("two");
> >         System.out.println("Value of 'two': " + value);
> > 
> >         // 更新元素
> >         map.computeIfPresent("two", (key, oldValue) -> oldValue + 1);
> >         value = map.get("two");
> >         System.out.println("Updated value of 'two': " + value);
> > 
> >         // 删除元素
> >         map.remove("two");
> >         value = map.get("two");
> >         System.out.println("Value of 'two' after removal: " + value);
> >     }
> > }
> > ```
> >
> > ### 总结
> >
> > - **Java 1.7** 中的 `ConcurrentHashMap` 使用了基于分段锁的实现方式，将哈希表分为多个段，每个段使用一个锁来保护该段内的元素。
> > - **Java 1.8** 中的 `ConcurrentHashMap` 采用了更细粒度的锁和更高效的数据结构，如树化机制和 CAS 操作，以提高并发性能和内存利用率。
> >
> > 这两种实现方式都有其优势，但 Java 1.8 中的改进使得 `ConcurrentHashMap` 成为一个更加强大和高效的并发集合。
>
>  人选E：
> 一面：
> 1、问了数据库索引原理，b树和b➕ 树区别。
>
> > 第一：在B树中，非叶子节点和叶子节点都会存放数据，而B+树的所有的数据都会出现在叶子节点，在查询的时候，B+
> >
> > 树查找效率更加稳定
> >
> > 第二：在进行范围查询的时候，B+树效率更高，因为B+树都在叶子节点存储，并且叶子节点是一个双向链表
>
> 2、问了redis的淘汰策略， lru实现
>
> > **注意淘汰策略和过期策略不一样**
> >
> > Redis 的淘汰策略是指当内存使用达到一定限制时，Redis 采取的处理策略来释放内存。Redis 提供了几种不同的淘汰策略，以便用户可以根据自己的需求选择最适合的策略。下面详细介绍 Redis 的淘汰策略以及 LRU（Least Recently Used）淘汰策略的具体实现。
> >
> > ### Redis 的淘汰策略
> >
> > 1. **noeviction**：
> >    - 这是默认的淘汰策略。当 Redis 达到内存限制时，如果需要分配更多内存，Redis 将返回一个错误，而不是驱逐任何键。
> >    - 这种策略可以保证数据的一致性，但可能会导致操作失败。
> >
> > 2. **allkeys-lru**：
> >    - 当 Redis 达到内存限制时，它会根据 LRU 算法删除最近最少使用的键。
> >    - 这是最常用的淘汰策略之一。
> >
> > 3. **allkeys-random**：
> >    - 当 Redis 达到内存限制时，随机选择一个键进行删除。
> >    - 这种策略不考虑键的使用频率。
> >
> > 4. **volatile-lru**：
> >    - 当 Redis 达到内存限制时，它会根据 LRU 算法删除设置了过期时间的键中最近最少使用的键。
> >    - 这种策略仅删除设置了过期时间的键。
> >
> > 5. **volatile-random**：
> >    - 当 Redis 达到内存限制时，随机选择设置了过期时间的键进行删除。
> >    - 这种策略仅删除设置了过期时间的键。
> >
> > 6. **volatile-ttl**：
> >    - 当 Redis 达到内存限制时，删除最近将要过期的键。
> >    - 这种策略仅删除设置了过期时间的键。
> >
> > ### LRU 淘汰策略的实现
> >
> > LRU 淘汰策略是 Redis 中常用的策略之一，它根据键的最近使用情况进行淘汰。下面详细介绍 LRU 淘汰策略的实现细节。
> >
> > #### LRU 淘汰策略的实现
> >
> > Redis 中的 LRU 淘汰策略是通过近似的 LRU 算法实现的，而不是精确的 LRU。这是因为在大规模数据集中，维护精确的 LRU 记录会导致额外的开销和复杂性。Redis 采用了一种近似的 LRU 算法，通过以下步骤实现：
> >
> > 1. **维护一个 LRU 计数器**：
> >    - Redis 维护了一个 LRU 计数器，每当服务器执行一次命令时，该计数器就会增加。
> >    - 这个计数器可以看作是一个时间戳，用于近似表示键的最后访问时间。
> >
> > 2. **为每个键分配一个 LRU 计数器值**：
> >    - 当一个键被访问时，它的 LRU 计数器值会被更新为当前的 LRU 计数器值。
> >    - 这个计数器值被存储在键的元数据中。
> >
> > 3. **选择淘汰的键**：
> >    - 当 Redis 需要释放内存时，它会选择 LRU 计数器值较小的键进行淘汰。
> >    - 由于 LRU 计数器值较小表示键较长时间没有被访问，因此这些键被视为最近最少使用的键。
> >
> > 4. **近似 LRU**：
> >    - Redis 的 LRU 实现是一种近似算法，因为它只考虑了键的最近访问时间，而没有精确跟踪每个键的访问次数。
> >    - 这种近似算法在大多数情况下足够好，同时避免了维护精确 LRU 记录所带来的额外开销。
> >
> > ### 示例
> >
> > 下面是一个简单的示例，展示如何在 Redis 中设置淘汰策略：
> >
> > ```bash
> > # 连接到 Redis 服务器
> > redis-cli
> > 
> > # 设置内存限制和淘汰策略
> > CONFIG SET maxmemory 100mb
> > CONFIG SET maxmemory-policy allkeys-lru
> > ```
> >
> > 在上面的示例中，我们首先连接到 Redis 服务器，然后使用 `CONFIG SET` 命令设置内存限制为 100MB，并设置淘汰策略为 `allkeys-lru`。
> >
> > ### 总结
> >
> > - **Redis 的淘汰策略** 包括 noeviction、allkeys-lru、allkeys-random、volatile-lru、volatile-random 和 volatile-ttl。
> > - **LRU 淘汰策略** 是通过近似的 LRU 算法实现的，维护一个 LRU 计数器，并为每个键分配一个 LRU 计数器值。
> > - **近似 LRU** 算法简化了实现，并且在大多数情况下足够好。
> >
> > 通过合理选择和配置淘汰策略，可以确保 Redis 在内存受限的情况下依然能够提供稳定的服务。
>
> 3、算法题是树的层次遍历
> 4、Redis zset的数据结构
> 5、Maybatis#和$的区别
>
> > MyBatis 是一个流行的持久层框架，它支持 SQL 映射和动态 SQL。在 MyBatis 中，`#` 和 `$` 符号用于处理 SQL 语句中的动态参数。这两者在 MyBatis 中的使用方式和含义有所不同。
> >
> > ### #（参数占位符）
> >
> > 当使用 `#` 符号时，MyBatis 会**将参数视为预编译语句（PreparedStatement）中的占位符**。这种方式可以**防止 SQL 注入攻击**，并且在大多数情况下是推荐使用的。
> >
> > #### 语法示例
> >
> > ```xml
> > <select id="selectUserById" parameterType="int" resultType="com.example.User">
> >     SELECT * FROM users WHERE id = #{id}
> > </select>
> > ```
> >
> > #### 说明
> >
> > - `#{id}` 表示参数 `id` 将被用作 SQL 语句中的占位符。
> > - MyBatis 会自动将参数值转换为预编译语句中的适当类型。
> > - 这种方式可以有效地防止 SQL 注入攻击。
> >
> > ### $（字符串替换）
> >
> > 当使用 `$` 符号时，MyBatis 会**将参数直接替换为字符串值**。这种方式**不会进行预编译**，而是**将参数值直接拼接到 SQL 语句中**。这种方式**容易导致 SQL 注入攻击**，因此一般不推荐使用。
> >
> > #### 语法示例
> >
> > ```xml
> > <select id="selectUserById" parameterType="int" resultType="com.example.User">
> >     SELECT * FROM users WHERE id = ${id}
> > </select>
> > ```
> >
> > #### 说明
> >
> > - `${id}` 表示参数 `id` 的值将被直接插入到 SQL 语句中。
> > - 这种方式**不会进行预编译，因此可能存在 SQL 注入的风险**。
> > - 通常用于动态 SQL 语句中的表名或列名等场景。
> >
> > ### 动态 SQL
> >
> > 在 MyBatis 中，还可以使用 `<if>`、`<choose>`、`<when>`、`<otherwise>` 等标签来构建动态 SQL 语句。在动态 SQL 中，`#` 和 `$` 的使用规则同样适用。
> >
> > #### 动态 SQL 示例
> >
> > ```xml
> > <select id="findUsersByName" parameterType="java.lang.String" resultType="com.example.User">
> >     SELECT * FROM users
> >     <where>
> >         <if test="name != null">
> >             AND name LIKE '%${name}%'
> >         </if>
> >     </where>
> > </select>
> > ```
> >
> > #### 说明
> >
> > - `<if>` 标签用于根据条件决定是否包含特定的 SQL 子句。
> > - `${name}` 表示参数 `name` 的值将被直接插入到 SQL 语句中。
> > - 由于这里使用了 `$` 符号，需要注意 SQL 注入的风险。
> >
> > ### 总结
> >
> > - **#（参数占位符）**：
> >   - 使用 `#` 符号可以防止 SQL 注入。
> >   - 适用于大多数参数传递场景。
> >   - 推荐使用。
> >
> > - **$（字符串替换）**：
> >   - 使用 `$` 符号会直接将参数值插入到 SQL 语句中。
> >   - 存在 SQL 注入的风险。
> >   - 仅在需要动态构建 SQL 语句（如表名或列名）时使用。
> >
> > 在大多数情况下，推荐使用 `#` 符号来传递参数，以确保 SQL 语句的安全性和性能。如果确实需要使用 `$` 符号，请确保对参数值进行适当的验证和转义处理，以避免 SQL 注入攻击。
>
> 6、Redis锁失效的几种场景和处理方案
>
> > https://blog.csdn.net/ZDK_csdn/article/details/122487945
> >
> > ****
>
> mysql两阶段提交
>
> > https://cloud.tencent.com/developer/article/1965127
> >
> > 1. 写入 redo log，处于 prepare 状态。
> > 2. 写 binlog。
> > 3. 修改 redo log 状态变为 commit。
> >
> > ### **2. 为什么需要两阶段提交**
> >
> > 如果没有两阶段提交，那么 binlog 和 redolog 的提交，无非就是两种形式：
> >
> > 1. 先写 binlog 再写 redolog。
> > 2. 先写 redolog 再写 binlog。
> >
> > 这两种情况我们分别来看。
> >
> > 假设我们要向表中插入一条记录 R，如果是先写 binlog 再写 redolog，那么假设 binlog 写完后崩溃了，此时 redolog 还没写。那么重启恢复的时候就会出问题：binlog 中已经有 R 的记录了，当从机从主机同步数据的时候或者我们使用 binlog 恢复数据的时候，就会同步到 R 这条记录；但是 redolog 中没有关于 R 的记录，所以崩溃恢复之后，插入 R 记录的这个事务是无效的，即数据库中没有该行记录，这就造成了数据不一致。
> >
> > 相反，假设我们要向表中插入一条记录 R，如果是先写 redolog 再写 binlog，那么假设 redolog 写完后崩溃了，此时 binlog 还没写。那么重启恢复的时候也会出问题：redolog 中已经有 R 的记录了，所以崩溃恢复之后，插入 R 记录的这个事务是有效的，通过该记录将数据恢复到数据库中；但是 binlog 中还没有关于 R 的记录，所以当从机从主机同步数据的时候或者我们使用 binlog 恢复数据的时候，就不会同步到 R 这条记录，这就造成了数据不一致。
> >
> > 那么按照前面说的两阶段提交就能解决问题吗？
> >
> > 我们来看如下三种情况：
> >
> > **情况一：**一阶段提交之后崩溃了，即`写入 redo log，处于 prepare 状态` 的时候崩溃了，此时：
> >
> > 由于 binlog 还没写，redo log 处于 prepare 状态还没提交，所以崩溃恢复的时候，这个事务会回滚，此时 binlog 还没写，所以也不会传到备库。
> >
> > **情况二：**假设写完 binlog 之后崩溃了，此时：
> >
> > redolog 中的日志是不完整的，处于 prepare 状态，还没有提交，那么恢复的时候，首先检查 binlog 中的事务是否存在并且完整，如果存在且完整，则直接提交事务，如果不存在或者不完整，则回滚事务。
> >
> > **情况三：**假设 redolog 处于 commit 状态的时候崩溃了，那么重启后的处理方案同情况二。
> >
> > 由此可见，两阶段提交能够确保数据的一致性。

# 拼多多1面

8.19 20:00 ~ 21:00

介绍项目

问了业务相关的

## 数据流转流程、平台侧架构等

## 解释策略模式，上下文环境

> 策略模式：SFTP文件清洗加工落表，很多个SFTP文件判断产生很多if-else，使用策略模式根据文件名前缀获取对应处理bean，消除if-else判断
>
> 上下文环境：基础指标计算上下游依赖关系，净值->收益率->年化收益率->波动率->年化波动率->夏普比率->年化夏普比率，计算完成得到基础指标上下文环境，后续进一步计算衍生指标，直接获取上下文，或许需要的指标进而计算衍生指标。

## 项目中涉及到数据处理比较多，谈谈实际项目中使用的数据库。

> 其实不太知道面试官想了解哪方面？说了索引优化、数据库和缓存双写一致性、数据库和ES双写一致性

## 场景设计题：根据现实QQ的实际场景设计一个QQ，用户亿级别，

1、根据用户手机号、邮箱、身份证号等去注册；QQ号一串数字，递增。

2、用户根据手机号、邮箱、QQ号进行登录；

3、注册的时候跳过靓号。

> https://segmentfault.com/a/1190000039422231

> 设计一个亿级别用户的 QQ 系统是一项复杂的工程挑战，需要考虑高并发、高可用、数据一致性等多个方面。下面我将根据您的要求，设计一个基本的架构方案。（问的通义千问）
>
> ### 1. 系统架构概述
>
> 为了支撑亿级别的用户量，我们需要构建一个可扩展、高性能、分布式的系统架构。这里将采用微服务架构，并使用多种分布式技术来确保系统的稳定性和扩展性。
>
> #### 技术栈
>
> - **后端语言**: Java
> - **微服务框架**: Spring Cloud
> - **消息队列**: Kafka
> - **数据库**: MySQL + Redis
> - **缓存**: Redis
> - **负载均衡**: Nginx
> - **API 网关**: Zuul 或 Spring Cloud Gateway
> - **服务注册与发现**: Eureka 或 Consul
> - **分布式事务**: Seata 或 TCC
> - **API 文档**: Swagger
> - **容器化**: Docker
> - **持续集成/持续部署**: Jenkins
>
> ### 2. 功能模块设计
>
> #### 2.1 用户注册模块
>
> - **手机号注册**:
>   - 验证手机号唯一性。
>   - 发送短信验证码。
>   - 用户同意服务条款后提交注册信息。
>   - 生成 QQ 号码（递增方式）。
>   - 跳过靓号（特定号码序列）。
>
> - **邮箱注册**:
>   - 验证邮箱唯一性。
>   - 发送邮件验证码。
>   - 用户同意服务条款后提交注册信息。
>   - 生成 QQ 号码。
>   - 跳过靓号。
>
> - **身份证号注册**:
>   - 验证身份证号唯一性。
>   - 实名验证。
>   - 用户同意服务条款后提交注册信息。
>   - 生成 QQ 号码。
>   - 跳过靓号。
>
> #### 2.2 用户登录模块
>
> - **手机号登录**:
>   - 输入手机号和密码。
>   - 验证码登录（可选）。
>   - 登录成功返回 Token。
>
> - **邮箱登录**:
>   - 输入邮箱地址和密码。
>   - 验证码登录（可选）。
>   - 登录成功返回 Token。
>
> - **QQ 号登录**:
>   - 输入 QQ 号和密码。
>   - 验证码登录（可选）。
>   - 登录成功返回 Token。
>
> ### 3. 技术实现细节
>
> #### 3.1 数据库设计
>
> - **用户表** (`users`):
>   - `id`: 主键，自增长。
>   - `qq_number`: QQ 号码，唯一。
>   - `phone`: 手机号，唯一。
>   - `email`: 邮箱地址，唯一。
>   - `password`: 密码，加密存储。
>   - `registration_method`: 注册方式（手机号、邮箱、身份证）。
>   - `is_active`: 是否激活账户。
>   - `created_at`: 创建时间。
>   - `updated_at`: 更新时间。
>
> - **靓号表** (`handsome_numbers`):
>   - `number`: 靓号，唯一。
>   - `status`: 状态（已注册、未注册）。
>
> #### 3.2 分布式 ID 生成器
>
> - **使用雪花算法** (`Snowflake`) 生成全局唯一的 ID 作为 QQ 号码。
> - **跳过靓号**:
>   - 在生成 QQ 号码时，查询靓号表中的号码，确保不使用靓号。
>
> #### 3.3 分布式缓存
>
> - **Redis** 用于缓存频繁访问的数据，例如用户登录状态、验证码等。
>
> #### 3.4 分布式事务
>
> - **Seata** 或 **TCC** 用于处理跨服务的事务。
>
> #### 3.5 负载均衡
>
> - **Nginx** 作为反向代理，将请求分发到各个服务实例。
>
> #### 3.6 微服务通信
>
> - **RESTful API** 或 **gRPC** 用于服务间的通信。
>
> ### 4. 示例代码
>
> #### 4.1 用户注册服务
>
> ```java
> import org.springframework.web.bind.annotation.PostMapping;
> import org.springframework.web.bind.annotation.RequestBody;
> import org.springframework.web.bind.annotation.RestController;
> 
> @RestController
> public class RegistrationController {
> 
>     private UserService userService;
> 
>     @PostMapping("/register/phone")
>     public ResponseEntity registerByPhone(@RequestBody PhoneRegistrationRequest request) {
>         userService.registerByPhone(request.getPhoneNumber(), request.getPassword());
>         return ResponseEntity.ok().build();
>     }
> 
>     @PostMapping("/register/email")
>     public ResponseEntity registerByEmail(@RequestBody EmailRegistrationRequest request) {
>         userService.registerByEmail(request.getEmail(), request.getPassword());
>         return ResponseEntity.ok().build();
>     }
> 
>     @PostMapping("/register/idcard")
>     public ResponseEntity registerByIdCard(@RequestBody IdCardRegistrationRequest request) {
>         userService.registerByIdCard(request.getIdCardNumber(), request.getPassword());
>         return ResponseEntity.ok().build();
>     }
> }
> ```
>
> #### 4.2 用户服务
>
> ```java
> import org.springframework.stereotype.Service;
> 
> @Service
> public class UserService {
> 
>     private UserRepository userRepository;
> 
>     public void registerByPhone(String phoneNumber, String password) {
>         if (!userRepository.isPhoneNumberUnique(phoneNumber)) {
>             throw new IllegalStateException("Phone number already exists.");
>         }
>         Long qqNumber = generateQqNumber();
>         User user = new User(qqNumber, phoneNumber, password);
>         userRepository.save(user);
>     }
> 
>     private Long generateQqNumber() {
>         Snowflake snowflake = new Snowflake(0L, 0L); // 初始化雪花算法
>         Long qqNumber = snowflake.nextId();
>         // 跳过靓号
>         if (isHandsomeNumber(qqNumber)) {
>             return generateQqNumber(); // 递归生成直到找到非靓号
>         }
>         return qqNumber;
>     }
> 
>     private boolean isHandsomeNumber(Long number) {
>         // 查询靓号表，判断是否为靓号
>         return handsomeNumberRepository.existsById(number);
>     }
> }
> ```
>
> ### 5. 高可用和容灾
>
> - **多数据中心部署**:
>   - 在不同地理位置部署多个数据中心，确保服务的高可用性。
> - **主备切换**:
>   - 实现主备切换策略，当主数据中心出现问题时，自动切换到备用数据中心。
>
> ### 6. 监控与报警
>
> - **Prometheus** 和 **Grafana** 用于监控系统性能指标。
> - **ELK Stack** (Elasticsearch, Logstash, Kibana) 用于日志收集和分析。
> - **Alertmanager** 用于发送报警通知。
>
> ### 7. 总结
>
> 以上是一个针对亿级别用户的 QQ 系统的基本设计框架。在实际开发过程中还需要考虑更多细节和技术选型，例如安全性、性能优化等方面。这个设计方案可以根据具体的业务需求和技术背景进行调整和优化。

## 数据库和缓存双写一致性

## 解释线程池参数

> https://javaguide.cn/java/concurrent/java-thread-pool-summary.html
>
> `ThreadPoolExecutor` 3 个最重要的参数：
>
> - `corePoolSize` : 任务队列未达到队列容量时，最大可以同时运行的线程数量。
> - `maximumPoolSize` : 任务队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。
> - `workQueue`: 新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。
>
> `ThreadPoolExecutor`其他常见参数 :
>
> - `keepAliveTime`:线程池中的线程数量大于 `corePoolSize` 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 `keepAliveTime`才会被回收销毁。
> - `unit` : `keepAliveTime` 参数的时间单位。
> - `threadFactory` :executor 创建新线程的时候会用到。
> - `handler` :拒绝策略。
>
> 这里简单分析一下整个流程（对整个逻辑进行了简化，方便理解）：
>
> 1. 如果当前运行的线程数小于核心线程数，那么就会新建一个线程来执行任务。
> 2. 如果当前运行的线程数等于或大于核心线程数，但是小于最大线程数，那么就把该任务放入到任务队列里等待执行。
> 3. 如果向任务队列投放任务失败（任务队列已经满了），但是当前运行的线程数是小于最大线程数的，就新建一个线程来执行任务。
> 4. 如果当前运行的线程数已经等同于最大线程数了，新建线程将会使当前运行的线程超出最大线程数，那么当前任务会被拒绝，拒绝策略会调用`RejectedExecutionHandler.rejectedExecution()`方法。
>
> ![图解线程池实现原理](https://oss.javaguide.cn/github/javaguide/java/concurrent/thread-pool-principle.png)
>
> ​																					**图解线程池实现原理**

## 算法题：给出前序遍历和中序遍历的数组，输出后续遍历数组，写出来了

```java
// 大概实现如下：具体没有跑测试用例

List<Integer> ans = new ArrayList<>();
public List<Integer> postOrder(List<Integer> preOrder, List<Integer> inOrder) {
    Map<Integer, Integer> mp = new HashMap<>();
    for(int i = 0; i < inOrder.size(); i++) {
      mp.put(inOrder.get(i), i);
    }
    buildTree(preOrder, 0, inOrder, 0, inOrder.size() - 1);
    return ans;
}

private void buildTree(List<Integer> preOrder, int preStart, List<Integer> inOrder, int left, int right, Map<Integer, Integer> mp) {
    if(preStart >= preOrder.size()) {
        return ;
    }
    // 根节点
    int root = preOrder.get(preStart);
    // 根节点在中序遍历中的索引值
    int index = mp.get(root);
    int left = buildTree(preOrder, preStart + 1, inOrder, left, index - 1);
    int right = buildTree(preOrder, preStart + 1, inOrder, index + 1, right);
    ans.add(left);
    ans.add(right);
    ans.add(root);
}
```



对应力扣105题：[105. 从前序与中序遍历序列构造二叉树](https://leetcode.cn/problems/construct-binary-tree-from-preorder-and-inorder-traversal/?envType=study-plan-v2&envId=top-100-liked)

```java
/**
 * Definition for a binary tree node.
 * public class TreeNode {
 *     int val;
 *     TreeNode left;
 *     TreeNode right;
 *     TreeNode() {}
 *     TreeNode(int val) { this.val = val; }
 *     TreeNode(int val, TreeNode left, TreeNode right) {
 *         this.val = val;
 *         this.left = left;
 *         this.right = right;
 *     }
 * }
 */
class Solution {
    private Map<Integer, Integer> indexMap = new HashMap<>();

    public TreeNode buildTree(int[] preorder, int[] inorder) {
        int n = preorder.length;
        for(int i = 0; i < n; i++) {
            indexMap.put(inorder[i], i);
        }
        return helper(preorder, inorder, 0, n - 1, 0, n - 1);
    }

    public TreeNode helper(int[] preorder, int[] inorder, 
    int preorder_left, int preorder_right, int inorder_left,int inorder_right) {
        if(preorder_left > preorder_right) {
            return null;
        }

        // 前序遍历中的第一个节点就是根节点
        int preorder_root = preorder_left;
        // 在中序遍历中定位根节点
        int inorder_root = indexMap.get(preorder[preorder_root]);

        // 先把根节点建立出来
        TreeNode root = new TreeNode(preorder[preorder_root]);
        // 得到左子树中的节点数目
        int size_left_subtree = inorder_root - inorder_left;
        // 递归地构造左子树，并连接到根节点
        // 先序遍历中「从 左边界+1 开始的 size_left_subtree」个元素就对应了中序遍历中「从 左边界 开始到 根节点定位-1」的元素
        root.left = helper(preorder, inorder, preorder_left + 1, preorder_left + size_left_subtree, inorder_left, inorder_root - 1);
        // 递归地构造右子树，并连接到根节点
        // 先序遍历中「从 左边界+1+左子树节点数目 开始到 右边界」的元素就对应了中序遍历中「从 根节点定位+1 到 右边界」的元素
        root.right = helper(preorder, inorder, preorder_left + size_left_subtree + 1, preorder_right, inorder_root + 1, inorder_right);
        return root;
    }
}
```



你在杭州，多多在上海，为什么要找工作？

# 拼多多2面

8.21 11:00 ~ 11:54

主要聊项目，平台主要做什么的，干什么用的，聊业务，没问八股文

面试官打开了招行app，招银理财是不是收益不太好，用很久了感觉没什么变化，除了招行还卖别的公司的产品.......

为什么想要跳出来找工作

为什么在武汉上学，去杭州工作了，哪里人?

> 我说杭州互联网氛围更好，面试官：招行又不是互联网，我说机会更多，面试官：你是校招进入招行，又不是社招，谈不上机会多少，我说不太喜欢武汉的气候，面试官说没去过武汉

## 算法题：给定一个有序数组，除了一个元素只出现一次，其余元素出现两次，找出只出现一次的元素，例如[1,1,2,2,6,77,77]，找到6。

给出了三种解法，1、使用Map统计；2、使用异或^运算的特殊性质；3、使用二分查找，边界条件没写好，死循环了。

```java
// 二分查找，死循环了
class Solution {
    int target = -1;
    public int singleNonDuplicate(int[] nums) {
        binarySearch(nums, 0, nums.length);
        return target;
    }

    private void binarySearch(int[] nums, int left, int right) {
        if(left > right || left < 0 || left >= nums.length || right < 0 || right >= nums.length) {
            return ;
        }
        while(left <= right) {
            int mid = left + (right - left) / 2;
            if(mid - 1 >= 0 && mid + 1 <= right && nums[mid] != nums[mid - 1] && nums[mid] != nums[mid + 1]) {
                target = nums[mid];
                return ;
            }
            binarySearch(nums, left, mid - 1);
            binarySearch(nums, mid + 1, right);
        }
    }
}
```

> [LCR 070. 有序数组中的单一元素](https://leetcode.cn/problems/skFtm2/)
>
> [540. 有序数组中的单一元素](https://leetcode.cn/problems/single-element-in-a-sorted-array/)

是否了解拼多多，了解拼多多工作时间，在招行的工作时间是什么样子的？

# 同花顺

8.21 15:00 ~ 16:03

自我介绍

## 介绍项目亮点难点

> 从数据侧、数字化赋能侧两方面架构演进进行介绍的。
>
> 数据侧：随着业务发展，金融品类增加，数据任务越来越多，引入Dolphin Scheduler之前面临的问题和挑战，到引入DS解决了问题
>
> 赋能侧：从向行内B端输出，到向手机银行APP C端输出对原有系统架构的调整

## 介绍项目中的责任链模式，如何进行优化，尽可能通过并行执行来提高执行效率。

> 类似于，责任链中的每个节点有个标志位flag，如果当前节点执行成功就为true，还没有执行成功就为false，下游节点可以通过flag判断是否要进行并发执行，提高执行效率。
>
> https://blog.csdn.net/weixin_42762133/article/details/109628930

数据处理流程 balabala....

## 项目中mysql是怎么部署的？

> 采用MySQL MGR部署架构
>
> 自动完成主备切换和故障转移

# MySQL MGR 组复制

> https://blog.csdn.net/NICK_53/article/details/130332519
>
> ![image-20240825191758250](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825191758250.png)
>
> https://www.bilibili.com/video/BV1eV4y1D7fs/?spm_id_from=333.337.search-card.all.click&vd_source=d88527641a2c5d74436ce88743d2fd55
>
> https://www.bilibili.com/video/BV193411W7bF/?spm_id_from=333.788&vd_source=d88527641a2c5d74436ce88743d2fd55
>
> 
>
> ![image-20240825204458811](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204458811.png)
>
> ![image-20240825204521191](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204521191.png)
>
> ![image-20240825204621769](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204621769.png)
>
> ![image-20240825204425413](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825204425413.png)
>
> ![image-20240825205549978](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240825205549978.png)

## mysql主备切换和自动转移怎么做的？

> 1、基于mysql mgr架构，实现自动主备切换和故障转移；
>
> 2、业务代码中通过sentinel，主库访问异常的时候，降级到备库

## IOC

> https://cloud.tencent.com/developer/article/2373610

## MySQL主从复制原理

> https://cloud.tencent.com/developer/article/2051407
>
> ## 主从复制的原理
>
> ①当Master节点进行insert、update、delete操作时，会按顺序写入到binlog中。
>
> ②salve从库连接master主库，Master有多少个slave就会创建多少个binlog dump线程。
>
> ③当Master节点的binlog发生变化时，binlog dump 线程会通知所有的salve节点，并将相应的binlog内容推送给slave节点。
>
> ④I/O线程接收到 binlog 内容后，将内容写入到本地的 relay-log。
>
> ⑤SQL线程读取I/O线程写入的relay-log，并且根据 relay-log 的内容对从数据库做对应的操作。
>
> ![详细讲解mysql 主从复制原理「建议收藏」](https://ask.qcloudimg.com/http-save/yehe-8223537/8ed8de24e0e8bf0c94bbf9f8ae471ec9.png)

## mysql事务隔离级别、MVCC原理

> 读未提交（read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
> 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
> 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。这是MySQL InnoDB 引擎的默认隔离级别；
> 串行化（serializable ）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。
>
> 
>
> 其中mvcc的意思是多版本并发控制。指维护一个数据的多个版本，使得读写操作没有冲突，它的底层实现主要是分为了三个部分，第一个是隐藏字段，第二个是undo log日志，第三个是readView读视图
>
> 隐藏字段是指：在mysql中给每个表都设置了隐藏字段，有一个是trx_id(事务id)，记录每一次操作的事务id，是自增的；另一个字段是roll_pointer(回滚指针)，指向上一个版本的事务版本记录地址
>
> undo log主要的作用是记录回滚日志，存储老版本数据，在内部会形成一个版本链，在多个事务并行操作某一行记录，记录不同事务修改数据的版本，通过roll_pointer指针形成一个链表
>
> readView解决的是一个事务查询选择版本的问题，在内部定义了一些匹配规则和当前的一些事务id判断该访问那个版本的数据，不同的隔离级别快照读是不一样的，最终的访问的结果不一样。如果是rc隔离级别，每一次执行快照读时生成ReadView，如果是rr隔离级别仅在事务中第一次执行快照读时生成ReadView，后续复用。

## jetcache实际项目中使用、如何保证一致性。

jetcache基于Redis Pub/Sub，保证单节点更新或失效事件广播到分布式集群中的每个节点，使得所有节点jvm中的缓存失效，保证多级缓存的一致性。如果Redis的发布订阅出现问题了吗，怎么办？

> 基于Redis Pub/Sub，通过broadcastChannel和syscLocal设置，保证多级缓存一致性，是jetcache框架提供的能力，我们实际项目中依赖于jetcache框架提供的能力，没有额外针对redis的发布订阅去做进一步的高可用保证，当然想自己做也是可以的，jetcache提供了可扩展的接口，使用者可以自行实现，比如使用RabbitMQ自行构建HA高可用系统。

## 设计题：10个G的csv文件保存id，系统堆内存就1个G，统计文件中出现次数最多的10个id。

> 答了分批次、大顶堆
>
> 可以从大数据组件角度考虑
>
> https://www.cnblogs.com/zl1991/p/13109760.html
>
> 如一亿个Ip求Top  10，可先%1000将ip分到1000个小文件中去，**并保证一种ip只出现在一个文件中**，再对每个小文件中的ip进行hashmap计数统计并按数量排序，最后归并或者最小堆依次处理每个小文件的top10以得到最后的结果。

反问：负责业务、技术栈

# 百度一面

8. 22 14:00 ~ 15:00

自我介绍

介绍项目，项目中你负责的主要内容、技术栈是什么

## 项目中RabbitMQ使用场景、延迟队列怎么实现

> https://www.cnblogs.com/yaopengfei/p/18221824
>
> 基于死信交换机+ttl实现延迟队列
>
> **1. 目标**
>
>  生产者发送一个消息到队列，想让消费者延迟 n 秒后，再消费。
>
> **2. 实现原理**
>
>  **声明延迟队列 delayed_queue， 绑定死信交换机dead_exchange 和 死信队列dead_queue， 延迟队列中的消息到了ttl后，会通过死信交换机dead_exchange 进入 死信队列dead_queue，** **消费者最终是从 死信队列dead_queue 中消费消息。**
>
> **3. 缺点**
>
>  A. 仅仅支持在队列层次上实现ttl，不支持每条消息上实现不同的ttl，如果想在消息层次上实现ttl，需要安装下面的插件rabbitmq_delayed_message_exchange【实测装了也不好用】
>
>  B. **需要两套队列 和 交换机。**
>
> **4. 优点**
>
>  不需要按照任何插件

## spring事务传播机制

> https://blog.csdn.net/qq_27184497/article/details/116525588
>
> spring事务传播行为一共有7种：（前言： 当前事务指的是调用者自带的事务，A调用B，那么A就是当前事务）
>
>     REQUIRED  （默认传播行为），支持当前事务，如果当前没有事务，就新建一个事务，这个当前事务指的是上一个方法的事务，是别人传递过去的，类似于重入锁，A方法和B方法都有事务，A方法调用B方法，A的事务会传递给B，使它们共用同一个事务，我起了个名字叫做重入事务
>     SUPPORTS 如果存在一个事务，支持当前事务，如果没有事务，则非事务执行，
>     REQUIRES_NEW  开启一个新的事务。如果一个事务已经存在，则先将这个存在的事务挂起
>     MANDATORY  如果已经存在一个事务，支持当前事务。如果没有一个活动的事务，则抛出异常 
>     NOT_SUPPORTED  总是非事务地执行，并挂起任何存在的事务
>     NEVER   总是非事务地执行，不加入任何事务；
>     NESTED   如果一个活动的事务存在，则运行在一个嵌套的事务中。 如果没有活动事务, 则按  REQUIRED 属性执行。

## A方法调用B方法，B怎么知道A事务的？（答的Aop）

> 在 Java 中，当你在一个事务管理器（例如 Spring 的 `PlatformTransactionManager`）的作用范围内调用一个方法时，该方法将会在一个事务上下文中执行。如果方法 A 调用方法 B，并且方法 A 已经在一个事务中，那么方法 B 将会知道并加入这个事务。这种行为是通过 Spring AOP（Aspect-Oriented Programming）实现的。
>
> ### Spring 事务传播行为
>
> Spring 提供了一种基于 AOP 的事务管理模型，允许你在方法级别声明事务。Spring 通过 `@Transactional` 注解和 XML 配置等方式支持事务管理。当你在方法上声明了 `@Transactional` 注解时，Spring 会为该方法创建一个代理，这个代理会拦截方法调用，并根据注解上的配置来决定是否开启新的事务或者加入现有的事务。
>
> ### 示例代码
>
> 假设我们有两个服务类 `ServiceA` 和 `ServiceB`，并且 `ServiceA` 的 `methodA()` 调用了 `ServiceB` 的 `methodB()`。
>
> ```java
> import org.springframework.stereotype.Service;
> import org.springframework.transaction.annotation.Transactional;
> 
> @Service
> public class ServiceA {
> 
>     private final ServiceB serviceB;
> 
>     public ServiceA(ServiceB serviceB) {
>         this.serviceB = serviceB;
>     }
> 
>     @Transactional
>     public void methodA() {
>         System.out.println("Executing methodA in transaction.");
>         serviceB.methodB();
>     }
> }
> 
> @Service
> public class ServiceB {
> 
>     @Transactional
>     public void methodB() {
>         System.out.println("Executing methodB in the same transaction as methodA.");
>     }
> }
> ```
>
> ### 解释
>
> 1. **方法 A (`methodA`)**:
>    - 在 `ServiceA` 中定义。
>    - 使用 `@Transactional` 注解，这意味着当调用此方法时，它将在一个事务中执行。
>    - 方法内部调用了 `ServiceB` 的 `methodB()`。
>
> 2. **方法 B (`methodB`)**:
>    - 在 `ServiceB` 中定义。
>    - 同样使用 `@Transactional` 注解。
>    - 当 `methodA()` 调用 `methodB()` 时，`methodB()` 会加入 `methodA()` 的事务。
>
> ### 事务传播行为
>
> 这里的关键在于 `methodB()` 上的 `@Transactional` 注解，默认情况下它的传播行为是 `PROPAGATION_REQUIRED`。这意味着：
>
> - 如果当前存在事务，则 `methodB()` 会加入该事务。
> - 如果当前不存在事务，则 `methodB()` 会创建一个新的事务。
>
> 因为 `methodA()` 已经在一个事务中，所以当 `methodA()` 调用 `methodB()` 时，`methodB()` 将会加入 `methodA()` 的事务中。
>
> ### 事务传播行为的配置
>
> 如果你想要显式地配置 `methodB()` 的事务传播行为，你可以在 `@Transactional` 注解中指定 `propagation` 属性。例如，如果你想让 `methodB()` 创建一个新的事务，即使 `methodA()` 已经在一个事务中，你可以这样配置：
>
> ```java
> @Transactional(propagation = Propagation.REQUIRES_NEW)
> public void methodB() {
>     // ...
> }
> ```
>
> ### 总结
>
> 当方法 A 调用方法 B 时，方法 B 是通过 **Spring AOP 的代理机制**了解到方法 A 的事务上下文的。具体来说，**Spring 会为方法 B 创建一个代理对象，当方法 A 调用方法 B 时，实际上是调用了一个由 Spring 创建的代理对象，该代理对象会根据事务传播行为的配置来决定是否加入现有事务或创建新事务。通过这种方式，方法 B 可以知道方法 A 的事务上下文，并根据配置的行为来决定自己的事务行为。**

## AOP怎么实现的？JDK和Cglib动态代理有什么区别，生成的代理对象有什么区别？

> AOP（Aspect-Oriented Programming，面向切面编程）是一种编程范式，用于将横切关注点（cross-cutting concerns）从业务逻辑中分离出来。在Java中，最常用的AOP框架是Spring AOP，它提供了强大的AOP功能，可以很容易地集成到Spring应用程序中。
>
> ### Spring AOP 实现原理
>
> Spring AOP 主要通过两种方式实现 AOP：
>
> 1. **JDK 动态代理**：
>    - JDK 动态代理只能针对实现了接口的类创建代理对象。
>    - 它主要依赖于 `java.lang.reflect.Proxy` 类和 `java.lang.reflect.InvocationHandler` 接口。
>    - 通过动态代理，Spring 可以为任何实现了接口的类创建代理对象，这些代理对象可以拦截方法调用并执行额外的操作（如事务管理）。
>
> 2. **CGLIB 代理**：
>    - CGLIB（Code Generation Library）是一种高性能的字节码生成库，它可以为没有实现接口的类创建代理。
>    - CGLIB 通过继承被代理类来创建代理子类，因此可以对任何类创建代理，而不仅仅是实现了接口的类。
>    - 由于是通过继承实现的，因此不适用于不能被继承的类（比如最终类 `final`）。
>
> ### JDK 动态代理与 CGLIB 代理的区别
>
> 1. **接口要求**：
>    - **JDK 动态代理**：需要被代理的类实现至少一个接口。
>    - **CGLIB 代理**：不需要被代理的类实现接口，可以代理任意类（除了 `final` 类）。
>
> 2. **代理对象的创建**：
>    - **JDK 动态代理**：通过 `java.lang.reflect.Proxy` 类和 `InvocationHandler` 接口创建代理对象。
>    - **CGLIB 代理**：通过字节码技术生成被代理类的子类，从而实现代理。
>
> 3. **性能**：
>    - **JDK 动态代理**：通常比 CGLIB 快，因为它不需要创建子类。
>    - **CGLIB 代理**：可能稍微慢一些，因为它涉及到字节码操作和类加载。
>
> 4. **适用场景**：
>    - **JDK 动态代理**：适用于那些实现了接口的类。
>    - **CGLIB 代理**：适用于没有实现接口的类。
>
> 5. **生成的代理对象**：
>    - **JDK 动态代理**：生成的代理对象实现了与被代理对象相同的接口。
>    - **CGLIB 代理**：生成的代理对象是被代理对象的子类。
>
> ### 示例
>
> 下面是一个简单的示例，展示了如何使用 JDK 动态代理和 CGLIB 代理来创建代理对象。
>
> #### JDK 动态代理示例
>
> ```java
> import java.lang.reflect.InvocationHandler;
> import java.lang.reflect.Method;
> import java.lang.reflect.Proxy;
> 
> interface MyInterface {
>     void doSomething();
> }
> 
> class MyInterfaceImpl implements MyInterface {
>     @Override
>     public void doSomething() {
>         System.out.println("Doing something...");
>     }
> }
> 
> class MyInvocationHandler implements InvocationHandler {
>     private Object target;
> 
>     public MyInvocationHandler(Object target) {
>         this.target = target;
>     }
> 
>     @Override
>     public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
>         System.out.println("Before method call");
>         Object result = method.invoke(target, args);
>         System.out.println("After method call");
>         return result;
>     }
> }
> 
> public class JdkProxyExample {
>     public static void main(String[] args) {
>         MyInterface myInterface = new MyInterfaceImpl();
>         MyInterface proxy = (MyInterface) Proxy.newProxyInstance(
>                 MyInterface.class.getClassLoader(),
>                 new Class[]{MyInterface.class},
>                 new MyInvocationHandler(myInterface)
>         );
>         proxy.doSomething();
>     }
> }
> ```
>
> #### CGLIB 代理示例
>
> ```java
> import net.sf.cglib.proxy.Enhancer;
> import net.sf.cglib.proxy.MethodInterceptor;
> import net.sf.cglib.proxy.MethodProxy;
> 
> public class CglibProxyExample {
>     static class MyClass {
>         public void doSomething() {
>             System.out.println("Doing something...");
>         }
>     }
> 
>     static class MyMethodInterceptor implements MethodInterceptor {
>         @Override
>         public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable {
>             System.out.println("Before method call");
>             Object result = proxy.invokeSuper(obj, args);
>             System.out.println("After method call");
>             return result;
>         }
>     }
> 
>     public static void main(String[] args) {
>         MyClass myClass = new MyClass();
>         Enhancer enhancer = new Enhancer();
>         enhancer.setSuperclass(MyClass.class);
>         enhancer.setCallback(new MyMethodInterceptor());
>         MyClass proxy = (MyClass) enhancer.create();
>         proxy.doSomething();
>     }
> }
> ```
>
> ### 总结
>
> - **JDK 动态代理**：适用于实现了接口的类，性能较好，代理对象实现相同的接口。
> - **CGLIB 代理**：适用于所有类（除了 `final` 类），通过继承实现，适合未实现接口的类。
>
> Spring AOP 会自动选择最适合的代理方式。如果目标对象实现了接口，Spring 将使用 JDK 动态代理；如果没有实现接口，则使用 CGLIB 代理。这种选择机制使得开发人员无需关心具体的实现细节，只需关注业务逻辑即可。

## 事务隔离级别

## A方法调用B方法，B抛出异常，A事务会回滚吗？事务什么情况下会失效？

> 在Java中使用Spring框架时，事务管理通常是通过Spring的声明式事务管理来实现的。当一个方法（例如A方法）调用另一个方法（例如B方法）时，事务的传播行为决定了事务边界如何跨越这些方法调用。
>
> ### A方法调用B方法时事务的行为
>
> 假设A方法和B方法都处于同一个事务范围内，并且A方法调用了B方法：
>
> 1. **如果B方法抛出了未被标记为`@Transactional(readOnly=true)`的受检异常（checked exception），那么整个事务将会回滚。**
> 2. **如果B方法抛出了运行时异常（runtime exception）或错误（error），那么整个事务也会回滚。**
> 3. **如果B方法抛出了被标记为`@Transactional(readOnly=true)`的受检异常，那么事务不会回滚，除非配置了`rollbackFor`属性来指定特定类型的异常也应当触发回滚。**
>
> ### 事务失效的情况
>
> Spring的事务管理可能会在以下几种情况下失效：
>
> 1. **方法不在同一类中调用**：如果A方法调用的是另一个类中的B方法，而这两个类不在同一个事务范围中，则B方法抛出的异常不会影响A方法的事务状态。为了使事务生效，需要确保方法调用在事务管理的范围内。
>
> 2. **方法内部自我调用**：如果A方法内部调用了本类中的B方法，即使B方法上有`@Transactional`注解，也不会启动新的事务，除非该方法声明了`propagation=Propagation.REQUIRES_NEW`。这是因为Spring的事务管理是基于代理模式的，对于同一个类的方法调用，不会经过代理，因此事务管理器无法拦截这些调用。
>
> 3. **同步方法调用**：如果A方法是在B方法内部通过同步的方式调用的（例如，使用`new Thread`启动新线程并调用A方法），则B方法的事务不会扩展到A方法的调用中。这是因为事务管理器只管理当前线程的事务边界，而在新的线程中开始的方法调用将不在原始事务的范围内。
>
> 4. **配置问题**：如果事务管理器没有正确配置或者没有为特定的方法启用事务管理（例如，没有使用`@Transactional`注解或XML配置中未指定），事务将不会生效。
>
> 5. **异常处理不当**：如果方法**抛出的异常被不恰当地捕获并吞掉了**，那么事务管理器可能无法检测到异常的发生，这会导致事务不会回滚。为了保证事务能够正常回滚，应当**避免无条件地捕获异常而不抛出**。
>
> 6. **使用了`@Transactional(readOnly=true)`**：如果一个方法被标记为`@Transactional(readOnly=true)`，这意味着该方法预期只读取数据而不会修改数据。如果该方法确实执行了写操作并且抛出了受检异常，事务默认情况下不会回滚，除非指定了`rollbackFor`属性。
>
> ### 示例
>
> 假设有一个服务类`ServiceA`，其中包含两个方法`methodA`和`methodB`：
>
> ```java
> @Service
> public class ServiceA {
> 
>     @Transactional
>     public void methodA() {
>         try {
>             methodB();
>         } catch (Exception e) {
>             // 如果这里不抛出异常，事务将不会回滚
>             throw e; // 或者抛出一个新的 RuntimeException
>         }
>     }
> 
>     @Transactional
>     public void methodB() {
>         // 这里抛出一个运行时异常
>         throw new RuntimeException("An error occurred in methodB.");
>     }
> }
> ```
>
> 在这个例子中，如果`methodB`抛出了运行时异常，那么整个事务（包括`methodA`）都会回滚。但如果在`methodA`中捕获了这个异常而没有重新抛出，则事务可能不会按预期回滚。
>
> ### 总结
>
> - **如果B方法抛出异常**，并且该异常类型被事务管理器配置为应当触发回滚（通常是所有的未检查异常），那么整个事务（包括A方法）将回滚。
> - **事务可能失效**，如果方法不在同一个事务范围内调用、方法内部自我调用不当、配置错误、异常处理不当等情况出现。

> # spring 事务管理之只读事务@Transactional(readOnly = true)
>
> https://blog.csdn.net/jinwufeiyang/article/details/119300491

## mysql索引原理、B+数和B数区别？

> * B-树的每一个节点，存了关键字和对应的数据地址，而B+树的非叶子节点只存关键字，不存数据地址。因此B+树的每一个非叶子节点存储的关键字是远远多于B-树的，B+树的叶子节点存放关键字和数据，因此，从树的高度上来说，B+树的高度要小于B-树，使用的磁盘I/O次数少，因此查询会更快一些。
>
> * B-树由于每个节点都存储关键字和数据，因此离根节点近的数据，查询的就快，离根节点远的数据，查询的就慢；B+树所有的数据都存在叶子节点上，因此在B+树上搜索关键字，找到对应数据的时间是比较平均的，没有快慢之分
>
> * 在B-树上如果做区间查找，遍历的节点是非常多的；B+树所有叶子节点被连接成了有序链表结构，因此做整表遍历和区间查找是非常容易的
>   https://blog.csdn.net/lsz20000813/article/details/124512063

## 索引(a,b,c)，查询(a,c)，会走索引查询吗？ 使用in查询，索引命中情况？

> 最左匹配原则，a走索引查询，c不走
>
> https://deepinout.com/mysql/mysql-questions/86_tk_1704237349.html

## 怎么知道查询有没有命中索引？

> explain执行计划，看key和key_len字段
>
> https://blog.csdn.net/liruichuan/article/details/103575911

## 可重复读有什么问题（幻读）、什么是幻读、怎么解决

> https://cloud.tencent.com/developer/article/2419481
>
> https://xiaolincoding.com/mysql/transaction/phantom.html#%E5%B9%BB%E8%AF%BB%E8%A2%AB%E5%AE%8C%E5%85%A8%E8%A7%A3%E5%86%B3%E4%BA%86%E5%90%97
>
> 在数据库事务中，可重复读（Repeatable Read）是一种隔离级别，它保证了在同一事务中多次读取同样的数据时，结果是一致的。然而，这种隔离级别并不能完全避免所谓的“幻读”现象。
>
> ### 幻读
>
> **幻读**（Phantom Reads）是指在同一个事务中，多次执行相同的查询返回的结果集不一致的现象。具体来说，是指在事务中第一次读取某范围内的数据后，在事务还未提交前，有其他事务插入了一些新的记录，导致再次读取相同范围内的数据时，出现了第一次读取时不存在的新记录。
>
> #### 幻读示例
>
> 假设有一个表 `orders`，包含 `order_id` 和 `status` 两列，其中 `status` 表示订单的状态。现在有两个事务：事务 A 和事务 B。
>
> 1. **事务 A 开始**：
>    - 事务 A 执行 `SELECT * FROM orders WHERE status = 'NEW'`，获取到一些结果。
>    
> 2. **事务 B 开始**：
>    - 事务 B 插入一条新的记录，其中 `status` 为 `'NEW'`。
>
> 3. **事务 A 再次查询**：
>    - 事务 A 再次执行 `SELECT * FROM orders WHERE status = 'NEW'`，这时它会看到事务 B 新插入的那条记录。
>
> 4. **事务 A 提交**：
>    - 事务 A 提交。
>
> 在这个过程中，事务 A 第二次查询时看到了一条新的记录，这就是幻读。
>
> ### 可重复读下的幻读问题
>
> 在可重复读隔离级别下，虽然事务 A 第一次读取数据时不会看到事务 B 插入的新记录，但在事务 A 提交之前，如果事务 B 插入了新的记录，并且事务 A 再次执行相同的查询，那么事务 A 就可以看到这些新记录。这是**因为可重复读隔离级别不会锁定整个表或范围，而是对每个读取的行进行锁定**。
>
> ### 如何解决幻读问题
>
> 1. **使用更高隔离级别**：
>    - 使用 **串行化**（Serializable）隔离级别可以完全避免幻读。在串行化级别下，事务会锁定整个表或范围，确保在事务执行期间不会有任何其他事务对该表或范围进行修改。
>    - 但串行化级别的代价是降低了并发能力，可能导致更多的等待和锁争用。
>
> 2. **数据库特定的技术**：
>    - **MySQL InnoDB 引擎** 使用 **Next-Key Locking**（临检锁）来防止幻读。这是一种特殊的锁定机制，它不仅锁定查询结果集中的行，还锁定所有可能插入到这些行之间的间隙。
>    - Next-Key Locking 实质上是记录锁（Record Locks）和间隙锁（Gap Locks）的组合，它可以有效防止幻读，同时保持较高的并发性能。
>
> 3. **应用程序层面的解决方案**：
>    - 在某些情况下，可以通过在应用程序中增加额外的逻辑来避免幻读。例如，可以增加额外的过滤条件，或者在事务开始时锁定所需的范围。
>
> ### 示例
>
> 假设你正在使用 MySQL 的 InnoDB 存储引擎，你可以通过设置事务隔离级别来避免幻读：
>
> ```sql
> -- 设置事务隔离级别为可重复读
> SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
> 
> BEGIN;
> 
> -- 查询某个范围内的数据
> SELECT * FROM orders WHERE status = 'NEW';
> 
> -- 其他事务插入新记录
> 
> -- 再次查询相同的范围
> SELECT * FROM orders WHERE status = 'NEW';
> 
> COMMIT;
> ```
>
> 在这个例子中，由于使用了可重复读隔离级别，并且 MySQL InnoDB 使用了 Next-Key Locking，事务 A 在第二次查询时不会看到事务 B 新插入的记录，从而避免了幻读。
>
> ### 总结
>
> - **幻读** 是指在事务中执行相同的查询时，第二次查询看到了第一次查询时不存在的新记录。
> - **可重复读** 隔离级别可以避免读取到其他事务修改的数据，但在某些情况下仍然可能出现幻读。
> - **解决幻读的方法** 包括使用更高的隔离级别（如串行化）或采用数据库特定的技术（如 MySQL InnoDB 的 Next-Key Locking）。
> - 选择合适的隔离级别和锁定机制对于平衡并发性能和数据一致性是非常重要的。

## MVCC

> 其中mvcc的意思是多版本并发控制。指维护一个数据的多个版本，使得读写操作没有冲突，它的底层实现主要是分为了三个部分，第一个是隐藏字段，第二个是undo log日志，第三个是readView读视图
>
> 隐藏字段是指：在mysql中给每个表都设置了隐藏字段，有一个是trx_id(事务id)，记录每一次操作的事务id，是自增的；另一个字段是roll_pointer(回滚指针)，指向上一个版本的事务版本记录地址
>
> undo log主要的作用是记录回滚日志，存储老版本数据，在内部会形成一个版本链，在多个事务并行操作某一行记录，记录不同事务修改数据的版本，通过roll_pointer指针形成一个链表
>
> readView解决的是一个事务查询选择版本的问题，在内部定义了一些匹配规则和当前的一些事务id判断该访问那个版本的数据，不同的隔离级别快照读是不一样的，最终的访问的结果不一样。如果是rc隔离级别，每一次执行快照读时生成ReadView，如果是rr隔离级别仅在事务中第一次执行快照读时生成ReadView，后续复用。

## notify和notifyAll的区别，notify是公平的吗

> https://blog.csdn.net/m0_58354370/article/details/130044659
>
> 在Java中，`Object`类提供了`wait()`、`notify()`和`notifyAll()`这三个方法来实现线程间的同步通信。这些方法主要用于对象的内部锁（即synchronized块或方法中的锁）。
>
> ### notify()
>
> `notify()`方法唤醒正在等待该对象锁的一个线程。具体来说：
>
> - **唤醒一个线程**：`notify()`会从等待该对象锁的所有线程中**随机选择一个线程并唤醒**它。
> - **释放锁**：调用`notify()`的线程必须持有该对象的锁，并且在调用`notify()`之后仍然持有该锁。
> - **随机选择**：`notify()`不会按照任何特定顺序选择线程，而是由JVM随机选择一个等待线程。
>
> ### notifyAll()
>
> `notifyAll()`方法**唤醒正在等待该对象锁的所有线程**。具体来说：
>
> - **唤醒所有线程**：`notifyAll()`会唤醒所有等待该对象锁的线程。
> - **释放锁**：调用`notifyAll()`的线程必须持有该对象的锁，并且在调用`notifyAll()`之后仍然持有该锁。
> - **所有线程竞争**：被唤醒的线程将竞争对象锁，只有获得锁的线程才能继续执行。
>
> ### notify是否公平
>
> `notify()`方法本身并不涉及公平性的问题。公平性是指线程获取锁的顺序是否遵循先进先出（FIFO）的原则。在Java中，锁本身的实现可以是公平的或非公平的。`notify()`和`notifyAll()`只是用来唤醒等待线程的方法，它们不直接涉及公平性。
>
> ### 公平锁 vs. 非公平锁
>
> - **公平锁**：按照线程等待锁的顺序来决定哪个线程可以获得锁。这种锁通常会导致更高的延迟，因为线程需要等待前面的线程释放锁。
> - **非公平锁**：不保证线程获取锁的顺序。这种锁可能会导致某些线程优先获取锁，而其他线程则需要等待更长时间。
>
> 在Java中，`ReentrantLock`类提供了一个构造函数来指定锁是否是公平的。默认情况下，`ReentrantLock`是非公平的，但可以通过传递`true`作为参数来创建一个公平的锁。
>
> ### 示例
>
> 下面是一个简单的示例，演示如何使用`notify()`和`notifyAll()`：
>
> ```java
> public class NotifyDemo {
> 
>     public static void main(String[] args) {
>         final Object lock = new Object();
> 
>         Thread thread1 = new Thread(() -> {
>             synchronized (lock) {
>                 try {
>                     System.out.println(Thread.currentThread().getName() + " waiting...");
>                     lock.wait();
>                     System.out.println(Thread.currentThread().getName() + " notified and running...");
>                 } catch (InterruptedException e) {
>                     Thread.currentThread().interrupt();
>                 }
>             }
>         }, "Thread1");
> 
>         Thread thread2 = new Thread(() -> {
>             synchronized (lock) {
>                 try {
>                     System.out.println(Thread.currentThread().getName() + " waiting...");
>                     lock.wait();
>                     System.out.println(Thread.currentThread().getName() + " notified and running...");
>                 } catch (InterruptedException e) {
>                     Thread.currentThread().interrupt();
>                 }
>             }
>         }, "Thread2");
> 
>         thread1.start();
>         thread2.start();
> 
>         try {
>             Thread.sleep(1000);
>         } catch (InterruptedException e) {
>             Thread.currentThread().interrupt();
>         }
> 
>         synchronized (lock) {
>             System.out.println(Thread.currentThread().getName() + " calling notifyAll...");
>             lock.notifyAll();
>         }
>     }
> }
> ```
>
> 在这个示例中，两个线程`Thread1`和`Thread2`都在等待同一个锁。主线程在1秒后调用`notifyAll()`唤醒所有的等待线程。由于`notifyAll()`唤醒了所有线程，所以两个线程都会尝试获取锁并继续执行。
>
> ### 总结
>
> - `notify()`唤醒一个等待线程，而`notifyAll()`唤醒所有等待线程。
> - `notify()`和`notifyAll()`都需要在同步块或方法中调用，并且调用线程必须持有锁。
> - `notify()`和`notifyAll()`本身不涉及公平性问题，公平性是由锁的实现决定的。
>
> 在实际应用中，选择使用`notify()`还是`notifyAll()`取决于具体的应用场景和需求。如果只需要唤醒一个线程，使用`notify()`可以减少不必要的线程竞争；如果需要唤醒所有等待线程，使用`notifyAll()`可以确保所有线程都有机会继续执行。

## sychronized和reentranLock的区别、公平锁和非公平锁有什么区别，sychronized是公平锁还是非公平

> `synchronized` 和 `ReentrantLock` 是 Java 中用于实现线程同步的两种常见机制。它们各自有独特的特性和使用场景。下面我们将详细讨论它们之间的区别，以及公平锁和非公平锁的区别。
>
> ### synchronized 和 ReentrantLock 的区别
>
> 1. **语法和使用**：
>    - **synchronized** 是 Java 关键字，可以直接作用于方法或代码块。
>    - **ReentrantLock** 是 `java.util.concurrent.locks` 包中的一个类，需要显式地创建实例，并调用 `lock()` 和 `unlock()` 方法来控制锁的获取和释放。
>
> 2. **灵活性**：
>    - **synchronized** 更加简洁，但提供的功能有限。
>    - **ReentrantLock** 提供了更多的高级功能，如尝试获取锁（`tryLock`）、可中断锁获取（`lockInterruptibly`）、条件变量（`Condition`）等。
>
> 3. **可重入性**：
>    - **synchronized** 和 **ReentrantLock** 都支持可重入性，即一个已经持有锁的线程可以再次获取该锁，而不会造成死锁。
>
> 4. **锁的获取和释放**：
>    - **synchronized** 在发生异常时会自动释放锁。
>    - **ReentrantLock** 在发生异常时不会自动释放锁，需要手动调用 `unlock()` 方法来释放锁，否则可能导致资源泄露。
>
> 5. **性能**：
>    - 在 Java 5 之后，`synchronized` 的性能得到了显著提升，使用了**锁膨胀**和适应性**自旋**等技术，使其在某些情况下可以与 `ReentrantLock` 相媲美。
>    - **ReentrantLock** 通常提供了更好的性能，尤其是在需要高级功能的情况下。
>
> 6. **锁的类型**：
>    - **synchronized** 是非公平锁。
>    - **ReentrantLock** 支持公平锁和非公平锁，可以通过构造函数来指定。
>
> ### 公平锁和非公平锁的区别
>
> 1. **公平锁**：
>    - 公平锁保证线程获取锁的顺序与请求锁的顺序相同。
>    - 这意味着等待时间最长的线程将优先获得锁。
>    - 公平锁可以减少饥饿问题，但可能会降低系统的整体吞吐量。
>
> 2. **非公平锁**：
>    - 非公平锁不保证线程获取锁的顺序。
>    - 这意味着一个后来的线程有可能先于前面等待的线程获取锁。
>    - 非公平锁通常提供了更高的吞吐量，但可能会导致某些线程长时间等待。
>
> ### synchronized 是公平锁还是非公平锁
>
> `synchronized` 默认是非公平锁。这意味着它不会严格按照线程请求锁的顺序来分配锁。`synchronized` 的行为更像是非公平锁，因为它允许持有锁的线程优先获取锁，而不是等待其他线程释放锁后再获取。这也意味着 `synchronized` 锁在某些情况下可能会导致饥饿问题。
>
> ### 示例代码
>
> 下面是使用 `synchronized` 和 `ReentrantLock` 的示例：
>
> #### 使用 synchronized
>
> ```java
> public class SynchronizedExample {
> 
>     private int count = 0;
> 
>     public synchronized void increment() {
>         count++;
>     }
> 
>     public synchronized int getCount() {
>         return count;
>     }
> }
> ```
>
> #### 使用 ReentrantLock
>
> ```java
> import java.util.concurrent.locks.ReentrantLock;
> 
> public class ReentrantLockExample {
> 
>     private int count = 0;
>     private final ReentrantLock lock = new ReentrantLock();
> 
>     public void increment() {
>         lock.lock();
>         try {
>             count++;
>         } finally {
>             lock.unlock();
>         }
>     }
> 
>     public int getCount() {
>         lock.lock();
>         try {
>             return count;
>         } finally {
>             lock.unlock();
>         }
>     }
> }
> ```
>
> ### 总结
>
> - **synchronized** 和 **ReentrantLock** 都可以用于实现线程同步，但它们在使用方式、灵活性、性能等方面有所不同。
> - **ReentrantLock** 提供了更多的高级功能，并且可以明确指定是否使用公平锁。
> - **synchronized** 是非公平锁。
> - 选择使用哪种锁取决于你的具体需求，包括性能要求、是否需要高级功能以及是否关心公平性。

## Redis数据结构，hash是什么实现的，底层数据结构是什么， 在什么情况下使用什么数据结构

> https://xiaolincoding.com/redis/data_struct/command.html#%E4%BB%8B%E7%BB%8D-3
>
> hash：哈希表或压缩列表
>
> 

## 算法题：使用栈模拟队列

https://leetcode.cn/problems/implement-queue-using-stacks/description/



# 蚂蚁财险

8.23 14:00~14:40

自我介绍

介绍项目

## 项目中遇到了什么问题怎么解决的？

## 在高并发场景下，怎么设计一个高性能、RT尽量短的高可用保障？从哪些角度考虑？

> 在高并发场景下设计一个高性能、低响应时间（RT）且高可用的系统是一项挑战性的任务。为了实现这一目标，需要从多个角度考虑，并采取一系列技术和策略。下面是一些关键的设计要点和建议：
>
> ### 1. 系统架构
> - **微服务架构**：将系统拆分为小型、松耦合的服务，可以更容易地进行扩展和维护。
> - **API网关**：作为前端和后端服务之间的入口，可以集中处理认证、限流、熔断等逻辑。
> - **服务发现与负载均衡**：使用服务发现机制来动态发现可用的服务实例，并使用负载均衡器来分发请求。
>
> ### 2. 数据库设计
> - **读写分离**：将读操作和写操作分开处理，以减轻主数据库的压力。
> - **分库分表**：将数据分布在多个数据库和表中，减少单一数据库的压力。
> - **索引优化**：合理使用索引，避免索引失效，提高查询性能。
> - **缓存策略**：使用缓存（如Redis）来减少对数据库的直接访问，降低数据库负载。
>
> ### 3. 性能优化
> - **异步处理**：对于耗时的操作，使用异步处理方式来提高响应速度。
> - **批量处理**：对于可以批量处理的操作，尽量合并请求，减少网络往返次数。
> - **数据预加载**：提前加载数据到缓存中，减少延迟。
> - **资源复用**：合理利用资源，避免重复创建和销毁对象。
>
> ### 4. 并发控制
> - **限流**：限制单位时间内处理的请求数量，避免系统过载。
> - **熔断机制**：当检测到某个服务或组件出现问题时，自动断开与之的连接，防止故障扩散。
> - **降级策略**：当系统资源紧张时，主动降低服务级别或关闭非核心功能。
>
> ### 5. 负载均衡
> - **硬件负载均衡**：使用硬件负载均衡器（如F5、Nginx等）来分发请求。
> - **软件负载均衡**：使用软件负载均衡器（如HAProxy、Kubernetes Service等）来动态分配流量。
>
> ### 6. 高可用性
> - **容灾机制**：在不同数据中心部署副本，以应对灾难性故障。
> - **冗余设计**：为关键组件提供冗余备份，确保故障时可以快速切换。
> - **故障转移**：设计故障转移策略，确保主节点故障时可以从备份节点接管服务。
> - **健康检查**：定期检查服务的健康状况，及时发现并修复问题。
>
> ### 7. 监控与报警
> - **性能监控**：实时监控系统的关键性能指标（如响应时间、吞吐量等）。
> - **错误日志**：记录错误和异常信息，便于事后分析。
> - **报警机制**：当监控到异常情况时，自动发送报警通知。
>
> ### 8. 测试与优化
> - **压力测试**：定期进行压力测试，评估系统的极限承载能力。
> - **性能瓶颈分析**：分析性能瓶颈，针对性地进行优化。
> - **持续集成与部署**：使用CI/CD流程自动化测试和部署，确保质量。
>
> ### 示例
>
> 假设你正在设计一个高并发的在线购物系统，可以考虑以下策略：
>
> 1. **微服务架构**：将系统拆分为商品服务、订单服务、支付服务等多个微服务。
> 2. **读写分离**：为数据库配置读写分离，主数据库负责写操作，从数据库负责读操作。
> 3. **缓存策略**：使用Redis缓存热门商品信息和用户购物车数据，减少数据库访问。
> 4. **限流机制**：在API网关层实现限流策略，防止系统过载。
> 5. **熔断机制**：为每个微服务实现熔断机制，当某个服务响应时间过长或失败率过高时，自动断开与该服务的连接。
> 6. **负载均衡**：使用Kubernetes Service或Nginx等工具进行流量分发。
> 7. **高可用性**：在不同的数据中心部署多个实例，实现地域级容灾。
>
> 通过上述设计和策略，可以构建一个高性能、低响应时间且高可用的系统。需要注意的是，具体的实现细节和技术选型会根据项目需求和团队经验有所差异。

## 说说分库分表、数据记录的唯一Id怎么得到，对id有什么要求吗

> MySQL 中的分库分表（Sharding）和数据记录的唯一ID获取是两种不同的概念，但它们在大型分布式系统中经常一起使用以提高性能和可扩展性。
>
> ### 分库分表 (Sharding)
>
> 分库分表是一种数据库水平分割的技术，用于将大量的数据分布到多个物理数据库上。这可以通过几种方式实现，包括按范围（Range-based）、按哈希（Hash-based）或者按列表（List-based）等方式。
>
> #### 分库分表的目的：
> - 提升性能：通过减少单一数据库的压力，加速查询速度。
> - 扩展能力：允许系统随着数据量的增长而扩展。
> - 提高可用性：通过数据冗余减少单点故障的影响。
>
> #### 分库分表的方法：
> 1. **按范围分表**：
>    - 根据某个字段的值范围来分配数据到不同的表中。比如按照用户ID的范围进行分表。
>
> 2. **按哈希分表**：
>    - 对某个字段的值进行哈希运算，然后根据哈希值模N的结果将数据分布到N个表中。这种方式保证了数据均匀分布。
>
> 3. **按列表分表**：
>    - 根据预定义的列表来决定数据存储在哪张表中。例如，某些特定的用户ID只能存放在指定的表中。
>
> ### 数据记录的唯一ID获取
>
> 在MySQL中，为了确保数据记录的唯一性，通常会为每条记录分配一个唯一的ID。这个ID可以通过多种方式生成，包括但不限于以下几种：
>
> #### 常见的ID生成策略：
> 1. **自增ID (Auto-Increment)**：
>    - 最简单的方法之一，MySQL 提供了 `AUTO_INCREMENT` 属性，可以自动为表中的某列生成唯一的递增数字。
>
> 2. **UUID (Universally Unique Identifier)**：
>    - UUID 是一种广泛使用的唯一标识符标准，可以在多台机器上同时生成而不会冲突。MySQL 支持通过函数 `UUID()` 生成UUID。
>
> 3. **Snowflake算法**：
>    - 一种分布式ID生成算法，由Twitter开发，能够生成全局唯一的64位整数。它可以替代自增ID，在分布式环境下使用。
>
> 4. **基于Redis的ID生成器**：
>    - 利用Redis的原子操作特性来生成全局唯一的ID。
>
> 5. **第三方服务**：
>    - 可以使用专门的ID生成服务，如Kafka的ID生成器等。
>
> #### 对ID的要求：
> - **唯一性**：每个ID必须是全局唯一的，即使在分布式环境中也是如此。
> - **连续性**：在某些场景下，连续的ID有助于提高性能（如索引查找）。
> - **可预测性**：有时需要避免ID可预测，以防数据泄露风险。
> - **性能**：ID生成机制应当高效，避免成为瓶颈。
> - **持久性**：ID一旦生成就不应改变。
>
> ### 实际应用中如何选择
>
> 在实际应用中选择哪种ID生成策略取决于具体的需求。例如：
>
> - **对于小规模应用**：可以使用MySQL自带的 `AUTO_INCREMENT` 功能。
> - **对于分布式应用**：推荐使用Snowflake算法或者基于Redis的服务，以确保全局唯一性。
> - **对于需要跨服务一致性的应用**：可以使用中央服务来生成ID，并通过API调用来获取。
>
> 总的来说，选择合适的分库分表策略和ID生成机制需要综合考虑应用的特点和性能需求。在设计时，还需要考虑到未来的扩展性和维护性。

## jetcache原理、怎么保证二级缓存的一致性、能保证强一致吗

> 基于Redis Pub/Sub实现二级缓存一致性，不能保证强一致，保证最终一致性，引入缓存就是为了提高响应速度，如果要求强一致就要加锁，加锁又影响响应时间，二者不能同时兼顾，保证最终一致性即可。

## 缓存穿透、缓存击穿、缓存雪崩

> 穿透：访问不存在的key，jetcahe注解@Cache，支持cacheNullValue属性设置，缓存null值
>
> 击穿：热点key失效，大量请求打过来，@CachePenetrationProtect（缓存击穿保护），在大规模分布式热点的情况下，一旦热点数据失效，会有大量的请求同时尝试重建缓存，这不但会导致资源浪费，更加危险的是会造成数据库瞬时极大的压力，当缓存访问未命中的情况下，对并发进行的加载行为进行保护，实现的是单JVM内的保护，即同一个JVM中同一个key只有一个线程去加载，其它线程等待结果。
>
> @CachePenetrationProtect注解：
>
> 当缓存访问未命中的情况下，对并发进行的加载行为进行保护。
> 当前版本实现的是单JVM内的保护，即同一个JVM中同一个key只有一个线程去加载，其它线程等待结果。
>
> >- **CacheAside模式**的缓存由于本身有淘汰策略，**在数据失效后，缓存组件会直接访问数据库尝试重建缓存**。
> >- **在大规模分布式热点的情况下，一旦热点数据失效，会有大量的请求同时尝试重建缓存，这不但会导致资源浪费，更加危险的是会造成数据库瞬时极大的压力**。
> >- JetCache通过注解**@CachePenetrationProtect实现了JVM内存锁级的击穿保护**，**使并发重建的请求限制到可控范围**。( **如果数据利用率高还可以使用@CacheRefresh的方式来实现基于分布式锁的缓存重建能力** )
> >
> >**@CrashRefresh同一个刷新周期内只有一个节点完成刷新操作，通过分布式锁实现**
>
> 雪崩：同一时间大部分缓存失效

## mysql怎么实现主从同步、读写分离的？主从同步和读写分离架构适合于所有互联网场景吗

> MySQL 主从同步（Replication）和读写分离（Read-Write Splitting）是两种常见的数据库优化策略，用于提高系统的可扩展性和可用性。
>
> ### MySQL 主从同步
>
> MySQL 主从同步是指在一个或多个MySQL服务器之间复制数据。通常一个服务器作为主服务器（Master），负责接收所有的写操作；其他服务器作为从服务器（Slave），它们会自动复制主服务器的数据变更。这样可以实现数据的冗余备份，并且可以利用从服务器进行只读查询来分散主服务器的压力。
>
> #### 实现步骤：
> 1. **配置主服务器**：
>    - 开启二进制日志（Binary Logging）。
>    - 设置全局唯一标识符（Server ID）。
>    
> 2. **配置从服务器**：
>    - 设置全局唯一标识符（Server ID）。
>    - 指定要复制的主服务器的位置（例如通过IP地址和端口）。
>    - 配置复制过滤规则（如果需要的话）。
>
> 3. **启动复制**：
>    - 在从服务器上启动复制进程。
>
> 4. **监控复制状态**：
>    - 使用 `SHOW SLAVE STATUS\G` 命令检查复制的状态。
>
> ### 读写分离
>
> 读写分离是一种负载均衡技术，它将读取请求与写入请求分开处理。通常，所有的写操作（如INSERT, UPDATE, DELETE等）都发送到主服务器，而只读请求（如SELECT）则可以被路由到任何从服务器。
>
> #### 实现方法：
> 1. **手动路由**：
>    - 应用程序直接判断并选择正确的数据库实例进行访问。
>    
> 2. **使用中间件/代理**：
>    - 使用如ProxySQL、MaxScale等工具来自动管理请求路由。
>
> 3. **应用层实现**：
>    - 在应用程序中实现逻辑，根据请求类型决定发送给哪个服务器。
>
> ### 是否适用于所有互联网场景？
>
> 主从同步和读写分离并不总是适用于所有互联网场景，这取决于具体的业务需求和技术限制。以下是一些考虑因素：
>
> - **数据一致性要求**：
>   - 如果应用程序需要强一致性，那么从服务器上的数据可能会有延迟，不适合某些场景。
>   
> - **网络延迟**：
>   - 如果主服务器和从服务器之间的距离很远，网络延迟可能会成为一个问题。
>
> - **成本**：
>   - 需要额外的硬件资源和支持，增加了运维复杂度和成本。
>
> - **系统复杂度**：
>   - 这种架构会增加系统的复杂度，对于简单的应用来说可能过于复杂。
>
> 因此，在决定是否采用主从同步和读写分离之前，需要综合考虑以上因素以及业务的实际需求。对于一些高并发、高可用性的应用来说，这种架构是非常有效的。而对于一些小型应用或者对数据一致性要求极高的应用，则可能需要考虑其他方案。

## 数据处理也可以采用OLAP系统，为什么没有使用？

## 数仓、OLTP、OLAP

> https://blog.csdn.net/weixin_43596734/article/details/119784553

离职多久了，为什么离职？



# 京东1面

2024/8/23 18:00 ~ 19:00	两个面试官

自我介绍

介绍项目

## 慢查询怎么应对

> 慢查询是指那些执行时间较长的SQL查询，它们会影响数据库系统的性能和响应时间。定位和解决慢查询是数据库优化的重要环节。下面是一些定位和解决慢查询的方法：
>
> ### 定位慢查询
>
> 1. **开启慢查询日志**：
>    - 在MySQL中，可以通过配置`slow_query_log`参数来开启慢查询日志。
>    - 设置`long_query_time`参数来指定慢查询的阈值（默认为10秒）。
>    - 可以指定日志文件的位置和名称，例如通过`slow_query_log_file`参数。
>
> 2. **使用`EXPLAIN`语句**：
>    - 对于有问题的SQL查询，使用`EXPLAIN`命令来分析查询的执行计划。
>    - 查看输出结果中的`type`列，了解表扫描的方式（如`ALL`表示全表扫描，`range`表示范围扫描）。
>    - 关注`possible_keys`、`key`、`key_len`等列，判断是否使用了合适的索引。
>
> 3. **使用性能监控工具**：
>    - 使用MySQL自带的`SHOW PROCESSLIST`命令查看当前正在运行的查询及其状态。
>    - 利用数据库管理系统提供的性能监控工具，如MySQL的`Performance Schema`或`InnoDB Monitor`。
>
> 4. **使用第三方工具**：
>    - 使用如`MySQLTuner`、`pt-query-digest`等工具来分析慢查询日志，找出常见的慢查询模式。
>    - 利用`MySQL Workbench`的性能分析功能来诊断慢查询。
>
> ### 解决慢查询
>
> 1. **优化查询语句**：
>    - 简化查询逻辑，避免不必要的子查询或联接操作。
>    - 限制返回的数据量，使用`LIMIT`关键字来限制结果集的大小。
>    - 使用更有效的表联接方式（如`INNER JOIN`代替`LEFT JOIN`）。
>
> 2. **创建合适的索引**：
>    - 为经常出现在`WHERE`子句中的列创建索引。
>    - 创建复合索引（包含多个列的索引），以支持多个字段的组合查询。
>    - 避免在索引列上使用函数，因为这会导致索引失效。
>
> 3. **调整数据库配置**：
>    - 调整缓冲池大小（如`innodb_buffer_pool_size`）以适应工作负载的需求。
>    - 优化查询缓存（如`query_cache_type`和`query_cache_size`）设置，但在高并发场景下应谨慎使用查询缓存。
>    - 调整`innodb_flush_log_at_trx_commit`参数以改善写入性能。
>
> 4. **使用分区**：
>    - 对于大型表，可以考虑使用分区来将数据分割成更小的部分，从而加速查询速度。
>    - 分区可以按照日期、范围或列表等方式进行。
>
> 5. **读写分离**：
>    - 如果查询较多，可以考虑使用读写分离的架构，将读请求和写请求分开处理。
>    - 通过中间件或应用程序逻辑来实现读写请求的路由。
>
> 6. **使用缓存**：
>    - 对于频繁访问的数据，可以使用缓存（如Redis）来减少对数据库的直接访问。
>    - 通过缓存结果来减少数据库的负载。
>
> 7. **优化数据模型**：
>    - 重新设计数据模型，避免复杂的关联操作。
>    - 考虑使用物化视图或汇总表来存储预先计算的结果。
>
> 8. **定期维护**：
>    - 定期分析和优化索引，删除不再需要的索引。
>    - 执行表的分析（`ANALYZE TABLE`）和优化（`OPTIMIZE TABLE`）操作，以更新统计信息和整理表结构。
>
> ### 示例
>
> 假设我们有一个慢查询，如下所示：
>
> ```sql
> SELECT * FROM products WHERE name LIKE '%widget%';
> ```
>
> 1. **定位问题**：
>    - 开启慢查询日志，查看是否有此查询记录。
>    - 使用`EXPLAIN`分析查询执行计划：
>      ```sql
>      EXPLAIN SELECT * FROM products WHERE name LIKE '%widget%';
>      ```
>    - 检查`EXPLAIN`结果中的`type`、`possible_keys`、`key`等列。
>
> 2. **解决问题**：
>    - **优化查询语句**：简化查询逻辑，例如使用全文索引来搜索文本内容。
>    - **创建合适的索引**：为`products`表的`name`列创建全文索引。
>    - **调整数据库配置**：如果查询涉及大量数据，可以尝试调整`innodb_buffer_pool_size`以提高性能。
>    - **使用缓存**：对于频繁访问的产品名称，可以考虑使用缓存来减少数据库的查询次数。
>
> 通过以上步骤，可以有效地定位和解决慢查询问题，从而提高数据库的性能和响应速度。

## msyql索引失效的情况

> MySQL中的索引是为了加速查询而存在的数据结构。然而，在某些情况下，MySQL可能会选择不使用索引，导致索引失效。了解索引失效的情况对于优化查询和提高数据库性能非常重要。下面是MySQL索引失效的一些常见情况：
>
> ### 索引失效的情况
>
> 1. **使用函数或表达式**：
>    - 如果在查询条件中使用了函数或表达式，MySQL可能无法使用索引。
>    - 例如：`SELECT * FROM users WHERE YEAR(birth_date) = 1990;`
>
> 2. **使用`!=`或`<>`操作符**：
>    - 当使用`!=`或`<>`操作符时，MySQL通常无法使用索引。
>    - 例如：`SELECT * FROM users WHERE age != 30;`
>
> 3. **使用`OR`操作符连接非等值条件**：
>    - 如果`OR`操作符两边的条件没有同时出现在索引中，则索引可能失效。
>    - 例如：`SELECT * FROM users WHERE age = 30 OR gender = 'female';`
>
> 4. **使用`LIKE`操作符开头为通配符**：
>    - 当`LIKE`操作符的开头为通配符（如`%search%`）时，索引可能无法使用。
>    - 例如：`SELECT * FROM users WHERE name LIKE '%John%';`
>
> 5. **在索引列上使用`IS NULL`或`IS NOT NULL`**：
>    - `IS NULL`或`IS NOT NULL`操作符通常不会使用索引。
>    - 例如：`SELECT * FROM users WHERE address IS NULL;`
>
> 6. **查询条件中使用变量**：
>    - 如果查询条件中的值是通过变量传递的，MySQL可能无法使用索引。
>    - 例如：`SET @age = 30; SELECT * FROM users WHERE age = @age;`
>
> 7. **隐式类型转换**：
>    - 如果查询条件与索引列的数据类型不匹配，可能导致索引失效。
>    - 例如：`SELECT * FROM users WHERE id = '1';` （如果`id`列是整数类型）
>
> 8. **在`ORDER BY`或`GROUP BY`子句中使用未索引的列**：
>    - 如果`ORDER BY`或`GROUP BY`子句中使用了未包含在索引中的列，索引可能失效。
>    - 例如：`SELECT * FROM users ORDER BY last_name DESC;`
>
> 9. **复合索引使用不完全**：
>    - 如果复合索引中的第一列没有被使用，索引可能失效。
>    - 例如，对于`(col1, col2)`的复合索引，查询`SELECT * FROM table WHERE col2 = 'value';`不会使用索引。
>
> 10. **全文索引的限制**：
>     - 全文索引在某些情况下可能无法使用，例如查询中包含的单词太少或太常见。
>     - 例如：`SELECT * FROM articles WHERE MATCH(title) AGAINST('the');`
>
> ### 解决方法
>
> 1. **避免使用函数或表达式**：
>     - 尽量避免在查询条件中使用函数或表达式，或者创建包含函数结果的索引。
>
> 2. **使用等值条件**：
>     - 尽量使用`=`操作符，如果必须使用`!=`或`<>`，可以考虑反向查询。
>
> 3. **优化`OR`操作符**：
>     - 如果可能，将`OR`操作符两边的条件合并到同一个索引中。
>
> 4. **使用前缀匹配**：
>     - 如果使用`LIKE`操作符，尽量使通配符出现在字符串的末尾。
>
> 5. **使用`NULL`值的索引**：
>     - 创建索引时，考虑包含`NULL`值的列。
>
> 6. **显式指定值**：
>     - 尽量避免使用变量，或者在查询计划中明确指定值。
>
> 7. **避免隐式类型转换**：
>     - 确保查询条件与索引列的数据类型匹配。
>
> 8. **创建复合索引**：
>     - 创建包含`ORDER BY`或`GROUP BY`子句中使用的列的复合索引。
>
> 9. **使用完整的复合索引**：
>     - 在查询中使用复合索引的所有列。
>
> 10. **优化全文索引查询**：
>     - 使用更具体的关键词进行全文索引查询。
>
> ### 示例
>
> 假设我们有一个名为`users`的表，其中包含`id`、`name`、`age`、`gender`等字段。我们来看看一些可能导致索引失效的查询示例及其优化方法：
>
> 1. **使用函数**：
>    - 查询：`SELECT * FROM users WHERE YEAR(birth_date) = 1990;`
>    - 优化：可以创建包含`YEAR(birth_date)`的函数索引（MySQL 8.0+支持函数索引）。
>
> 2. **使用`OR`操作符**：
>    - 查询：`SELECT * FROM users WHERE age = 30 OR gender = 'female';`
>    - 优化：创建一个包含`age`和`gender`的复合索引。
>
> 3. **使用`LIKE`操作符**：
>    - 查询：`SELECT * FROM users WHERE name LIKE '%John%';`
>    - 优化：改为`SELECT * FROM users WHERE name LIKE 'John%';`
>
> 通过了解索引失效的情况及其解决方法，你可以更好地优化MySQL查询，提高数据库的性能。

## 线程的四种创建模式、那种可以具有返回值

> 在Java中，创建线程主要有两种基本方式，而不是四种。这两种方式分别是继承`Thread`类和实现`Runnable`接口。此外，Java还提供了另外两种创建线程的方式，即使用`Callable`接口和`ExecutorService`。下面详细介绍这四种创建线程的方式及其特点：
>
> ### 1. 继承`Thread`类
> 这是创建线程最直接的方式。你需要定义一个类继承自`Thread`类，并重写`run()`方法。
>
> #### 特点：
> - 直接继承`Thread`类。
> - 重写`run()`方法来定义线程的行为。
> - 不能直接返回值，因为`run()`方法的返回类型是`void`。
>
> #### 示例：
> ```java
> public class MyThread extends Thread {
>     @Override
>     public void run() {
>         System.out.println("Thread running...");
>     }
> }
> 
> public static void main(String[] args) {
>     MyThread thread = new MyThread();
>     thread.start();
> }
> ```
>
> ### 2. 实现`Runnable`接口
> 这种方式比继承`Thread`类更灵活，因为你只需要实现`Runnable`接口，并重写`run()`方法。
>
> #### 特点：
> - 实现`Runnable`接口。
> - 重写`run()`方法来定义线程的行为。
> - 不能直接返回值，因为`run()`方法的返回类型是`void`。
> - 可以与多个线程共享同一个`Runnable`实例。
>
> #### 示例：
> ```java
> public class MyRunnable implements Runnable {
>     @Override
>     public void run() {
>         System.out.println("Thread running...");
>     }
> }
> 
> public static void main(String[] args) {
>     Thread thread = new Thread(new MyRunnable());
>     thread.start();
> }
> ```
>
> ### 3. 使用`Callable`接口
> `Callable`接口与`Runnable`接口类似，但它允许线程返回值，并且支持抛出异常。
>
> #### 特点：
> - 实现`Callable`接口。
> - 重写`call()`方法来定义线程的行为。
> - 可以通过`Future`接口来获取线程的返回值。
> - 支持抛出异常。
>
> #### 示例：
> ```java
> import java.util.concurrent.Callable;
> import java.util.concurrent.FutureTask;
> 
> public class MyCallable implements Callable<Integer> {
>     @Override
>     public Integer call() throws Exception {
>         return 100;
>     }
> }
> 
> public static void main(String[] args) {
>     MyCallable callable = new MyCallable();
>     FutureTask<Integer> futureTask = new FutureTask<>(callable);
>     
>     // 创建线程并启动
>     Thread thread = new Thread(futureTask);
>     thread.start();
>     
>     try {
>         // 获取返回值
>         Integer result = futureTask.get();
>         System.out.println("Result: " + result);
>     } catch (Exception e) {
>         e.printStackTrace();
>     }
> }
> ```
>
> ### 4. 使用`ExecutorService`
> `ExecutorService`是一个接口，提供了管理和控制线程的方法。它可以管理一个线程池，使得线程可以重用。
>
> #### 特点：
> - 使用`ExecutorService`来管理线程。
> - 可以提交`Runnable`或`Callable`任务。
> - 支持返回值（通过`Callable`）。
> - 提供了线程池管理功能，可以控制线程的数量。
>
> #### 示例：
> ```java
> import java.util.concurrent.*;
> 
> public class ExecutorExample {
>     public static void main(String[] args) {
>         ExecutorService executor = Executors.newFixedThreadPool(2); // 创建固定大小的线程池
>         
>         // 提交一个Runnable任务
>         executor.submit(new Runnable() {
>             @Override
>             public void run() {
>                 System.out.println("Runnable task running...");
>             }
>         });
>         
>         // 提交一个Callable任务
>         Future<Integer> future = executor.submit(new Callable<Integer>() {
>             @Override
>             public Integer call() throws Exception {
>                 return 100;
>             }
>         });
>         
>         try {
>             // 获取返回值
>             Integer result = future.get();
>             System.out.println("Callable result: " + result);
>         } catch (InterruptedException | ExecutionException e) {
>             e.printStackTrace();
>         }
>         
>         executor.shutdown(); // 关闭线程池
>     }
> }
> ```
>
> ### 总结
> - **继承`Thread`类**和**实现`Runnable`接口**主要用于创建没有返回值的线程。
> - **实现`Callable`接口**可以创建有返回值的线程。
> - **使用`ExecutorService`**不仅可以创建线程，还可以管理线程池，适用于复杂的线程管理场景。
>
> 因此，如果你需要创建一个可以返回值的线程，应该使用`Callable`接口或者结合`Callable`和`ExecutorService`。

## 线程池的参数、原理

## 线程数怎么设置，有8c 8g的机器，应用调用三个相互没有关联的微服务(只是查询数据返回，没有复杂的逻辑计算)，怎么设置线程数

> 在设置线程数时，需要考虑的因素包括机器的硬件资源、应用的具体需求以及所调用的微服务的响应时间和并发处理能力。对于一台具有8核CPU和8GB内存的机器，如果应用仅用于调用三个相互没有关联的微服务进行简单的查询数据返回操作，那么线程数的设置可以参考以下原则：
>
> ### 一般原则
>
> 1. **考虑CPU核心数**：
>    - 一般来说，线程数可以设置为CPU核心数的1.5到2倍。这是因为现代处理器支持超线程技术，可以同时处理更多的线程。
>    - 对于8核CPU的机器，可以考虑设置线程池的最大线程数为12到16个。
>
> 2. **考虑微服务的响应时间**：
>    - 如果微服务的响应时间较快，可以适当增加线程数以提高并发处理能力。
>    - 如果微服务的响应时间较长，可以适当减少线程数以避免过多的线程导致的上下文切换开销。
>
> 3. **考虑网络I/O**：
>    - 由于微服务调用主要是网络I/O操作，而不是CPU密集型操作，因此可以设置较高的线程数以充分利用网络带宽。
>
> 4. **考虑内存限制**：
>    - 线程也需要占用一定的内存资源，因此在设置线程数时也要考虑内存的限制。
>    - 8GB的内存对于简单的查询操作来说应该是足够的，但如果应用中还有其他较大的内存消耗，需要适当调整线程数。
>
> ### 具体设置
>
> 假设你使用的是`ExecutorService`来管理线程池，可以考虑以下设置：
>
> 1. **固定线程池 (`newFixedThreadPool`)**：
>    - 用于处理大量的短期任务，线程数固定。
>    - 可以设置为8到16个线程，具体数值可以根据微服务的实际响应时间和并发能力进行调整。
>
> 2. **可缓存线程池 (`newCachedThreadPool`)**：
>    - 适用于执行很多短期异步任务的小程序。
>    - 这种线程池可以根据需要创建新线程，但会在线程闲置一段时间后终止线程。
>    - 适用于不确定线程数的情况，但要注意可能会导致大量线程的创建。
>
> 3. **定时线程池 (`newScheduledThreadPool`)**：
>    - 如果需要定期执行某些任务，可以使用定时线程池。
>    - 线程数可以设置为3到5个，具体数值取决于任务的频率和执行时间。
>
> 4. **固定大小的线程池 (`newFixedThreadPool`)**：
>    - 适用于处理大量短期任务。
>    - 可以设置为8到16个线程。
>
> ### 示例代码
>
> 假设使用`newFixedThreadPool`来创建一个线程池，可以这样设置：
>
> ```java
> import java.util.concurrent.ExecutorService;
> import java.util.concurrent.Executors;
> 
> public class ThreadPoolExample {
> 
>     public static void main(String[] args) {
>         int coreCount = Runtime.getRuntime().availableProcessors();
>         int maxThreads = (int) (coreCount * 1.5); // 或者 (int) (coreCount * 2)
>         
>         ExecutorService executor = Executors.newFixedThreadPool(maxThreads);
> 
>         // 假设有三个微服务调用
>         for (int i = 0; i < 3; i++) {
>             final int serviceId = i;
>             executor.submit(() -> {
>                 // 调用微服务
>                 // 这里可以使用HttpClient、OkHttp等库来发起HTTP请求
>                 // 示例：
>                 // String response = HttpClientUtil.sendGetRequest("http://microservice-" + serviceId + "/data");
>                 System.out.println("Microservice " + serviceId + " called.");
>             });
>         }
> 
>         executor.shutdown(); // 关闭线程池
>     }
> }
> ```
>
> 在这个例子中，我们设置了线程池的最大线程数为CPU核心数的1.5倍。如果微服务的响应时间较快，可以考虑将线程数设置得更高一些；如果响应时间较慢，则可以适当降低线程数。
>
> ### 注意事项
>
> - **监控和调整**：在实际应用中，需要根据实际运行情况进行监控和调整。可以使用Java的监控工具（如VisualVM、JConsole等）来监控线程池的使用情况。
> - **负载测试**：通过负载测试来评估不同线程数下的性能表现，以找到最佳的线程数设置。
> - **微服务调用的优化**：考虑使用异步调用、批处理等方式来进一步优化微服务的调用，减少线程池的负担。
>
> 通过上述设置和调整，可以有效地利用硬件资源，提高应用的并发处理能力和响应速度。

## 核心线程8，最大线程数20，并发能力可以达到多少？

## 垃圾回收器使用的什么、了解G1吗

## JDK8实际项目中一般采用什么垃圾回收器：

> 在Java Development Kit (JDK) 8中，选择合适的垃圾回收器（GC）对于提高应用程序的性能和响应时间至关重要。JDK 8提供了多种垃圾回收器，每种都有其特定的应用场景和优势。以下是JDK 8中常用的几种垃圾回收器以及它们在实际项目中的应用场景：
>
> ### 1. Serial GC (串行垃圾回收器)
>
> - **特点**：
>   - 单线程，适用于单核或轻负载的系统。
>   - 主要用于客户端应用，或者是开发和测试环境中。
>   
> - **使用场景**：
>   - 对于小型应用或者资源有限的嵌入式设备，Serial GC是一个不错的选择。
>
> ### 2. Parallel GC (并行垃圾回收器)
>
> - **特点**：
>   - 多线程，适用于多核系统，专注于吞吐量最大化。
>   - 使用-XX:+UseParallelGC标志启用。
>   
> - **使用场景**：
>   - 对于后台批处理任务或者需要最大化吞吐量的应用程序，Parallel GC是一个很好的选择。
>   - 适合服务器端应用，特别是需要在限定时间内完成大量工作的场景。
>
> ### 3. Concurrent Mark Sweep (CMS) GC
>
> - **特点**：
>   - 低停顿时间，专注于减少应用程序暂停时间。
>   - 使用-XX:+UseConcMarkSweepGC标志启用。
>   - 使用了并发标记-清除算法。
>   
> - **使用场景**：
>   - 对于需要低延迟响应的应用程序，CMS GC是一个很好的选择。
>   - 适合交互式应用，如Web服务器、游戏服务器等。
>
> ### 4. G1 (Garbage First) GC
>
> - **特点**：
>   - **平衡吞吐量和停顿时间**，适用于大堆内存。
>   - 使用-XX:+UseG1GC标志启用。
>   - 设计用于大规模应用，可以处理数十GB的堆大小。
>   
> - **使用场景**：
>   - 对于大堆内存的应用程序，G1 GC是一个非常好的选择。
>   - 适合企业级应用、大数据处理和云计算平台。
>
> ### 实际项目中的选择
>
> 在实际项目中，选择哪种垃圾回收器取决于多个因素，包括应用程序的需求、系统资源和性能目标。以下是一些指导原则：
>
> - **小型应用**：如果应用规模较小，资源有限，可以选择Serial GC。
> - **批处理任务**：如果应用程序主要关注吞吐量，可以选择Parallel GC。
> - **交互式应用**：如果应用程序需要低延迟响应，可以选择CMS GC。
> - **大规模应用**：对于大型应用或大堆内存的应用，G1 GC是一个较好的选择。
>
> ### 示例
>
> 假设你正在为一个大型的Web应用选择垃圾回收器，该应用需要处理大量的用户请求，并且堆内存大小可能超过10GB。在这种情况下，G1 GC是一个合适的选择。
>
> #### 配置示例
>
> ```bash
> java -Xms10g -Xmx10g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35 -jar myapp.jar
> ```
>
> - `-Xms10g` 和 `-Xmx10g` 设置初始和最大堆内存大小均为10GB。
> - `-XX:+UseG1GC` 启用G1 GC。
> - `-XX:MaxGCPauseMillis=200` 设置最大GC暂停时间为200毫秒。
> - `-XX:InitiatingHeapOccupancyPercent=35` 当堆内存占用达到35%时，触发G1 GC。
>
> ### 总结
>
> 在选择垃圾回收器时，需要考虑应用的具体需求和性能目标。对于大多数现代服务器应用，G1 GC因其平衡了吞吐量和停顿时间，且适用于大规模应用，通常是一个很好的选择。不过，最终的选择应该基于性能测试和监控结果，以确保选定的垃圾回收器能够满足应用的实际需求。

## 手机银行app C端系统 JDK8实际项目中一般采用什么垃圾回收器

> 对于手机银行App这类面向C端用户的系统，**用户体验和响应时间是非常重要的**。因此，选择一个能够保证**低延迟和快速响应**的垃圾回收器至关重要。在Java Development Kit (JDK) 8中，Concurrent Mark Sweep (CMS) GC 和 G1 (Garbage First) GC 是两种广泛应用于需要低延迟的应用场景中的垃圾回收器。
>
> ### 1. CMS (Concurrent Mark Sweep) GC
>
> - **特点**：
>   - 专注于减少应用程序暂停时间。
>   - 使用了并发标记-清除算法。
>   - 适用于需要低延迟的应用程序。
>   
> - **使用场景**：
>   - 对于需要低延迟响应的应用程序，CMS GC 是一个很好的选择。
>   - 适合交互式应用，如手机银行App这类需要快速响应的场景。
>
> ### 2. G1 (Garbage First) GC
>
> - **特点**：
>   - **平衡吞吐量和停顿时间**，适用于大堆内存。
>   - 设计用于大规模应用，可以处理数十GB的堆大小。
>   - 支持混合模式，可以在并发和非并发模式之间切换。
>   
> - **使用场景**：
>   - 对于大堆内存的应用程序，G1 GC 是一个非常好的选择。
>   - 适合企业级应用、大数据处理和云计算平台，同时也适用于需要低延迟的应用。
>
> ### 手机银行App的场景
>
> 对于手机银行App这样的C端系统，我们需要考虑以下几个方面：
>
> - **用户体验**：用户体验至关重要，因此需要保证低延迟和快速响应。
> - **资源限制**：手机设备相对于服务器来说资源有限，因此需要高效利用资源。
> - **内存管理**：随着功能的增加，应用的内存占用也会增加，需要良好的内存管理。
>
> ### 推荐选择
>
> 对于手机银行App这类应用，推荐使用 **G1 GC**。G1 GC 在设计时考虑到了低延迟的需求，并且支持大堆内存的管理，可以很好地平衡吞吐量和停顿时间。此外，G1 GC 还具备以下优势：
>
> - **并行和并发**：G1 GC 可以在多个CPU核心上并行执行，同时也支持并发标记和清除。
> - **分区堆管理**：G1 GC 将堆划分为多个区域，并只清理那些最有可能产生垃圾的区域，从而减少了暂停时间。
> - **可预测的停顿时间**：G1 GC 可以通过配置参数来控制GC暂停时间的目标，这非常适合需要低延迟的应用。
>
> ### 示例配置
>
> 假设你正在为手机银行App配置JVM参数，可以考虑以下配置：
>
> ```bash
> java -Xms256m -Xmx512m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35 -jar bankapp.jar
> ```
>
> - `-Xms256m` 和 `-Xmx512m` 设置初始和最大堆内存大小分别为256MB和512MB。
> - `-XX:+UseG1GC` 启用G1 GC。
> - `-XX:MaxGCPauseMillis=200` 设置最大GC暂停时间为200毫秒。
> - `-XX:InitiatingHeapOccupancyPercent=35` 当堆内存占用达到35%时，触发G1 GC。
>
> ### 总结
>
> 对于手机银行App这类C端系统，选择G1 GC是一个比较好的选择，因为它**可以较好地平衡吞吐量和停顿时间**，并且适用于需要**低延迟**的应用场景。当然，最终的选择应该基于性能测试和监控结果，以确保选定的垃圾回收器能够满足应用的实际需求。

## 京东 淘宝等后端系统使用什么垃圾处理器

> 在谈论“垃圾处理器”时，我们通常指的是垃圾回收器（Garbage Collector, GC），它是Java虚拟机（JVM）的一部分，负责自动管理内存，回收不再使用的对象所占用的空间。不过，根据上下文，您可能是在询问与电商平台后端系统相关的垃圾处理器，这里我们主要讨论与垃圾回收器相关的内容。
>
> ### Java垃圾回收器
>
> 对于像京东、淘宝这样的大型电商平台后端系统，它们通常会选择能够提供**高性能、低延迟和可预测暂停时间**的垃圾回收器。JDK 8中提供了几种垃圾回收器选项，其中G1 (Garbage First) GC 是一个常用的选择，尤其是在需要**平衡吞吐量和停顿时间**的场景中。
>
> ### G1 GC的特点
>
> - **分区堆管理**：G1 GC 将堆划分为多个区域，并只清理那些最有可能产生垃圾的区域，从而减少了暂停时间。
> - **并行和并发**：G1 GC 可以在多个CPU核心上并行执行，同时也支持并发标记和清除。
> - **可预测的停顿时间**：G1 GC 可以通过配置参数来控制GC暂停时间的目标，这非常适合需要低延迟的应用。
> - **适用于大堆内存**：G1 GC 设计用于处理数十GB甚至更大的堆内存。
>
> ### 适用场景
>
> 对于像京东、淘宝这样的大型电商平台，它们通常具有以下特点：
>
> - **高并发**：这些平台需要处理大量的并发用户请求。
> - **大数据量**：需要处理大量的商品信息、用户信息等。
> - **低延迟要求**：为了保证良好的用户体验，需要尽可能减少延迟。
> - **资源利用**：需要高效利用服务器资源，包括CPU和内存。
>
> ### 配置示例
>
> 对于这些大型电商平台，可以考虑以下JVM配置来使用G1 GC：
>
> ```bash
> java -Xms16g -Xmx16g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35 -jar backend.jar
> ```
>
> - `-Xms16g` 和 `-Xmx16g` 设置初始和最大堆内存大小均为16GB。
> - `-XX:+UseG1GC` 启用G1 GC。
> - `-XX:MaxGCPauseMillis=200` 设置最大GC暂停时间为200毫秒。
> - `-XX:InitiatingHeapOccupancyPercent=35` 当堆内存占用达到35%时，触发G1 GC。
>
> ### 总结
>
> 对于像京东、淘宝这样的大型电商平台后端系统，G1 GC是一个很好的选择，因为它可以提供良好的性能和低延迟，同时支持大规模的应用场景。当然，最终的选择应该基于性能测试和监控结果，以确保选定的垃圾回收器能够满足应用的实际需求。
>
> 如果您是指的电商平台后端系统中处理垃圾数据的逻辑或工具，请提供更多背景信息，以便我可以提供更具体的答案。

## 数据批处理系统使用什么垃圾回收器（ETL数据侧）

>在数据批处理系统中，垃圾回收器的选择**通常侧重于最大化吞吐量和减少处理时间，因为这类系统通常不需要特别低的延迟，而是更加关注整体的处理效率**。对于数据批处理系统，JDK 8中推荐使用的垃圾回收器是 **Parallel GC** (并行垃圾回收器) 和 **G1 GC** (Garbage First GC)。
>
>### 1. Parallel GC (并行垃圾回收器)
>
>- **特点**：
>  - 专注于吞吐量最大化。
>  - 使用多线程并行执行垃圾回收过程。
>  - 适用于多核系统，能够利用多核的优势提高性能。
>  
>- **使用场景**：
>  - 适合批处理任务或者需要最大化吞吐量的应用程序。
>  - 适用于后台处理任务，如数据批处理系统。
>
>### 2. G1 GC (Garbage First GC)
>
>- **特点**：
>  - **平衡吞吐量和停顿时间**，适用于大堆内存。
>  - 设计用于大规模应用，可以处理数十GB的堆大小。
>  - 支持混合模式，可以在并发和非并发模式之间切换。
>  - 可以通过配置参数来控制GC暂停时间的目标。
>  
>- **使用场景**：
>  - 对于大堆内存的应用程序，G1 GC是一个非常好的选择。
>  - 适合企业级应用、大数据处理和云计算平台。
>  - 也可以用于需要低延迟的应用场景，尽管它的主要优势在于**平衡吞吐量和停顿时间**。
>
>### 数据批处理系统的考虑因素
>
>对于数据批处理系统，以下几点是选择垃圾回收器时需要考虑的：
>
>- **吞吐量**：数据批处理系统通常需要处理大量的数据，因此需要选择能够提供高吞吐量的垃圾回收器。
>- **堆内存大小**：数据批处理系统可能会使用大量的内存来存储数据，因此需要考虑垃圾回收器对于大堆内存的支持。
>- **并行处理**：数据批处理系统通常需要利用多核CPU的优势，因此需要选择支持并行处理的垃圾回收器。
>
>### 配置示例
>
>假设你正在配置一个数据批处理系统，可以考虑以下配置：
>
>#### 使用Parallel GC
>
>```bash
>java -Xms16g -Xmx16g -XX:+UseParallelGC -jar batchprocessor.jar
>```
>
>- `-Xms16g` 和 `-Xmx16g` 设置初始和最大堆内存大小均为16GB。
>- `-XX:+UseParallelGC` 启用Parallel GC。
>
>#### 使用G1 GC
>
>```bash
>java -Xms16g -Xmx16g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35 -jar batchprocessor.jar
>```
>
>- `-Xms16g` 和 `-Xmx16g` 设置初始和最大堆内存大小均为16GB。
>- `-XX:+UseG1GC` 启用G1 GC。
>- `-XX:MaxGCPauseMillis=200` 设置最大GC暂停时间为200毫秒。
>- `-XX:InitiatingHeapOccupancyPercent=35` 当堆内存占用达到35%时，触发G1 GC。
>
>### 总结
>
>对于数据批处理系统，推荐使用 **Parallel GC** 或者 **G1 GC**。如果系统更加**注重吞吐量最大化**，可以选择 **Parallel GC**；如果需要**平衡吞吐量和停顿时间**，尤其是对于大堆内存的应用，可以选择 **G1 GC**。最终的选择应该基于性能测试和监控结果，以确保选定的垃圾回收器能够满足应用的实际需求。

## JVM结构是什么样的

> https://cloud.tencent.com/developer/article/2379817



## Redis的数据结构

> https://xiaolincoding.com/redis/data_struct/command.html#string
>
> Redis有以下几种常用的数据类型：
>
> ![img](https://developer.qcloudimg.com/http-save/yehe-1275095/51549a275af2664874965ff4e8eb471f.png)
>
> https://xiaolincoding.com/redis/data_struct/command.html#%E6%80%BB%E7%BB%93



## 有3000个商品信息，id是商品b编号，其余六个字段描述商品信息，在分片集群模式下，用什么数据结构，可以在分片集群模式下批量查询的时候，redis的网络IO尽可能更少

> 在分片集群模式下，为了在批量查询时减少Redis的网络IO消耗，我们需要考虑如何将相关的数据尽可能地存储在同一分片内，以便减少跨分片的查询次数。同时，我们也需要考虑如何高效地组织数据，使得查询能够一次性获取尽可能多的信息，从而减少查询次数。
>
> 对于你提到的3000个商品信息，我们可以考虑以下几种策略来存储和查询数据：
>
> ### 数据结构选择
> 1. **哈希表 (Hash)**
>    - **优点**：每个商品可以作为一个哈希表存储，其中商品ID作为键，其他六个字段作为字段-值对。
>    - **查询**：使用`HGETALL`命令可以一次性获取整个商品的所有信息，减少网络往返次数。
>
> 2. **JSON 文档**
>    - **优点**：如果你使用的Redis版本支持JSON数据类型（如Redis 6.0及以上版本），可以将每个商品作为一个JSON文档存储。
>    - **查询**：使用`JSON.GET`命令获取整个文档或指定字段。
>
> 3. **字符串 (String)**
>    - **优点**：如果数据量不大，也可以将商品信息序列化为字符串存储。
>    - **查询**：使用`GET`命令获取整个商品信息字符串。
>
> ### 分片策略
> 为了确保相关数据位于同一分片，你可以考虑以下策略：
>
> 1. **按ID范围分片**
>    - 将商品按照ID范围分配到不同的分片中。例如，可以将商品ID进行哈希运算并映射到特定的分片上。
>    - **优点**：可以确保具有连续ID的商品位于同一分片，便于批量查询。
>
> 2. **按特定字段分片**
>    - 如果某些字段特别重要，可以基于这些字段进行分片。例如，假设商品有一个分类字段，可以根据这个字段将商品分配到不同的分片中。
>    - **优点**：可以针对特定属性进行优化查询。
>
> ### 批量查询策略
> 为了减少网络IO消耗，可以采用以下策略：
>
> 1. **批量获取**
>    - 使用`HMGET`（对于哈希表）或`MGET`（对于字符串）命令一次性获取多个商品的信息。
>    - **优点**：可以减少网络往返次数。
>
> 2. **管道化 (Pipelining)**
>    - 在客户端，可以将多个查询命令打包发送给Redis服务器，而不需要等待每个命令的响应。
>    - **优点**：减少了客户端与服务器之间命令交互的网络开销。
>
> 3. **脚本执行**
>    - 使用Redis的`EVAL`命令来运行Lua脚本，脚本可以在服务器端执行一系列操作。
>    - **优点**：可以在服务器端完成复杂的操作，减少网络通信。
>
> ### 示例
>
> 假设我们使用哈希表来存储商品信息，并且想要查询一批商品的信息，可以这样操作：
>
> 1. **创建哈希表**
>    ```redis
>    HMSET product:123 "name" "Product Name" "description" "Description" "price" 12.99 "category" "Electronics" "stock" 10 "manufacturer" "Manufacturer"
>    ```
>
> 2. **批量获取**
>    ```redis
>    HMGET product:123 product:456 product:789 "name" "price" "stock"
>    ```
>
> 3. **管道化**
>    ```python
>    # 假设使用Python Redis客户端
>    pipeline = redis_conn.pipeline()
>    for product_id in product_ids:
>        pipeline.hgetall(f'product:{product_id}')
>    results = pipeline.execute()
>    ```
>
> 通过上述策略，可以有效地减少Redis分片集群中的网络IO消耗。当然，具体的实施方案还需要根据你的实际需求和技术栈来确定。



## 分库分表

> 

## 在向分库分表数据库系统中添加新节点扩容或者单机数据库做迁移，怎么保证线上应用不受影响。

> ![image-20240824091344592](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240824091344592.png)
>
> 在向分库分表的数据库系统中添加新节点或进行单机数据库迁移时，保证线上应用不受影响是非常重要的。以下是几个关键点和建议，可以帮助你在进行这些操作时最小化对线上服务的影响，并确保缓存一致性。
>
> ### 1. 添加新节点
> 当你向分库分表系统中添加新节点时，需要确保数据的一致性和服务的连续性。
>
> #### 关键步骤：
> 1. **数据迁移**：使用工具（如MySQL的`mysqldump`、Oracle的`expdp`等）将数据从现有节点迁移到新节点。
> 2. **数据校验**：迁移完成后，确保数据的完整性和一致性。
> 3. **测试验证**：在不影响生产环境的情况下，进行充分的功能和性能测试。
> 4. **增量同步**：在正式切换之前，设置增量同步机制，确保新旧节点之间的数据差异最小。
> 5. **切流策略**：逐渐将流量从旧节点转移到新节点，比如先将读请求转移到新节点，观察一段时间后再转移写请求。
>
> ### 2. 单机数据库迁移
> 当需要迁移单机数据库时，可以采用类似的方法，但需要注意的是，由于涉及到整个数据库的迁移，因此需要更加谨慎。
>
> #### 关键步骤：
> 1. **备份与恢复**：使用备份工具将数据完整地备份下来，然后在新的数据库环境中恢复。
> 2. **增量更新**：使用日志文件或其他工具捕获迁移期间的数据变更，确保迁移过程中的数据一致性。
> 3. **服务切换**：逐步切换服务，先从只读开始，再逐渐切换写操作。
> 4. **监控与回滚计划**：在整个迁移过程中实施严格的监控，并准备好回滚计划。
>



## jetcache 缓存一致性

> * 1、多级缓存如何保证local本地jvm内存缓存、remote redis缓存、数据库的一致性；
>
>   * 1、更新请求经过负载均衡打到了某个节点，只能更新当前节点本地jvm和redis，如果新的请求打到其他节点的本地jvm，就会出现不一致的情况；
>
>     2、针对上述1的问题，通过syncLocal可以让其他jvm里面的本地缓存失效，这样的话，所有节点本地jvm缓存失效，但redis中还存在并且存的是新数据，当新请求打过来，本地jvm缓存未命中，会请求redis中的新数据，然后把redis中的数据刷新到本地jvm中，再将新数据返回给用户，这样就保证了缓存的一致性。
>
>   * 3、基于Redis发布/订阅模式，通过broadcastChannel和syncLocal设定，在操作(更新/删除)当前节点时将集群中除本节点以外的其他节点本地缓存失效，保证多级缓存一致性，保证集群环境下多节点多级缓存一致性。
>
> * 2、二级缓存使用场景：
>
>   * 内存有限、比较珍贵，一般二级缓存在面对非常高的并发量的时候，为了缓解频繁Redis IO才使用采用二级缓存。部分接口对性能要求比较高，手机银行app包我们的接口 ，同时我们提供给手机银行的接口中的数据源有些来自于主机，我们的接口包主机接口，返回给手机银行app，这部分数据不会落msyql，比如五星基金，定时每五分钟@CacheUpdate五星基金接口数据。

## jetcache定时刷新缓存时，比如使用@CacheRefresh，jetcache通过分布式锁保证在一个刷新周期内只有一个节点执行刷新操作。:star:

有没有使用微服务？

为什么加入招银、又为什么离开？

招银的职级和绩效、期待涨幅

## 遇到的最大的困难是什么、怎么解决的？

## 你认为的高并发高可用系统是什么样的，怎么建设？

> https://developer.aliyun.com/article/1311642

# 科大讯飞-消费者BG 1面

8.27 14:00 ~ 14:30

自我介绍

项目介绍、项目中使你成长最大的地方？

数据侧详细介绍怎么做的，结合具体例子说明

项目有几个人负责开发，开发模式是什么样的？

> 项目负责人接到需求，独自做需求分析和开发设计，然后组内成员一起做开发设计评审，然后项目负责人实施，接入测试，ST、UAT，产品经理、业务验证，验收通过进行上线投产，上线后持续监控项目运行情况，必要时人工介入，修复调整。

技术预研举个详细例子

> 举了建设产品异动监控系统预研，参考业界已有的风控系统，比如度小满金融，再结合我们实际的业务场景进行设计规划建设。
>
> 还举了使用前端选用组件展示PDF的例子，暴露也写前端了.......

项目中有使用什么设计模式吗？

责任链模式具体怎么实现的，通过什么字段判断？

未来规划

除了工作之外，平时关注什么技术，通过什么渠道、怎么学习的

了解大模型吗

离职原因

反问：部门负责的业务和大模型有关吗？（消费者BG负责虚拟人、数字人相关业务，和大模型相关）



# 涂鸦智能1面

8.27 17:00 ~ 17:26

自我介绍

介绍项目

不同用户角色视角看到的数据不一样，差异化和个性化怎么做的？

机构树是什么样的、表结构什么样的、数据量大概多少

数据任务有多少、数据任务时效性

是否对DS数据任务进行持久化

## Dolphin Scheduler原理是什么样的

> https://www.bookstack.cn/read/dolphinscheduler-3.0.0-zh/20645ed9c701aaea.md
>
> https://blog.csdn.net/DolphinScheduler/article/details/140697631
>
> Apache DolphinScheduler 是一个用于大数据处理任务调度的强大工具，它支持分布式部署，具备高可用性和可扩展性。下面将详细介绍其核心原理和架构设计。
>
> ### 核心原理
>
> #### 1. **工作流与任务**
> DolphinScheduler 使用 DAG（有向无环图）来表示任务之间的依赖关系。每个 DAG 包含一个或多个任务（Task），这些任务可以是各种类型的操作，例如 Shell 脚本、MapReduce、Spark 作业、SQL 查询等。任务实例（Task Instance）代表特定任务的一次执行，而流程实例（Workflow Instance）则是流程定义（Workflow Definition）的一次执行。
>
> #### 2. **调度机制**
> DolphinScheduler 使用 Quartz 作为其分布式调度器，并支持 cron 表达式的可视化生成。这意味着用户可以在界面上方便地设置任务的调度周期，例如每天凌晨执行一次数据同步操作。
>
> #### 3. **架构设计**
> DolphinScheduler 的架构经历了多次迭代，其中一个重要改进是从基于 ZooKeeper 的任务队列转变为引入 Netty 进行 Master 和 Worker 之间的通信。这一变化提高了系统的通信效率和可靠性。
>
> - **Master 组件**：负责接收来自用户界面的请求，并进行任务调度决策。它还管理着工作流的生命周期，包括创建、启动、停止等操作。
> - **Worker 组件**：执行由 Master 分配的具体任务。Worker 不再直接操作数据库，而是通过 Master 与 MetaStore 交互，减轻了数据库的压力。
> - **MetaStore**：通常是关系型数据库（如 MySQL），用于存储任务定义、执行状态等元数据信息。
> - **Web UI**：提供了可视化的界面让用户能够轻松地设计、配置和监控工作流。
>
> #### 4. **任务类型与执行**
> DolphinScheduler 支持多种任务类型，如 SQL、Shell、Spark 等。此外，还可以通过自定义插件支持更多的任务类型。当一个任务被调度执行时，Master 会将任务分配给可用的 Worker 节点，Worker 节点则负责实际的任务执行，并将执行结果反馈给 Master。
>
> #### 5. **任务调度算法**
> Master 在分发任务给 Worker 时，提供了几种不同的算法，包括随机、轮询和资源线性加权。这使得任务调度更为灵活，可以根据实际需要选择最适合的调度策略。
>
> ### 总结
> Apache DolphinScheduler 通过其先进的架构设计和丰富的功能特性，为企业级用户提供了强大的任务调度解决方案。它不仅支持多种任务类型，还允许用户通过友好的用户界面来管理复杂的工作流程。随着社区的发展，DolphinScheduler 持续地吸收社区用户的反馈，不断优化其功能，使其成为数据处理领域的重要工具之一。

DS技术选型是你一个人做的吗？怎么告警？

RabbitMQ使用

为什么离职、有没有考核、一年考核几次

反问：业务线、技术栈、这轮结果大概多久反馈（一周以内）



# B站 1面

8.28 17:00~18:00

自我介绍

然后就做题

给一个ip字符串，返回合法的ip地址组合列表，比如"1111"返回["1.1.1.1"]，给你"25525525225" 返回["255.255.25.225", "255.255.252.25"]

写出来了，分割子串 + 回溯，但是测试用例没通过，边界条件有点问题。。。。。。

> https://leetcode.cn/problems/restore-ip-addresses/description/
>
> 原来是leetcode93，原来没做过，不是hot100，hot150中的

挑一个项目，你觉得最值得说的谈谈

为什么使用jetcache二级缓存，使用redission、jedis、lettuce不行吗？

jetcache原理

基于redis的pub/sub，怎么保证高可用

> 回答了项目基于jetcache框架提供的能力，如果想自己实现也可以，jetcache扩展性很好，提供了相应的接口自己扩展实现发布订阅模式，比如基于RabbitMQ实现。

面试官追问，使用rabbitmq怎么实现发布订阅，怎么保证高可用？

> 生产者发送消息手动确认；交换机、队列、消息持久化；消费者手动确认
>
> 在使用 RabbitMQ 实现发布/订阅（Publish/Subscribe，简称 Pub/Sub）模式时，确保系统的高可用性是非常重要的。发布/订阅模式允许一个或多个生产者向一个特定的交换机发送消息，而多个消费者可以绑定到同一个交换机上并接收消息。为了确保系统在各种情况下都能正常运行，我们需要考虑以下几个方面来实现高可用性：
>
> ### 1. 集群化 RabbitMQ 节点
> RabbitMQ 支持集群模式，可以将多个 RabbitMQ 节点组合成一个集群。这样即使某个节点发生故障，其他节点仍然可以继续处理消息。集群中的所有节点都可以接受客户端连接，并且可以通过镜像队列（Mirrored Queues）来确保消息的持久性和高可用性。
>
> ### 2. 镜像队列
> 在 RabbitMQ 中，可以**通过配置镜像队列来确保消息在集群内的多个节点上有副本**。当消息发送到交换机时，它会被路由到一个队列，并且这个队列的所有副本都会接收到该消息。这样，即使主节点失效，其他节点上的队列副本仍然可以继续服务。
>
> 要启用队列镜像，你需要在集群中的任意一个节点上安装 `rabbitmqha` 插件，并且在策略中指定哪些队列应该被镜像。
>
> ```sh
> rabbitmq-plugins enable rabbitmqha
> rabbitmqctl set_policy ha-all '.*' '{"ha-mode":"all"}'
> ```
>
> 上述命令将镜像所有队列到集群中的所有节点。
>
> ### 3. 数据持久化
> 为了防止消息丢失，可以将消息设置为持久化的。这意味着消息会被写入磁盘，即使 RabbitMQ 重启后也能恢复消息。持久化可以通过在发布消息时设置消息的 `delivery_mode` 属性为 `2` 来实现。
>
> ```python
> channel.basic_publish(
>     exchange='my_exchange',
>     routing_key='',
>     body='Hello World!',
>     properties=pika.BasicProperties(
>         delivery_mode = pika.spec.PERSISTENT_DELIVERY_MODE
>     )
> )
> ```
>
> ### 4. 客户端容错
> 客户端应该实现适当的重试逻辑，以应对网络问题或其他暂时性的故障。例如，在尝试连接失败时，客户端可以等待一段时间后重新尝试连接。
>
> ### 5. 监控和报警
> 为了更好地管理和维护集群，应该实施全面的监控策略，并设置相应的报警机制。这样可以在出现问题时及时得到通知，并采取行动。
>
> ### 6. 自动故障转移
> 如果使用的是 RabbitMQ 高可用性插件，那么可以配置自动故障转移（autoheal），当检测到网络分区时，可以自动将队列转移到另一个活跃的节点。
>
> 通过以上措施，你可以构建一个健壮的基于 RabbitMQ 的发布/订阅系统，该系统能够在不同的故障场景下保持服务的连续性。当然，具体实现时还需要根据实际情况调整配置，确保满足业务需求的同时也考虑到性能和成本等因素。

面试官是to C部门的， 只是帮目前岗位to B的部门面试，让讲讲有没有建设B端平台的经验。



为什么离职？

> 为了更好的发展，招银虽然不是国企，但是有些地方和国企比较相似，存在论资排辈的情况，最新的员工，绩效强制分布，入职以来就去年来了一个新员工，绩效保护期，除了他我就是最新的员工，背了低绩效，合同到期选择没有续签，寻求更好的发展



# ZOOM 1面

8.29 15:00 ~ 15:50	两个面试官一起面

自我介绍

面试官对着简历逐行提问。。。。。。第一次遇到这种情况

问了DolphinScheduler平台相关的很多，原理，怎么保证DS高可用

> https://segmentfault.com/a/1190000045156061

任务拆分怎么保证分发到具体哪个节点执行？

任务失败发送告警怎么实现的？

Master将任务分发到Worker的负载均衡策略有哪些？

> - Master的角色主要负责任务分发并监督Slave的健康状态，可以动态的将任务均衡到Slave上，以致Slave节点不至于“忙死”或”闲死”的状态。
> - Worker的角色主要负责任务的执行工作并维护和Master的心跳，以便Master可以分配任务给Slave。

## ES深分页了解吗？怎么解决？Filter和Query有什么区别？

> https://www.cnblogs.com/crazymakercircle/p/15973775.html
>
> https://cloud.tencent.com/developer/article/2416935
>
> >  在Elasticsearch中，分页是查询操作中不可或缺的一部分。随着数据量的增长，如何高效地分页查询数据急需需要面对的问题。Elasticsearch提供了三种主要的分页方式：`from + size`、`scroll`和`search_after`。下面详细介绍这三种分页方式的特点和使用场景。
>
> ![img](https://developer.qcloudimg.com/http-save/yehe-10752309/4466e37712cd5ce44f0ef1a785a81245.jpg)

策略模式怎么实现的？

## 抽象类和接口区别？策略模式使用接口还是抽象类实现的？

> https://javaguide.cn/java/basis/java-basic-questions-02.html#%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB%E6%9C%89%E4%BB%80%E4%B9%88%E5%85%B1%E5%90%8C%E7%82%B9%E5%92%8C%E5%8C%BA%E5%88%AB
>
> https://javabetter.cn/oo/abstract-vs-interface.html#_03%E3%80%81%E4%B8%A4%E8%80%85%E5%B7%AE%E5%88%AB
>
> #### [1）语法层面上](https://javabetter.cn/oo/abstract-vs-interface.html#_1-语法层面上)
>
> - 抽象类可以提供成员方法的实现细节，而接口中只能存在 public abstract 方法；
> - 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的；
> - 接口中不能含有静态代码块，而抽象类可以有静态代码块；
> - 一个类只能继承一个抽象类，而一个类却可以实现多个接口。
>
> #### [2）设计层面上](https://javabetter.cn/oo/abstract-vs-interface.html#_2-设计层面上)
>
> 抽象类是对一种事物的抽象，即对类抽象，继承抽象类的子类和抽象类本身是一种 `is-a` 的关系。而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。
>
> 举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，就是都会飞。那么在设计的时候，可以将飞机设计为一个类 Airplane，将鸟设计为一个类 Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。
>
> 此时可以将 飞行 设计为一个接口 Fly，包含方法 fly()，然后 Airplane 和 Bird 分别根据自己的需要实现 Fly  这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承 Airplane 即可，对于鸟也是类似的，不同种类的鸟直接继承 Bird  类即可。从这里可以看出，继承是一个 "是不是"的关系，而 接口 实现则是  "有没有"的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），能飞行则可以实现这个接口，不能飞行就不实现这个接口。
>
> 接口是对类的某种行为的一种抽象，接口和类之间并没有很强的关联关系，举个例子来说，所有的类都可以实现 [`Serializable` 接口](https://javabetter.cn/io/Serializbale.html)，从而具有序列化的功能，但不能说所有的类和 Serializable 之间是 `is-a` 的关系。



机构树数据结构是什么样的，在Redis里面怎么存的？除了String还可以存什么结构?

## Redis 大key了解吗？怎么解决？怎么查询哪些是大key？

> https://javaguide.cn/database/redis/redis-questions-02.html#redis-bigkey-%E5%A4%A7-key
>
> #### [如何处理 bigkey？](#如何处理-bigkey)
>
> bigkey 的常见处理以及优化办法如下（这些方法可以配合起来使用）：
>
> - **分割 bigkey**：将一个 bigkey 分割为多个小 key。例如，将一个含有上万字段数量的 Hash 按照一定策略（比如二次哈希）拆分为多个 Hash。
>- **手动清理**：Redis 4.0+ 可以使用 `UNLINK` 命令来异步删除一个或多个指定的 key。Redis 4.0 以下可以考虑使用 `SCAN` 命令结合 `DEL` 命令来分批次删除。
> - **采用合适的数据结构**：例如，文件二进制数据不使用 String 保存、使用 HyperLogLog 统计页面 UV、Bitmap 保存状态信息（0/1）。
> - **开启 lazy-free（惰性删除/延迟释放）** ：lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程。
> 
> https://xiaolincoding.com/redis/base/redis_interview.html#redis-%E7%9A%84%E5%A4%A7-key-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86

# 追觅1面

8.29 19:00 ~ 19:46	两个面试官 md我开了摄像头，他俩没开摄像头

吊打面试官系列，哈哈哈哈，开个玩笑

自我介绍

## HashMap的实现

> JDK1.7和JDK1.8

## ArrayList的现实

## 有一个数据量很大的表，每一条记录其中一个字段的值是'http'，update操作改成'https'，会有什么问题，要怎么解决

> https://xiaolincoding.com/mysql/lock/update_index.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%8F%91%E7%94%9F%E8%BF%99%E7%A7%8D%E7%9A%84%E4%BA%8B%E6%95%85
>
> https://blog.csdn.net/LS7011846/article/details/109662061
>
> https://javaguide.cn/database/mysql/mysql-high-performance-optimization-specification-recommendations.html#%E8%B6%85-100-%E4%B8%87%E8%A1%8C%E7%9A%84%E6%89%B9%E9%87%8F%E5%86%99-update-delete-insert-%E6%93%8D%E4%BD%9C-%E8%A6%81%E5%88%86%E6%89%B9%E5%A4%9A%E6%AC%A1%E8%BF%9B%E8%A1%8C%E6%93%8D%E4%BD%9C

## mysql索引失效的场景

## MySQL主从同步怎么实现的、怎么解决主从延迟问题

> https://developer.aliyun.com/article/1169490
>
> ![image-20240829221111646](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240829221111646.png)
>
> ![image-20240829223745588](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240829223745588.png)

## 线程有哪些状态？BLOCKED和WAITING区别

> https://javabetter.cn/thread/thread-state-and-method.html#blocked-%E4%B8%8E-runnable-%E7%8A%B6%E6%80%81%E7%9A%84%E8%BD%AC%E6%8D%A2
>
> 线程的状态可以参考JDK中的Thread类中的枚举State
>
> ```java
> public enum State {
>         /**
>          * 尚未启动的线程的线程状态
>          */
>         NEW,
> 
>         /**
>          * 可运行线程的线程状态。处于可运行状态的线程正在 Java 虚拟机中执行，但它可能正在等待来自		 * 操作系统的其他资源，例如处理器。
>          */
>         RUNNABLE,
> 
>         /**
>          * 线程阻塞等待监视器锁的线程状态。处于阻塞状态的线程正在等待监视器锁进入同步块/方法或在调          * 用Object.wait后重新进入同步块/方法。
>          */
>         BLOCKED,
> 
>         /**
>          * 等待线程的线程状态。由于调用以下方法之一，线程处于等待状态：
> 		* Object.wait没有超时
>          * 没有超时的Thread.join
>          * LockSupport.park
>          * 处于等待状态的线程正在等待另一个线程执行特定操作。
>          * 例如，一个对对象调用Object.wait()的线程正在等待另一个线程对该对象调用Object.notify()			* 或Object.notifyAll() 。已调用Thread.join()的线程正在等待指定线程终止。
>          */
>         WAITING,
> 
>         /**
>          * 具有指定等待时间的等待线程的线程状态。由于以指定的正等待时间调用以下方法之一，线程处于定          * 时等待状态：
> 		* Thread.sleep
> 		* Object.wait超时
> 		* Thread.join超时
> 		* LockSupport.parkNanos
> 		* LockSupport.parkUntil
>          * </ul>
>          */
>         TIMED_WAITING,
> 
>         /**
>          * 已终止线程的线程状态。线程已完成执行
>          */
>         TERMINATED;
>     }
> ```
>
> 状态之间是如何变化的
>
> ![image-20230503203629212](D:/【八股文】/2023版Java面试-黑马/08-并发编程篇/讲义/多线程相关面试题.assets/image-20230503203629212.png)
>
> 分别是
>
> * 新建
>   * 当一个线程对象被创建，但还未调用 start 方法时处于**新建**状态
>   * 此时未与操作系统底层线程关联
> * 可运行
>   * 调用了 start 方法，就会由**新建**进入**可运行**
>   * 此时与底层线程关联，由操作系统调度执行
> * 终结
>   * 线程内代码已经执行完毕，由**可运行**进入**终结**
>   * 此时会取消与底层线程关联
> * 阻塞
>   * 当获取锁失败后，由**可运行**进入 Monitor 的阻塞队列**阻塞**，此时不占用 cpu 时间
>   * 当持锁线程释放锁时，会按照一定规则唤醒阻塞队列中的**阻塞**线程，唤醒后的线程进入**可运行**状态
> * 等待
>   * 当获取锁成功后，但由于条件不满足，调用了 wait() 方法，此时从**可运行**状态释放锁进入 Monitor 等待集合**等待**，同样不占用 cpu 时间
>   * 当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的**等待**线程，恢复为**可运行**状态
> * 有时限等待
>   * 当获取锁成功后，但由于条件不满足，调用了 wait(long) 方法，此时从**可运行**状态释放锁进入 Monitor 等待集合进行**有时限等待**，同样不占用 cpu 时间
>   * 当其它持锁线程调用 notify() 或 notifyAll() 方法，会按照一定规则唤醒等待集合中的**有时限等待**线程，恢复为**可运行**状态，并重新去竞争锁
>   * 如果等待超时，也会从**有时限等待**状态恢复为**可运行**状态，并重新去竞争锁
>   * 还有一种情况是调用 sleep(long) 方法也会从**可运行**状态进入**有时限等待**状态，但与 Monitor 无关，不需要主动唤醒，超时时间到自然恢复为**可运行**状态

## volatile关键字

## CAS和AQS

> https://javaguide.cn/java/concurrent/cas.html
>
> https://javaguide.cn/java/concurrent/aqs.html
>
> ![image-20240902184818930](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240902184818930.png)

## 反射，项目中具体使用反射详情，性能有没有影响？

> 讲了接口获取指标，指标很多需要一个一个判断把指标值set到实体中的例子，通过反射拼接得到set方法，执行，减少很多分支逻辑
>
> 数据跑批任务中使用的，不是面向C端的接口使用的，没什么性能影响

## Spring AOP和IOC

## 用的JDK几，默认垃圾回收器，了解不了解G1

> 面试官说JDK8垃圾回收器模式G1，我纠正了他

## 项目用的什么垃圾回收器

> 数据侧：JDK8默认垃圾回收器；数字化赋能侧：G1，面向服务端的垃圾回收器

## 了不了解CAP

## jetcache的原理

## 有没有用过nacos、eureka、项目是怎么部署的？

## 产品异动监控系统怎么实现的？

## 遇到的最大的挑战、谈谈你理解的高可用，怎么实现保障高可用

> 高可用是一个很大的课题，根据木桶理论，一个水桶能容纳水的上限取决于木桶中最低的那块木板，同样的对一个系统来说，系统的高可用和系统中的各个模块的高可用相关，保证系统的高可用需要保证各个模块各个环节的高可用。
>
> 

# 滴滴一面

8.30 11:00 ~ 11:45 Md第一次遇到面试官不开摄像头，就我自己开，名字好像叫liaofangshun?

脉脉上查了下，廖方顺，西工大硕

自我介绍

## 线程池参数、工作机制

## 类加载机制、双亲委派

> https://javaguide.cn/java/jvm/classloader.html
>
> https://cloud.tencent.com/developer/article/2055271

## 如果自己定义一个和jdk完全一样的String，编译器会报错吗？

> https://cloud.tencent.com/developer/article/2055271
>
> 自定义java.lang.String，编译器会报错，`Prohibited package name: java.lang`
>
> ![image-20240831171358384](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240831171358384.png)
>
> 包名不是`java.lang`，比如`com.jingdyangx.String`，编译运行正常：
>
> ```java
> package com.jingdyangx;
> public class String {
>  private java.lang.String name;
>  private Integer age;
>  public String(java.lang.String name, Integer age) {
>      this.name = name;
>      this.age = age;
>  }
>  @Override
>  public java.lang.String toString() {
>      return "String{" +
>              "name='" + name + '\'' +
>              ", age=" + age +
>              '}';
>  }
> }
> ```
>
> ```java
> package com.jingdyangx;
> public class Test {
>  public static void main(java.lang.String[] args) {
>      System.out.println("Hello World!");
>      String string = new String("张三", 18);
>      System.out.println(string);
>  }
> }
> ```
>
> ![image-20240831171623035](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240831171623035.png)
>
> ![image-20240915105652777](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240915105652777.png)
>
> ![image-20240915105707412](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240915105707412.png)
>
> https://www.bilibili.com/video/BV1X5411K7cw/?spm_id_from=333.337.search-card.all.click&vd_source=d88527641a2c5d74436ce88743d2fd55

## java有哪些方式可以保证同步代码块安全执行的？

> 在 Java 中，保证同步代码块安全执行的方式主要包括使用内置的同步机制（如 `synchronized` 关键字）和其他高级的并发工具类（如 `ReentrantLock`）。以下是几种常见的同步机制：
>
> ### 1. 使用 `synchronized` 关键字
>
> `Synchronized` 是 Java 内置的关键字，可以用于实现线程间的同步。它可以作用于方法或代码块。
>
> #### 同步方法
>
> ```java
> public class Counter {
>     private int count = 0;
> 
>     // 同步方法
>     public synchronized void increment() {
>         count++;
>     }
> 
>     // 同步方法
>     public synchronized int getCount() {
>         return count;
>     }
> }
> ```
>
> #### 同步代码块
>
> ```java
> public class Counter {
>     private int count = 0;
>     private Object lock = new Object();
> 
>     // 同步代码块
>     public void increment() {
>         synchronized (lock) {
>             count++;
>         }
>     }
> 
>     // 同步代码块
>     public int getCount() {
>         synchronized (lock) {
>             return count;
>         }
>     }
> }
> ```
>
> ### 2. 使用 `ReentrantLock`
>
> `ReentrantLock` 是 Java 并发库中的一个高级锁，它提供了比 `synchronized` 更多的控制功能，如可中断的锁等待、公平锁等。
>
> #### 示例
>
> ```java
> import java.util.concurrent.locks.Lock;
> import java.util.concurrent.locks.ReentrantLock;
> 
> public class Counter {
>     private int count = 0;
>     private final Lock lock = new ReentrantLock();
> 
>     public void increment() {
>         lock.lock();
>         try {
>             count++;
>         } finally {
>             lock.unlock();
>         }
>     }
> 
>     public int getCount() {
>         lock.lock();
>         try {
>             return count;
>         } finally {
>             lock.unlock();
>         }
>     }
> }
> ```
>
> ### 3. 使用 `ReentrantReadWriteLock`
>
> `ReentrantReadWriteLock` 允许多个读取线程同时访问共享资源，但只允许一个写入线程访问资源。读取线程可以同时进行读操作，但写入线程必须独占资源。
>
> #### 示例
>
> ```java
> import java.util.concurrent.locks.Lock;
> import java.util.concurrent.locks.ReentrantReadWriteLock;
> 
> public class DataStore {
>     private String data = "";
>     private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
> 
>     public void write(String newData) {
>         Lock writeLock = lock.writeLock();
>         writeLock.lock();
>         try {
>             data = newData;
>         } finally {
>             writeLock.unlock();
>         }
>     }
> 
>     public String read() {
>         Lock readLock = lock.readLock();
>         readLock.lock();
>         try {
>             return data;
>         } finally {
>             readLock.unlock();
>         }
>     }
> }
> ```
>
> ### 4. 使用 `Atomic` 类
>
> Java 并发库提供了 `java.util.concurrent.atomic` 包，其中包含了一组原子类（如 `AtomicInteger`），它们提供了线程安全的整数操作。
>
> #### 示例
>
> ```java
> import java.util.concurrent.atomic.AtomicInteger;
> 
> public class Counter {
>     private AtomicInteger count = new AtomicInteger(0);
> 
>     public void increment() {
>         count.incrementAndGet();
>     }
> 
>     public int getCount() {
>         return count.get();
>     }
> }
> ```
>
> ### 5. 使用 `ThreadLocal`
>
> `ThreadLocal` 变量可以确保每个线程都有自己独立的变量副本，从而避免了线程安全问题。
>
> #### 示例
>
> ```java
> public class ThreadSafeCounter {
>     private static final ThreadLocal<Integer> localCount = ThreadLocal.withInitial(() -> 0);
> 
>     public void increment() {
>         localCount.get().incrementAndGet();
>     }
> 
>     public int getCount() {
>         return localCount.get();
>     }
> }
> ```
>
> ### 6. 使用 `volatile` 关键字
>
> `Volatile` 变量可以确保所有线程看到的变量值是最新的，但它不提供原子性操作。因此，对于复合操作（如 `++`），仍然需要同步。
>
> #### 示例
>
> ```java
> public class Counter {
>     private volatile int count = 0;
> 
>     public void increment() {
>         synchronized (this) {
>             count++;
>         }
>     }
> 
>     public int getCount() {
>         return count;
>     }
> }
> ```
>
> ### 7. 使用 `ConcurrentHashMap`
>
> `ConcurrentHashMap` 是一个线程安全的哈希表，它通过分段锁来实现高效的并发操作。
>
> #### 示例
>
> ```java
> import java.util.concurrent.ConcurrentHashMap;
> 
> public class DataStore {
>     private ConcurrentHashMap<String, String> map = new ConcurrentHashMap<>();
> 
>     public void put(String key, String value) {
>         map.put(key, value);
>     }
> 
>     public String get(String key) {
>         return map.get(key);
>     }
> }
> ```
>
> ### 总结
>
> 选择哪种同步机制取决于具体的应用场景。`synchronized` 是最简单的同步方式，适合简单同步需求；`ReentrantLock` 提供了更灵活的锁控制；`Atomic` 类适用于简单的数值操作；`ThreadLocal` 适用于每个线程都需要独立变量副本的情况；`volatile` 适用于需要可见性保证的简单变量；`ConcurrentHashMap` 则适用于需要线程安全的哈希表操作。在实际开发中，可以根据具体需求选择合适的同步机制。

## sychronized可重入吗？发生异常会释放锁吗?

> ### [synchronized 底层原理了解吗？](https://javaguide.cn/java/concurrent/java-concurrent-questions-02.html#synchronized-底层原理了解吗)
>
> synchronized 关键字底层原理属于 JVM 层面的东西。
>
> #### [synchronized 同步语句块的情况](https://javaguide.cn/java/concurrent/java-concurrent-questions-02.html#synchronized-同步语句块的情况)
>
> ```java
> public class SynchronizedDemo {
>     public void method() {
>         synchronized (this) {
>             System.out.println("synchronized 代码块");
>         }
>     }
> }
> ```
>
> 通过 JDK 自带的 `javap` 命令查看 `SynchronizedDemo` 类的相关字节码信息：首先切换到类的对应目录执行 `javac SynchronizedDemo.java` 命令生成编译后的 .class 文件，然后执行`javap -c -s -v -l SynchronizedDemo.class`。
>
> ![synchronized关键字原理](https://oss.javaguide.cn/github/javaguide/java/concurrent/synchronized-principle.png)																				synchronized关键字原理
>
> 从上面我们可以看出：**`synchronized` 同步语句块的实现使用的是 `monitorenter` 和 `monitorexit` 指令，其中 `monitorenter` 指令指向同步代码块的开始位置，`monitorexit` 指令则指明同步代码块的结束位置。**
>
> 上面的字节码中包含一个 `monitorenter` 指令以及两个 `monitorexit` 指令，这是为了保证锁在同步代码块代码正常执行以及出现异常的这两种情况下都能被正确释放。
>
> 

## sychronized加在类上、静态方法上、成员方法上有什么区别？

> 在 Java 中，`synchronized` 关键字可以应用于类、静态方法以及实例方法。每种应用方式都有其特定的作用域和锁定范围，了解这些区别有助于更好地设计线程安全的代码。下面详细解释每种情况的区别：
>
> ### 1. `synchronized` 应用于类
>
> 当 `synchronized` 作用于类时，实际上是对类的 `Class` 对象进行加锁。这意味着任何对这个类的 `synchronized` 块或方法的操作都会受到这个锁的影响。
>
> #### 示例
>
> ```java
> public class MyClass {
>     public void someMethod() {
>         synchronized (MyClass.class) {  // 锁定 MyClass 的 Class 对象
>         }
>     }
> }
> ```
>
> ### 2. `synchronized` 应用于静态方法
>
> 当 `synchronized` 作用于静态方法时，实际上也是对类的 `Class` 对象进行加锁。这意味着静态方法之间的互斥访问也是基于类的 `Class` 对象。
>
> #### 示例
>
> ```java
> public class MyClass {
>     public static synchronized void staticMethod() {
>     }
> }
> ```
>
> ### 3. `synchronized` 应用于成员方法
>
> 当 `synchronized` 作用于成员方法时，实际上是对当前对象（`this`）进行加锁。这意味着所有调用该对象的 `synchronized` 成员方法的操作都必须排队执行。
>
> #### 示例
>
> ```java
> public class MyClass {
>     public synchronized void instanceMethod() {
>     }
> }
> ```
>
> ### 不同应用场景的比较
>
> #### 锁定对象的不同
>
> - **类对象锁** (`synchronized` 应用于类或静态方法)：
>   - 锁定的是类的 `Class` 对象。
>   - 适用于所有静态方法之间的互斥访问。
>   - 适用于所有通过类名调用的 `synchronized` 块。
>
> - **实例对象锁** (`synchronized` 应用于成员方法)：
>   - 锁定的是当前对象的引用（`this`）。
>   - 适用于同一对象的所有 `synchronized` 成员方法之间的互斥访问。
>
> #### 示例比较
>
> 假设我们有两个线程 `Thread-1` 和 `Thread-2`，它们都在操作 `MyClass` 的实例 `obj`：
>
> ```java
> public class MyClass {
>     public static synchronized void staticMethod() {
>         System.out.println(Thread.currentThread().getName() + " is executing staticMethod");
>     }
> 
>     public synchronized void instanceMethod() {
>         System.out.println(Thread.currentThread().getName() + " is executing instanceMethod");
>     }
> }
> 
> public class Main {
>     public static void main(String[] args) {
>         MyClass obj = new MyClass();
> 
>         Thread thread1 = new Thread(() -> {
>             for (int i = 0; i < 5; i++) {
>                 obj.instanceMethod();
>             }
>         }, "Thread-1");
> 
>         Thread thread2 = new Thread(() -> {
>             for (int i = 0; i < 5; i++) {
>                 obj.staticMethod();
>             }
>         }, "Thread-2");
> 
>         thread1.start();
>         thread2.start();
>     }
> }
> ```
>
> 在这个例子中：
>
> - `Thread-1` 调用 `instanceMethod`，它会对 `obj` 对象进行加锁。
> - `Thread-2` 调用 `staticMethod`，它会对 `MyClass.class` 对象进行加锁。
>
> 由于 `obj` 和 `MyClass.class` 是不同的对象，因此这两个线程可以并行执行它们的方法。
>
> ### 总结
>
> - **类对象锁**：锁定的是类的 `Class` 对象，适用于静态方法之间的互斥访问。
> - **实例对象锁**：锁定的是当前对象的引用（`this`），适用于同一对象的所有 `synchronized` 成员方法之间的互斥访问。
>
> 选择哪种锁定方式取决于你希望在哪个层次上实现互斥访问。如果你希望在类级别上实现互斥访问，可以使用类对象锁；如果你希望在对象实例级别上实现互斥访问，可以使用实例对象锁。在实际应用中，应根据具体的业务逻辑选择合适的锁定策略。

## 谈谈垃圾回收机制

## 用的什么垃圾回收器？

## spring bean的实例化过程

## @Autowired和@Resource区别

> https://developer.aliyun.com/article/1003903
>
> 1. 来源不同：@Autowired 来自 Spring 框架，而 @Resource 来自于（Java）JSR-250；
>
> 2. 依赖查找的顺序不同：@Autowired 先根据类型再根据名称查询，而 @Resource 先根据名称再根据类型查询；（@Autowird默认的注入方式为byType，也就是根据类型匹配，当有多个实现时，则通过byName注入，也可以通过配合@Qualifier注解来显式指定name值，指明要使用哪个具体的实现类，而@Resource注解默认通过byName方式注入）
>
> 3. 支持的参数不同：@Autowired 只支持设置 1 个参数，而 @Resource 支持设置 7 个参数；
>
> 4. 编译器 IDEA 的提示不同：当注入 Mapper 对象时，使用 @Autowired 注解编译器会提示错误，而使用 @Resource 注解则不会提示错误。
>
> 5. @Autowired注解注入的对象需要在IOC容器中存在，否则需要加上属性required=false，表示忽略当前要注入的bean，如果有直接注入，没有跳过，不会报错；如果没有指定required=false，且@Autowired` 注解的字段或方法没有找到合适的 Bean，Spring 会抛出 `org.springframework.beans.factory.UnsatisfiedDependencyException` 异常
>
>    @Resource：如果 `@Resource` 注解的字段或方法没有找到合适的 Bean，它不会抛出异常，而是让字段保持 `null`（除非指定了 `lookup` 属性）

## Spring如何解决循环依赖的？

> 三级缓存

## mysql的事务隔离级别？分别解决什么问题？什么是可重复读？什么是不可重复读？

jetcache相关、怎么保证多级缓存的一致性、数据库和缓存的一致性、缓存怎么更新的？

二级缓存存的什么数据、数据量大吗？

谈谈提到的高可用？

> https://www.cnblogs.com/javaguide/p/12216564.html

# 虾皮一面

8.30 15:00 ~ 16:10

自我介绍

项目介绍、平台测和数字化赋能、产品异动监控平台，业务方面介绍

然后是八股文

分布式任务调度系统Dolphin Scheduler

## RabbitMQ是否支持事务

> RabbitMQ 客户端中与事务机制相关的方法有三个:
>
> `channel.txSelect` 用于将当前的信道设置成事务模式。
>
> `channel . txCommit` 用于提交事务 。
>
> `channel . txRollback` 用于事务回滚,如果在事务提交执行之前由于 RabbitMQ 异常崩溃或者其他原因抛出异常,通过txRollback来回滚。
>
> https://blog.csdn.net/Fireworkit/article/details/136554486
>
> 

## 为什么选择RabbitMQ没有用的别的RocketMQ、Kafka

> https://developer.aliyun.com/article/1453629		https://xie.infoq.cn/article/7577289323449da236cd0f127
>
> 1.RabbitMQ：
>
> 适用于易用性和灵活性要求较高的场景
>
> - 异步任务处理：RabbitMQ提供可靠的消息传递机制，适用于处理异步任务，例如将耗时的任务放入消息队列中，然后由消费者异步处理，提高系统的响应速度和可伸缩性。
> - 解耦系统组件：通过使用RabbitMQ作为消息中间件，不同的系统组件可以通过消息进行解耦，实现松耦合的架构，提高系统的可维护性和灵活性。
> - 事件驱动架构：RabbitMQ的发布-订阅模式可以用于构建事件驱动架构，将系统中的事件作为消息发布到相应的主题，不同的消费者可以订阅感兴趣的主题进行相应的处理。
>
> 2.RocketMQ：
>
> 适用于大规模数据处理和高吞吐量的场景
>
> - 分布式事务：RocketMQ支持分布式事务消息，适用于涉及多个业务系统的分布式事务场景，确保消息的一致性和可靠性，同时提供高吞吐量的消息传递能力。
> - 实时日志处理：由于RocketMQ具备高吞吐量和低延迟的特点，可以用于实时日志处理，例如日志收集和分析、日志聚合等场景。
> - 流式处理：RocketMQ支持流式处理模式，可以将产生的数据流通过消息队列传递给流处理框架（如Flink、Spark Streaming），实现实时数据处理和分析。
>
> 3.Kafka：
>
> 适用于高吞吐量的实时数据流处理和流式处理场景
>
> - 数据管道和实时数据处理：Kafka的高吞吐量和可持久化存储特性使其成为构建可靠的数据管道和实时数据处理系统的理想选择，用于流式数据的收集、传输和处理。
> - 日志和事件流处理：Kafka的分区和有序性保证特性使其成为日志和事件流处理的理想解决方案，例如应用日志收集、事件溯源、业务监控等。
> - 实时流分析和机器学习：Kafka与流处理框架（如Flink、Spark Streaming）结合使用，支持实时流分析和机器学习任务，处理实时数据流以获取实时的洞察和决策支持。

## 如何保证MQ的高可用、存在顺序性消费的问题吗

> https://www.cnblogs.com/blackdd/p/12364142.html

项目中Redis用的什么数据结构、Redis的数据结构有哪些、Hash的底层数据结构

> https://xiaolincoding.com/redis/data_struct/command.html#介绍-3
>
> Hash 类型的底层数据结构是由压缩列表或哈希表实现的：
>
>     如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用压缩列表作为 Hash 类型的底层数据结构；
>     如果哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的 底层数据结构。

MVCC、事务隔离级别、分别能解决什么问题

undo log、redo log

> https://xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-redo-log

mysql的使用版本

mysql索引结构、B+树和B树区别有什么优势、主键索引和别的索引的区别、解释回表

用的Java版本，用过什么特性

> https://javaguide.cn/java/new-features/java8-common-new-features.html

jetcache原理、怎么保证一致性、解释基于Redis Pub/Sub实现，追问怎么万一pub/sub挂了呢

> 目前项目发布订阅依赖于jetcache提供的能力，jetcache比较灵活，开放了相关的接口，用户可以进行扩展，比如基于RabbitMQ实现发布订阅并保证高可用。

QPS、Sentinel有触发过告警吗

算法题：

> 至多包含 k 个不同字符的最长子串
>
> **详细描述**
>
> 给定一个字符串，找出其中不超过 K 个不同字符的最长子字符串的长度。
>
> **输入输出描述**
>
> > 输入描述
> >
> > 1. Input：String = "araaci", K = 2
> > 2. Input: String = "araaci", K = 1
> > 3. Input: String = "cbbebi", K = 3
> >
> > 输出描述
> >
> > 1. Output：4
> >    解释：不超过 2 个不同字符的最长子字符串是“araa”。
> > 2. Output：2
> >    解释：不超过 1 个不同字符的最长子字符串是“aa”。
> > 3. Output：5
> >    解释：不超过 3 个不同字符的最长子字符串是“cbbeb”和“bbebi”
>
> https://blog.csdn.net/qq_29051413/article/details/108814174



# 闲鱼1面

 2024-09-02 19:00

自我介绍

聊项目

流量多少

sentinel流程防护参数怎么设置的

> 先根据手机银行日活，根据二八法则，百分之80的流量在百分之20的时间内打过来，估算一个值，经过压力测试，确定一个值，项目上线之后在运营业务平台观察流量情况，观察QPS，做响应的调整
>
> 其实可以回答根据手机银行那边提供接口流量情况进行设置

压力测试怎么做的？

> 项目对接测试，测试同事进行压测，自己也用jmeter玩儿过

怎么做多维度的查询？

ES是自己部署吗

用的Java几

有没有遇到fullgc

怎么监控的

redis机构树、怎么存的

mysql数据表哪个表数量最多

RabbitMQ什么情况下用到的

mysql和redis的双写一致性

> 先更新数据库，再删除缓存。写缓存比写数据库快很多，能在很大程度上保证一致性。



算法题

```java
给定 n*m的矩阵， 0为可通行，1为障碍物，上下左右4个方向连通， 求分隔的连通区的数量
input :
4 4
0010
0100
1000                 
0000
output :
2

public class Solution {
  int ans = 0;
  public int calcZoneNums(int[][] matrix) {
    int m = matrix.length;
    int n = matrix[0].length;
    boolean[][] visited = new boolean[m][n];
    for(int i = 0; i < m; i++) {
      for(int j = 0; j < n; j++) {
        if(!visited[i][j] && matrix[i][j] == 0) {
          ans++;
          core(matrix, i, j, visited);
        }
      }
    }
    return ans;
  }

    private void core(int[][] matrix, int row, int col, boolean[][] visited) {
      int m = matrix.length;
      int n = matrix[0].length;
      if(row < 0 || row >= m || col < 0 || col >= n) {
        return ;
      }
      visited[i][j] = true;
      if(matrix[i][j] == 0) {
        matrix[i][j] = 1;
        core(matrix, row - 1, col);
        core(matrix, row + 1, col);
        core(matrix, row, col - 1);
        core(matrix, row, col + 1);
      }
    }
}
```



# 阿里云1面

9.3 19:00 ~ 20:00	第一次遇到女面试官

上来就做题

在任意元素组成的数组里面，找到满足a+b+c = x*y的组合

![image-20240903201210906](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240903201210906.png)

![image-20240903200542865](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240903200542865.png)

![image-20240903200659033](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240903200659033.png)

聊项目

哪个项目投入最多、工作量最大



# 追觅2面

9.4 18::00 ~ 18: 25

自我介绍

聊项目、业务

绩效考核和为招行带来的收益有内有关系

项目从头开始参与建设的吗

遇到什么困难

数据正确性怎么保证的？怎么知道算的对不对？

有没有遇到程序崩溃

> 遇到过OOM问题，一次性加载太多数据到内存

开发室有多少人、零售应用中心下面有多少室、跨团队怎么合作的、有没有遇到什么困难

职级

为什么离职

有没有拿到其他offer

觉得自己在公司能排在什么位置

有没有朋友在苏州

反问：开发模式、业务线(iot)、技术栈

> 软件、硬件、机械；软件都在一层楼，有问题很方便沟通



# 滴滴2面

9.4 20:00 ~ 21:00

![image-20240904210112463](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240904210112463.png)

自我介绍

我打开摄像头，面试官让我关了，说上一个同学面试开视频网络比较卡让关了

聊项目、聊架构

简历中的深度赋能，有什么指标去量化、去衡量赋能的效果吗？

产品异动监控系统发送告警给客户经理之后，实习处理比例如何？效果怎么样？

QPS多少，二级缓存有必要吗，使用二级缓存提高的复杂性

> 我答最高峰值QPS五六百，
>
> 比如五星之选

熔断、降级的理解，熔断和降级都是必要的吗

缓存穿透、缓存击穿、缓存雪崩

分级保障机制，不同的场景怎么进行区分做相应分级保障的

> 

产品异动监控系统怎么做的

目前有没有拿到别的offer

是否是离职状态、为什么离职、对新的工作有什么期待

反问：开发模式



# 腾讯金融科技-研效后台开发工程师(深圳)1面

本来16:30开始，面试官有事儿推迟了20分钟，16:50 ~ 17:30

自我介绍、聊项目

主要聊产品异动监控系统

平台用户有多少人使用

告警推送频率、变生成结果变发告警吗，还是结果生成之后发告警？

怎么保证没有重复告警的情况？

## MQ怎么保证幂等性

> https://blog.csdn.net/2401_83384536/article/details/136762250
>
> https://juejin.cn/post/6999460990945329188

调用一个接口怎么知道接口调用是否成功了？

接口调用成功率下降到95%，怎么提升成功率到4个9，99.99%？

面试官介绍他们部门的情况？

除了上面谈到的，还有哪些觉得重要的点没有聊到？

是否离职、为什么离职

反问

算法题：

给定一个整数数组，找到数组中的最长连续递增子序列，并返回其长度。

```
题目要求：
给定一个整数数组，找到数组中的最长连续递增子序列，并返回其长度。
示例1：
输入：[1, 3, 5, 4, 7]
输出：3
解释：最长连续递增子序列是 [1, 3, 5]，长度为 3。
示例2：
输入：[2, 2, 2, 2, 2]
```

![image-20240905172959000](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240905172959000.png)



# 闲鱼2面

9.5 19:00 ~ 19:40

自我介绍

聊项目、技术架构、业务架构

举个例子具体详细聊聊负责的工作

金融模型是指？

> 招银研究院提供的，从风险、收益角度衡量产品表现

怎么保证数据准确性、防止资损

QPS、系统稳定性要求、怎么保障系统稳定性

有多少人在用

有几个人在负责开发

服务以什么形式给别的渠道平台调用的，使用dubbo了吗，还是http?

> 

jetcache、二级缓存有什么问题、一致性的要求

最近看什么书？

> 数据密集型系统设计，让具体谈谈哪个点影响最深，G了。。。。。。

绩效、职级、对下一份工作有什么期待

> 在脉脉上看到说，阿里对绩效比较看重，说实话吃大亏

反问：开发模式、跨团队合作、业务需求和技术改进需求



# 滴滴3面 交叉面

9.6 10:30 ~ 11:15	面试官：薛亮

自我介绍

哪里人、研究生考研还是保送、那年的

聊项目

工作三年技术上的成长最大还是业务上？

业务流程、数据量、使用人数、产品异动监控

面试官也是银行干过，带了三个月就想跑路，最后在宁波银行干了10个月，觉得呆在银行业发展受限，果断跑路

在招银什么时候想出来的？

面试官这几年面了很多招银出来的人，得有二十几个，都很优秀、学历背景很优秀，本硕985，在招行干的业务限制自身发展，浪费人才，balabala.......

招银呆的越久想跳出来越难，工作五年再调出来，很难能match外面的工作岗位，比如五年跳蚂蚁，职级P7，match不住

这次不仅仅是换一份工作，更多是行业，有没有想好换个行业：从互联网+金融转变为互联网+能源，去了滴滴，工作几年之后身上的标签就从互联网金融变成互联网能源了，滴滴目前技术和一线大厂阿里、腾讯、字节可能有些差距，比招银还是强的，想好职业发展规划，还想在金融行业更应该去阿里、蚂蚁这种。

在招银虽然发展受限，技术深度和预期有差距，但是这些都不重要，意思到问题，跳出来学习就行了，本身学历背景很优秀，学习一些都不是问题

面试官谈了很多职业发展相关，呆在银行发展、跳出来发展........

清不清楚入职要负责的业务

HC比较紧张、能否尽快入职

### 最新沟通

> 9.10 和HR沟通，目前没有hc了，
>
> 招了两个人一个d6一个d7，我是d6，在我的前面发了一个offer，9.26号入职比较晚，存在不入职的风险，
>
> 让我先面别的，有变动会通知我
>
> 两个方向，换电和能源平台稳定性，换电d6，觉得让我做稳定性相关可能存在风险，就招了一个d7

# 阿里云-售后工程师

9.6 14:00 ~ 14:47

自我介绍

聊项目

面试的岗位和主要是阿里云面向B端大客户日常使用云服务过程中遇到的问题，做的技术支持、问题排查、解决

所以面试官对中间件感兴趣

RabbitMQ、消息消费可靠性

消息队列中消息流量有多少

单体应用还是微服务？Spring Cloud 微服务相关

sentinel怎么设置的、限流怎么配置的

目前的项目有哪些需要优化的地方

项目是单体还是微服务？让你对现在的项目做微服务的拆分，怎么做？

QPS多少、系统能承受的流量、分级保障机制、怎么做高可用的保障的？

Nacos原理，临时实例、非临时实例、数据模型

临时实例、非临时实例声明周期

应用怎么部署的，虚机部署还是容器部署？

> https://developer.baidu.com/article/details/3029199
>
> https://developer.aliyun.com/article/1314149

对计算机网络、操作系统了解吗

计算机网络、OSI分为几层、应用层有哪些协议、HTTP属于哪一层、传输层有哪些协议

> https://blog.csdn.net/wwy0324/article/details/109310658

怎么排查问题的？Arthas用过吗？Linux排查问题的命令、jps、jstack、jmap

岗位工作大多数时候面向toB大客户相应的技术支持、问题排查、解决，必要的时候排查问题过程中通过编码去定位问题

# OPPO1面

本来16:00开始，面试官迟到了20min，说是时间记错了，没发消息通知他

9.6 16:20 ~ 17:20

自我介绍

聊项目

线上遇到过什么生成问题、什么情况下会出现fullgc、OOM如何排查具体是那部分代码出现了问题、什么情况下会发生OOM

> https://developer.aliyun.com/article/1443878

CPU 100%什么原因、怎么排查

> https://www.yuque.com/tulingzhouyu/db22bv/fgmmd6vgdbkxeug6

项目中redis架构模式、redis有哪几种集群模式、分片集群原理

> 哨兵模式
>
> 三种：主从、哨兵、分片集群
>
> CRC16、哈希槽、16384

Redis分片集群中某个节点CPU使用率很高什么原因、内存使用率很高什么原因、数据倾斜怎么解决

> https://www.alibabacloud.com/help/zh/redis/user-guide/deal-with-data-skew-issues
>
> 在Redis分片集群中，如果发现某个节点的CPU使用率或内存使用率很高，可能有以下几个原因：
>
> ### CPU使用率高
> 1. **查询/写入负载过大**：如果有大量的读写操作集中在该节点上，可能会导致CPU使用率上升。
> 2. **复杂的查询**：执行复杂的查询操作，如排序（SORT）、聚合（例如使用SCAN命令进行迭代）等，会占用较多的CPU资源。
> 3. **客户端连接过多**：大量客户端同时连接到同一个节点也可能增加CPU负担。
> 4. **网络延迟**：如果网络状况不佳，可能会导致节点间的数据同步延迟，进而影响CPU性能。
> 5. **配置不当**：某些配置项设置不当，比如过大的AOF缓冲区大小，可能导致额外的CPU开销。
>
> ### 内存使用率高
> 1. **数据集增长**：随着存储的数据量增加，内存使用自然会上升。
> 2. **缓存策略**：如果Redis用作缓存，而缓存的数据量超出预期，可能会导致内存使用过高。
> 3. **对象过期策略**：如果没有合理地设置键的过期时间，可能会导致不再使用的数据占据内存空间。
> 4. **内存泄漏**：虽然Redis相对稳定，但是某些情况下可能存在内存泄漏问题。
>
> ### 数据倾斜
> 数据倾斜是指在分片集群中，某些节点存储的数据量远大于其他节点，这可能导致某些节点负载过高，而其他节点资源未充分利用。解决数据倾斜的方法包括：
>
> 1. **重新分片（Re-sharding）**：通过重新分配键到不同的节点来平衡数据分布。
> 2. **调整哈希槽分配**：在Redis集群模式下，可以调整哈希槽的分配来更均匀地分布数据。
> 3. **优化数据模型**：如果可能，重新设计数据模型，使得热点数据能够更均匀地分布在各个节点上。
> 4. **使用一致性哈希**：对于非严格意义上的Redis集群，可以考虑使用一致性哈希算法来分布数据，减少单点负载。
>
> 针对上述问题，建议定期监控集群状态，分析性能指标，以及根据实际情况调整集群配置和数据分布策略。如果问题持续存在或恶化，可能需要进一步的技术支持或专业咨询。

redis内存写满了会发生什么情况

> https://cloud.tencent.com/developer/article/2334016

redis有哪些驱逐策略

垃圾回收、项目使用的什么垃圾回收器、JDK8默认垃圾回收器、默认parallel scavenge和parallel old和G1对比

讲讲G1垃圾回收的详情、G1会出现OOM问题吗

JVM永久代、元空间、本地内存

JVM垃圾回收会回收元空间、本地内存吗

> https://blog.csdn.net/Monkey_King_GL/article/details/118735465

JVM对内存设置最小2G、最大4G、实际程序运行的时候怎么分配内存

> https://developer.aliyun.com/article/897008
>
> 如果将初始堆大小设置为与最大堆大小相同，则应用程序的启动时间也会更好。以下是 Oracle 文档的摘录：
>
> **如果初始堆太小，Java 应用程序的启动速度会变慢，因为 JVM 被迫频繁地执行垃圾回收，直到堆增长到更合理的大小。为获得最佳启动性能，请将初始堆大小设置为与最大堆大小相同。**

JVM内存最大最小都设置4G，实际如何分配？

JVM分配4G内存、实际发现使用超过4G，什么原因？

> https://www.cnblogs.com/wsx2019/p/15794910.html
>
> https://www.cnblogs.com/rude3knife/p/13570423.html
>
> JVM的内存
> 先放一张JVM的内存划分图，总体上可以分为堆和非堆（粗略划分，基于java8）
>
> ![在这里插入图片描述](https://gitee.com/firefish/blogimgs/raw/master/imgs/20210219140352638.png)
>
> ![JVM内存组成](https://gitee.com/firefish/blogimgs/raw/master/imgs/20210219160555660.png)
> 

jetcache、缓存击穿、自己怎么实现分布式锁，使用redis实现细节

Java有哪些锁、sychronized和reentranlock对比

sychronized锁升级

> https://juejin.cn/post/7232524757526429756

reentranlock可中断、怎么实现的、原理是什么(interuptly)

> https://www.cnblogs.com/duizhangz/p/16267995.html

项目中使用过JUC包下哪些类、线程安全的数据结构

五星之选实现技术难点、解决方案

redis数据结构、hash底层实现、哈希冲突除了链地址法还有哪些其他算法、分片集群支持批量获取元素吗

QPS、项目部署、多大资源、多少台机器、资源利用率怎么样

> 我说峰值QPS快1000，16c64g，10台机器

项目有做异地多机房部署吗

> 集群部署，平湖东、平湖西，各5台机器，负载均衡

mysql mgr原理



# 追觅3面

一面是经验比较丰富的平级同事，2面是经理，3面不是HR，具体薪资福利等会后续和HR聊

9.9 11:00 ~ 11:30

自我介绍

当时为什么选择加入招银、为什么离职

对于新工作看重什么

对追觅的了解

最有成就感的事情

有什么优点、缺点

> 缺点：不太会拒绝，大包大揽，权责不对等，

面过招银的人，技术栈比较保守，对此有没有学习？

> 招银老旧的业务可能存在这种情况，我所在的互联网开发室不存在，技术栈和互联网企业没差别

招银职级、绩效、薪资和期望薪资，本来没问的，我问了薪酬体系，反问我，然后说具体薪酬后续和HR聊



# TP-Link一面

9.9 14:30 ~ 15:28

自我介绍、学机械为啥选择写代码、哪年的

聊项目、项目具体做了什么、收获最大是什么

工作地点选择

团队多少人、担任什么角色

技术栈

项目中反射、Spring AOP、策略模式、责任链模式的使用

Sentinel限流

mysql、数据量多少、数据存的什么

RabbitMQ使用

redis数据类型、GEO和HyperLogLog底层数据结构

> https://xiaolincoding.com/redis/data_struct/command.html

项目中使用redis存的数据结构

> String类型，追问序列化方式使用的什么？：Jackson

项目中使用过什么算法、设计模式

计算机网络分层模型、OSI七层、TCP/IP四层、每一层往数据报文中加入了什么

算法题：给你一个游泳圈、上面标记了若干个正整数，选择什么数据结构存储，并计算不相邻的数字的最大和

> 开始回答使用链表收尾相连存储，后来提出用数组模拟环，O(1)时间复杂度获取元素
>
> 使用动态规划求解，dp[i]表示前i个元素最大不相邻数字和
>
> leetcode918：
>
> https://leetcode.cn/problems/maximum-sum-circular-subarray/



# 字节1面

9.9 19:00 ~ 20:40，其中问答19:00 ~ 20:15, 做题：20:15 ~ 20:40，需要自己自定义数据结构，自己构造测试用例测试通过，浪费时间.....

自我介绍

聊项目、聊了很多、聊的特别详细

接口耗时、QPS、jetcache二级缓存框架使用前后接口耗时分别是多少

> 峰值QPS 接近1w
>
> > HTTP GET请求的耗时取决于多个因素，包括但不限于网络状况、服务器响应时间、请求的复杂性以及客户端与服务器之间的距离。下面是一些影响GET请求耗时的主要因素：
> >
> > 1. **DNS解析时间**：浏览器需要将域名解析为IP地址，这个过程可能需要一些时间。
> > 2. **建立TCP连接时间**：浏览器需要与服务器建立一个TCP连接，这个过程包括三次握手（Three-way Handshake），耗时取决于网络状况。
> > 3. **发送请求时间**：一旦连接建立，客户端就会发送HTTP GET请求报文到服务器。
> > 4. **服务器处理时间**：服务器接收到请求后，需要处理请求并准备响应。
> > 5. **传输响应时间**：服务器将响应数据发送回客户端，这个过程的耗时取决于响应数据的大小和网络带宽。
> > 6. **接收响应时间**：客户端接收完整的HTTP响应，包括任何相关的资源（如图片、CSS文件等）。
> > 7. **TLS/SSL握手时间**（如果是HTTPS连接）：如果使用的是HTTPS协议，则还需要额外的TLS/SSL握手时间。
> >
> > 具体的耗时可以通过网络工具来测量，例如使用Chrome DevTools的Network面板、Fiddler、Postman或其他类似工具来查看每个阶段的具体耗时。
> >
> > ### 举例说明
> > 假设在一个理想的网络环境下（例如，直连且没有太多网络拥堵），一次简单的HTTP GET请求的耗时可能如下：
> >
> > - DNS解析：约10ms
> > - TCP握手：约10ms（往返时间RTT）
> > - 发送请求：几乎可以忽略
> > - 服务器处理：几毫秒到几十毫秒，取决于服务器负载和请求的复杂性
> > - 响应传输：取决于响应大小，如果响应很小，可能只需几毫秒
> > - 接收响应：几毫秒到几十毫秒
> >
> > 因此，在理想条件下，一次简单的HTTP GET请求的总耗时可能是几十毫秒。然而，在实际应用中，特别是当涉及到跨国请求或服务器负载较重的情况下，这个时间可能会显著增加。为了提高用户体验，通常会对这些环节进行优化，比如使用CDN（内容分发网络）来缩短物理距离，或者使用缓存来减少服务器处理时间等。

接口调用方式是什么？

> http形式提供，没有使用rpc

### 八股文：

hashmap实现、红黑树是什么

> JDK1.7
>
> JDK1.8
>
> https://www.cnblogs.com/yinbiao/p/10732600.html

post和get区别、post请求路径、请求头中可以携带数据吗

> https://segmentfault.com/a/1190000044725532
>
> https://blog.csdn.net/linysuccess/article/details/109221288

http和https区别、加密协议、md5

> https://www.runoob.com/w3cnote/http-vs-https.html
>
> https://cloud.baidu.com/article/3074524

SpringMVC工作原理、流程

> 1. 请求到达 DispatcherServlet
>
> 当客户端发送一个请求时，它首先会到达 DispatcherServlet。DispatcherServlet 是 Spring MVC 的中央控制器，负责接收所有的请求并将其分发给适当的处理程序。
>
> 2. HandlerMapping 确定处理程序
>
> DispatcherServlet 会查询 HandlerMapping 来确定请求应该由哪个控制器来处理。HandlerMapping 将请求映射到相应的控制器类和方法。
>
> 3. 控制器处理请求
>
> 一旦确定了适当的控制器，DispatcherServlet 将请求传递给该控制器。控制器执行业务逻辑，并调用适当的服务层方法来处理请求。
>
> 4. 调用 Service 层
>
> 在控制器中，通常会调用服务层来处理业务逻辑。服务层执行任何必要的业务逻辑，并与数据库或其他外部系统进行交互。
>
> 5. 返回 ModelAndView
>
> 一旦控制器完成其逻辑处理，它会创建一个 ModelAndView 对象，其中包含要传递给视图的模型数据以及要渲染的视图的名称。
>
> 6. ViewResolver 解析视图
>
> DispatcherServlet 将 ModelAndView 对象传递给 ViewResolver，它根据视图名称解析出实际的视图对象。
>
> 7. 渲染视图
>
> 一旦找到了视图对象，DispatcherServlet 将模型数据传递给视图对象，并要求它将数据呈现为 HTML 或其他格式的响应。视图对象使用模型数据来生成响应。
>
> 8. 返回响应
>
> 最后，生成的响应将返回给客户端，完成整个请求-响应周期。
>
> 
>
> 对于前后端分离架构来说，handler执行完成，通过HttpMessageConverter转化为json返回。

Spring bean的生命周期

> https://blog.csdn.net/qq_33732195/article/details/123837864

bean的创建方式

> https://www.cnblogs.com/lgjlife/p/11060570.html

spring和springboot区别

> https://www.cnblogs.com/jiftle/p/18385244
>
> https://blog.csdn.net/weixin_42888110/article/details/134121773
>
> https://blog.csdn.net/weixin_42888110/article/details/134121773

如何自定义starter、starter相对传统引入多个jar包有什么好处、maven引入多个starter发生版本冲突怎么解决

> https://blog.csdn.net/weixin_40642302/article/details/139273591
>
> https://blog.csdn.net/Jieur/article/details/122438549
>
> https://www.cnblogs.com/hello-shf/p/10864977.html

spring条件加载（@ConditionalOnClass）

> https://blog.csdn.net/xue_xiaofei/article/details/114603236
>
> https://cloud.tencent.com/developer/article/1551516

spring类加载机制、如何打破双亲委派

>  https://www.cnblogs.com/xieshijie/p/18285640

一个Springboot项目中可以有几个spring容器

如何自定义类加载器、自定义加载器的spring容器和springboot项目的容器是一个吗、如何将自定义classloader和容器绑定



Spring事务、实现原理

> https://blog.csdn.net/zhang_qing_yun/article/details/120229381

spring的事务隔离级别、事务传播机制

Spring @Transaction注解 A方法开启事务，调用B方法，B方法中有三个数据库更新操作，B没有开启事务，B方法中执行到第二个数据库更新操作异常，第一个数据库更新操作会回滚吗

事务隔离级别request_new、默认隔离级别

SpringBoot

mysql如何实现回滚

> https://xiaolincoding.com/mysql/log/how_update.html
>
> https://blog.csdn.net/m0_49449205/article/details/114988580

MySQL事务隔离级别、分别存在和解决了什么问题

> https://javaguide.cn/database/mysql/transaction-isolation-level.html

算法：判断一颗二叉树是不是另外一棵树的子树，需要自己自定义数据结构，自己构造测试用例测试通过，浪费时间.....

还好手撕出来了，自己构造测试用例也通过了

> ```java
> public boolean isSubTree(TreeNode root, TreeNode subRoot) {
>     if(root == null && subRoot == null) {
>         return true;
>     }
>     if(root == null || subRoot == null) {
>         return false;
>     }
>     boolean flag = false;
>     if(root.val == subRoot.val) {
>         flag = isSubTree(root.left, subRoot.left) && isSubTree(root.right, subRoot.right);
>     } else {
>         flag = isSubTree(root.left, subRoot) || isSubTree(root.right, subRoot);
>     }
>     return flag;
> }
> ```

# 万得1面

9.10 10:00 ~ 10:30

自我介绍

聊项目、项目细节

产品异动监控数据量

项目中使用的索引

mysql索引数据结构

二叉树、B树、B+树

有没有用过kafka

目前薪资、期望薪资、是否离职状态、为什么去南京、有定居南京的打算吗

# 国泰君安期货1面

9.10 14:00 ~ 14:50 3个面试官

自我介绍

聊项目、架构

QPS、数据量

jetcache、数据库、缓存更新机制

> 先更新db，再删除缓存

java的锁、lock需要注意什么、sychronized和reentranlock区别、是否可重入、可冲入的原理、

threadLocal、具体方法

> https://javaguide.cn/java/concurrent/threadlocal.html#%E5%89%8D%E8%A8%80

fullgc怎么定位问题

> https://www.yuque.com/tulingzhouyu/db22bv/itakbe1872na814s

绩效

流程：1面、2面笔试、3面主管、4面总监和HR



# 国泰君安期货2面（笔试）

9.10 19:00~19:50

># **Java开发****技术性笔试题**
>
>（请把答案直接写在**选项下面的代码框**处，**且不要查手机，****使用****代码软件**
>
>**泄题或者查手机后果自负**）
>
>## **1** **思维**
>
>### (1) **公寓有三名室友，一个出生在深圳，一个出生在上海，一个出生在广州。他们三个人的专业分别是计算机、金融和工程管理。其中：**
>
>① 甲不是学计算机的，乙不是学工程管理的
>
>② 学工程管理的出生在深圳
>
>③ 学计算机的不出生在上海
>
>④ 乙不出生在广州
>
>请根据已知条件，判断甲的专业：
>
>A. 计算机
>
>B. 工程管理
>
>C. 金融
>
>D. 三种专业都可能
>
>```
>B
>```
>
>### (2) **本市某名品店被盗，有4名嫌疑犯被接受调查。经过调查可以肯定是甲、乙、丙、丁中的某一个人所为。审讯中，**
>
>① 甲说："我不是小偷。"
>
>② 乙说："丁是小偷。"
>
>③ 丙说："乙是小偷。"
>
>④ 丁说："我不是小偷。"
>
>调查证实4个人中只有1个说的是真话。那么根据条件，下列哪个判断正确：
>
>A. 丁说的是假话，丁的确是小偷 
>
>B. 丙说的是真话，乙是小偷
>
>C. 乙说的是真话，丁是小偷
>
>D. 甲说的是假话，因此，甲是小偷
>
>```
>B
>```
>
>### (3) **"刘主任又戒酒了。" 通过这句话我们不能得出以下哪个论断：**
>
>A. 刘主任过去戒过酒，次数可能不止一次
>
>B. 刘主任过去戒酒未成功，这次仍可能如此
>
>C. 刘主任酒瘾很大，讲这话的人深信刘主任的酒瘾永远戒不掉
>
>D. 讲这话的人是在暗讽刘主任的戒酒行为
>
>E. 讲这话的人确信刘主任这次戒酒一定会成功
>
>```
>E
>```
>
>### (4) **如果你有无穷的水，现在有一个7公斤的水桶和一个9公斤的水桶，如何才能准确的称出8公斤的水？**
>
>```
>9 - 7 = 2
>7L水桶 ：A
>9L水桶 ：B
>A中装2L，同时向A和B中加水，A满时，B中5L
>然后将A清空，同时向A和B加水至B满，得到A中4L，
>将B中清空，同时向A和B中加水至A满，得到B中3L，
>一次类推可以得到B中1L，同时将A加满，即可得到8L
>```
>
>### (5) **某人去玩具店买小熊，单价30元,付给玩具店老板100元,玩具店老板没零钱,去水果店换了100元零钱回来找给那人70元.那人走后,水果店老板找到玩具店老板说刚才的100元是假币，玩具店老板赔偿了水果店老板100元.问:玩具店老板损失了多少钱?**
>
>```
>100
>```
>
>## **2** **综合技术**
>
>### (1) **t表有字段a和b，都是varchar类型，下面哪句SQL语句是可以执行的**
>
>A. DELETE TABLE t;
>
>B. SELECT * FROM t WHERE 1=1 OR a='1';
>
>C. UPDATE t SET a='1' AND b='2';
>
>D. INSERT t VALUES ('1','2');
>
>```
>B
>```
>
>### (2) **SQL实现分组查询的短语是** 
>
>A.ORDER BY			B. GROUP BY 		C. HAVING		D. PARTITION BY
>
>```
>B
>```
>
>### (3) **只删除表t的数据时，采用（  ）的效率比（  ）要高。**
>
>A. delete from t				B. drop t
>
>C. truncate table t 		D. delete table t
>
>```
>C，A
>```
>
>### (4) **以下哪2个设计模式是不存在的**
>
>A. 装饰器模式		B. 总线模式		C. 抽象工厂模式		D. 对象模式
>
>​    E. 观察者模式		F. 单例模式		G. 工厂模式	
>
>```
>BD
>```
>
>### (5) **以下哪2个语言是不存在的**
>
>A. C#			B. PL/SQL		C. java			D. TypeScript
>
>E. JSP.net		F. Swift		G. QScript		H. Go
>
>```
>BG
>```
>
>### (6) **“拒绝”的英语是哪个**
>
>A. subject		B. adjust		C. object		D. access
>
>​    E. reject		F. accept		G. close		H. confuse
>
>```
>E
>```
>
>### (7) **进程和线程的对应关系是**
>
>A. 1:n			B. m:n			C. n:1			D. 1:1
>
>```
>A
>```
>
>### (8) **linux系统中，修改文件test.txt权限为所有用户可读可写可执行的命令**
>
>A. chmod  test.txt  777			B. chmod  777  test.txt
>
>​    C. chown  -R  test.txt 		    D. chown  +rwx  test.txt
>
>```
>A
>```
>
>### (9) **以下哪个linux命令不能查看到文件**
>
>A. cat			B. less			C. vi			D. grep
>
>```
>B
>```
>
>### (10) **下面哪个浏览器是****Webkit****内核**
>
>A. IE/Edge		B. Firefox		C. chrome		  D. Safari
>
>```
>B
>```
>
>## **3** **JAVA**
>
>### **(1) 关于Java编译，下面哪一个正确**
>
>​            A.     Java程序经编译后产生machine code`
>
>​            B.     Java程序经编译后会生产byte code
>
>​            C.     Java程序经编译后会产生DLL
>
>​            D.     以上都不正确
>
>```
>B
>```
>
>**(2) 下列说法正确的**
>
>​            A.     class中的construtor不可省略
>
>​            B.     construtor与class同名，但方法不能与class同名
>
>​            C.     construtor在一个对象被new时执行
>
>​            D.     一个class只能定义一个construtor
>
>```
>C
>```
>
>### **(3) Java中接口的修饰符可以为（）**
>
>​            A.     private
>
>​            B.     protected
>
>​            C.     final
>
>​            D.     abstract
>
>```
>D
>```
>
>### **(4) 下列关于关键字的使用说法错误的是（）**
>
>​            A.     abstract不能与final并列修饰同一个类
>
>​            B.     abstract类中可以有private的成员
>
>​            C.     abstract方法必须在abstract类中
>
>​            D.     static方法能处理非static的属性
>
>```
>D
>```
>
>### **(5) 下列哪些语句关于内存回收的说法是正确的（）**
>
>​            A.     程序员必须创建一个线程来释放内存
>
>​            B.     内存回收程序负责释放无用内存
>
>​            C.     内存回收程序允许程序员直接释放内存
>
>​            D.     内存回收程序可以在指定的时间释放内存对象
>
>```
>B
>```
>
>### **(6) 以下对继承的描述锚误的是 ()**
>
>​            A.     Java中的继承允许一个子类继承多个父类
>
>​            B.     父类更具有通用性，子类更具体
>
>​            C.     Java中的继承存在着传递性
>
>​            D.     当实例化子类时会递归调用父类中的构造方法
>
>```
>A
>```
>
>### **(7) 以下哪个不是Java的引用类型？（）**
>
>​            A.     强引用
>
>​            B.     弱引用
>
>​            C.     软引用
>
>​            D.     实引用
>
>```
>D
>```
>
>### **(8)  下列关于sleep()和wait()的区别，说法错误的是（）**
>
>​            A.     sleep()来自Thread；wait()来自Object
>
>​            B.     sleep()不释放锁；wait()释放锁
>
>​            C.     sleep()接受时间为参数；wait()不能接受时间为参数
>
>​            D.     sleep()时间到后，自动恢复；wait()使用notify()/notifyall()唤醒
>
>```
>C
>```
>
>### **(9)  在Java多线程中，请用下面哪种方式不会使线程进入阻塞状态（）**
>
>​            A.     sleep()
>
>​            B.      suspend()
>
>​            C.      wait()
>
>​            D.      yield()
>
>```
>D
>```
>
>### **(10)****【多选】****下列哪个方法可用于创建一个可运行的类（）**
>
>​            A.     public class X implements Runnable{public void run() {……}}
>
>​            B.     public class X extends Thread{public void run() {……}}
>
>​            C.     public class X extends Thread{public int run() {……}}
>
>​            D.     public class X implements Runnable{protected void run() {……}}
>
>```
>AB
>```
>
>## **4  简答**
>
>### (1) **“**回文串”是一个正读和反读都一样的字符串，比如“level”或者“noon”等等就是回文串。使用你熟悉的语言（允许伪代码）写出判断函数（数量不限，不考察性能），函数参数是字符串，函数返回值是布尔型**。**
>
>函数一
>
>```
>public boolean checkString(String str) {
>    if(str == null || str.length() == 0) {
>        return false;
>    }
>    int left = 0;
>    int right = str.length() - 1;
>    while(left < right) {
>        if(str.charAt(left) != str.charAt(right) {
>            return false;
>        }
>        left++;
>        right--;
>    }
>    return true;
>}
>```
>
>### (2) **某商场推出圣诞购物季，可以同时享受以下的优惠，请设计出最优惠的方案：**
>
>### **优惠券：每1000元减100元；**
>
>### **满折优惠：满2件打9折，满3件打8折；**
>
>​            1、      简述有顾客购买了3件，每件价值800元的商品，优惠金额是多少？并画出逻辑流程图；
>
>```
>(3 * 800) / 1000 = 2;
>优惠金额：2 * 100 + (3 * 800 - 2 * 100) * 0.2 = 200 + 440 = 640
>
>count: 商品数量；
>price: 商品价格；
>使用优惠券额度：count * price / 1000 * 100;
>优惠总金额：total
>if(count < 2) {
>    total = count * price / 1000 * 100;
>} else if(count == 2) {
>    temp = count * price / 1000 * 100;
>    total = temp + (count * price - temp) * 0.1;
>} else {
>    temp = count * price / 1000 * 100;
>    total = temp + (count * price - temp) * 0.2;
>}
>```

# 国泰君安3面

9. 11 17:15 ~ 17:40、四个人一起面我，部门总经理和另外三个人

自我介绍

没怎么聊项目、项目几个人参与

遇到的最大困难

自身优点和缺点

> 缺点：不太会拒绝、包揽了自身职责意外的事情，没有做到权责对等

绩效、离职原因、工作地点、是否结婚和计划结婚

工作时间、加班怎么看

离职的时候、领导怎么说的

> 因为被了一次低绩效、对后续发展有一定影响，领导表示理解

大环境不好、为什么不在职找工作

> 可以拿到赔偿、歇一段时间

> 面试完事打电话过来又沟通了一下，3面总经理，4面公司副总裁，证券公司招人这么严格，3面总经理对离职换工作的理由觉得不太充分，打电话过来又详细聊了聊，觉得我说的不是很充分，容易让人觉得是因为拿了一个低绩效想换工作，建议下次面试我组织一下话术，说得更清楚些。
>
> 和组长领导有没有矛盾，绩效组长打还是室经理打，



# 海康1面

9.12 14:00 ~ 14:30，电话面试

自我介绍

聊项目

RabbitMQ怎么保证顺序消费、prefetch、消费者多线程？

> 

MQ消息积压怎么解决

MQ使用的什么模式

> Topic主题模式

项目中使用ES遇到什么问题、数据量不是特别大为什么选择ES而不使用Mysql

ES深分页怎么解决

> https://www.cnblogs.com/crazymakercircle/p/15973775.html
>
> https://cloud.tencent.com/developer/article/2416935
>
> >  在Elasticsearch中，分页是查询操作中不可或缺的一部分。随着数据量的增长，如何高效地分页查询数据急需需要面对的问题。Elasticsearch提供了三种主要的分页方式：`from + size`、`scroll`和`search_after`。下面详细介绍这三种分页方式的特点和使用场景。
>
> ![img](https://developer.qcloudimg.com/http-save/yehe-10752309/4466e37712cd5ce44f0ef1a785a81245.jpg)

redis架构

> 哨兵

redis和db怎么更新的、双写一致性

sentinel阈值怎么设置的、有真正触发过吗

sentinel限流触发，怎么区分是应该扩容了还是应该调整限流参数？

> 在使用Sentinel进行流量控制时，当触发限流时，需要判断是应该通过增加资源（如服务器实例）来扩容，还是应该调整限流参数。这通常取决于当前系统的负载情况、资源利用率以及业务需求。以下是一些帮助决策的因素：
>
> ### 1. 监控资源利用率
> - **CPU利用率**：如果CPU利用率接近或达到100%，那么很可能是因为计算能力不足。在这种情况下，增加更多的计算资源（如增加服务器实例）可能是必要的。
> - **内存利用率**：如果内存使用率很高，甚至开始使用交换空间（Swap），那么同样需要考虑增加内存资源或优化内存使用。
> - **磁盘I/O**：如果磁盘I/O成为瓶颈，那么增加存储资源或优化数据访问逻辑可能是必要的。
> - **网络带宽**：如果网络带宽成为瓶颈，考虑升级网络设备或优化网络通信。
>
> ### 2. 分析请求模式
> - **请求量**：如果请求量突然激增，超过了正常水平，那么可能是突发流量。这种情况下，可以暂时增加资源应对高峰，同时考虑调整限流参数来平滑流量。
> - **请求类型**：分析请求的类型，如果是大量短时高并发的请求，可以通过调整限流参数来限制流量；如果是长期稳定的高负载，可能需要增加资源。
>
> ### 3. 业务需求
> - **用户体验**：如果限流导致用户体验下降，例如响应时间变长或出现错误提示，那么可能需要调整限流参数，使更多请求能够通过，或者增加资源来提高处理能力。
> - **成本考量**：增加资源意味着更高的成本。如果业务规模不大，且限流参数调整可以满足需求，那么优先考虑调整限流参数。
>
> ### 4. 预警和告警
> - **预警机制**：设置合理的预警机制，当资源利用率接近阈值时提前通知运维人员，以便及时做出反应。
> - **告警**：当系统达到极限时，触发告警，告知需要采取行动，无论是调整限流参数还是增加资源。
>
> ### 5. 测试和模拟
> - **压力测试**：在生产环境外进行压力测试，模拟不同的负载情况，观察系统表现，以此作为决策依据。
> - **模拟扩容**：在不影响现有服务的情况下，模拟增加资源后的效果，评估是否能够改善当前状况。
>
> ### 6. 动态调整策略
> - **动态限流**：Sentinel支持动态调整限流规则，可以根据实时监控数据自动调整限流参数。
> - **弹性伸缩**：如果部署在云平台上，可以考虑使用云服务商提供的弹性伸缩功能，自动根据负载情况调整资源。
>
> ### 实践建议
> 1. **监控**：持续监控系统的各项指标，确保能够及时发现问题。
> 2. **分析**：深入分析触发限流的原因，找出根本问题所在。
> 3. **实验**：尝试小范围的调整限流参数或增加少量资源，观察效果后再做大规模调整。
> 4. **文档记录**：记录每次调整的背景、原因和结果，以便未来参考。
>
> 综上所述，区分何时扩容何时调整限流参数需要综合考虑多方面的因素，并且需要根据具体情况灵活应对。

在mysql主库发生异常时自动完成主备切换和故障转移，怎么实现的

> 基于MySQL的MGR架构

项目是单体的，还是分布式的

基于AOP自定义注解拦截非法请求，除了这种方式还可以怎么做，有什么区别

> 还可以使用Filter，
>
> 除了使用面向切面编程（AOP）和自定义注解来拦截非法请求之外，还有其他几种方法可以用来实现类似的功能。以下是几种常见的方法：
>
> ### 1. 过滤器（Filter）
> Java Web 应用中，可以使用Servlet过滤器（Filter）来拦截请求。过滤器可以在请求到达目标资源之前检查请求的有效性和合法性，如果检测到非法请求，过滤器可以直接返回错误信息而不让请求继续向下传递。
>
> #### 示例代码：
> ```java
> @WebFilter("/*")
> public class SecurityFilter implements Filter {
> 
>     @Override
>     public void init(FilterConfig filterConfig) throws ServletException {
>         // 初始化逻辑
>     }
> 
>     @Override
>     public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {
>         HttpServletRequest req = (HttpServletRequest) request;
>         HttpServletResponse res = (HttpServletResponse) response;
> 
>         // 检查请求是否合法
>         if (!isValidRequest(req)) {
>             res.sendError(HttpServletResponse.SC_FORBIDDEN);
>             return;
>         }
>         
>         chain.doFilter(request, response); // 继续请求流程
>     }
> 
>     @Override
>     public void destroy() {
>         // 清理资源
>     }
> 
>     private boolean isValidRequest(HttpServletRequest request) {
>         // 实现你的验证逻辑
>         return true; // 返回true表示请求有效，false表示无效
>     }
> }
> ```
>
> ### 2. 异常处理
> 通过全局异常处理器（ExceptionResolver）来捕获异常，并对异常进行统一处理。这种方式适用于在业务逻辑中抛出异常的情况，而不是通过拦截器来检查。
>
> #### 示例代码：
> ```java
> @ControllerAdvice
> public class GlobalExceptionHandler {
> 
>     @ExceptionHandler(IllegalArgumentException.class)
>     public ResponseEntity<Object> handleIllegalArgument(IllegalArgumentException ex, WebRequest request) {
>         // 处理非法参数异常
>         return new ResponseEntity<>("非法请求", HttpStatus.BAD_REQUEST);
>     }
> }
> ```
>
> ### 3. 使用Spring Security
> Spring Security 是一个强大的安全框架，可以用来保护Web应用免受各种攻击。你可以配置Spring Security来拦截请求，并进行认证和授权。
>
> #### 示例配置：
> ```java
> @Configuration
> @EnableWebSecurity
> public class SecurityConfig extends WebSecurityConfigurerAdapter {
> 
>     @Override
>     protected void configure(HttpSecurity http) throws Exception {
>         http
>             .authorizeRequests()
>                 .antMatchers("/admin/**").hasRole("ADMIN") // 只允许管理员访问
>                 .anyRequest().permitAll(); // 其他请求都允许访问
>     }
> }
> ```
>
> ### 4. 自定义拦截器（Interceptor）
> 类似于AOP的方式，但更加面向Web应用。可以定义一个拦截器来检查请求。
>
> #### 示例代码：
> ```java
> @Component
> public class CustomInterceptor implements HandlerInterceptor {
> 
>     @Override
>     public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
>         // 在请求处理前进行拦截
>         if (!isValidRequest(request)) {
>             response.sendError(HttpServletResponse.SC_FORBIDDEN);
>             return false;
>         }
>         return true;
>     }
> 
>     private boolean isValidRequest(HttpServletRequest request) {
>         // 实现你的验证逻辑
>         return true;
>     }
> }
> ```
>
> ### 5. 通过配置中间件
> 如果你的应用是在微服务架构下运行，还可以考虑在网关层（如Spring Cloud Gateway或Zuul）添加路由规则来拦截非法请求。
>
> 每种方法都有其适用场景，选择哪种方式取决于你的具体需求和技术栈。例如，如果你的应用是基于Spring Boot的，那么使用Spring Security可能是最合适的；而对于传统的Servlet应用，过滤器可能是更好的选择。

使用AOP自定义注解拦截和使用Filter拦截的区别

> 使用过滤器（Filter）和使用面向切面编程（AOP）与自定义注解来拦截请求，两者之间有几个关键的区别：
>
> ### 1. 作用范围
> - **过滤器（Filter）**：过滤器是Servlet容器提供的标准接口，它可以在请求到达目标资源（如Servlet、JSP页面等）之前和之后执行拦截操作。过滤器可以应用于所有的HTTP请求，无论它们是否由Spring框架处理。
> - **AOP与自定义注解**：AOP是一种编程范式，它允许开发者在方法调用前后插入横切关注点（cross-cutting concerns），如日志记录、事务管理等。在Spring框架中，AOP可以与自定义注解结合使用，来标记需要拦截的方法或类。这种方式主要用于拦截由Spring管理的Bean的方法调用。
>
> ### 2. 配置方式
> - **过滤器（Filter）**：过滤器可以通过`web.xml`文件或通过注解（如`@WebFilter`）来进行配置。这种方式更加通用，不需要依赖于特定的框架。
> - **AOP与自定义注解**：AOP配置通常是在Spring上下文中定义的，通过XML配置文件或`@Aspect`、`@Pointcut`、`@Before`等注解来指定切面和切入点。这种方式更依赖于Spring框架。
>
> ### 3. 拦截时机
> - **过滤器（Filter）**：过滤器的拦截发生在请求进入容器后，但在实际目标资源处理之前。这意味着它可以检查请求头、请求体等，并决定是否继续请求。
> - **AOP与自定义注解**：AOP的拦截发生在方法调用之前或之后。它主要用于拦截方法执行，而不是整个HTTP请求。
>
> ### 4. 灵活性与控制粒度
> - **过滤器（Filter）**：过滤器提供了较为粗粒度的控制能力，通常用于全局性的请求处理，如安全认证、日志记录等。
> - **AOP与自定义注解**：AOP提供了一种更为细粒度的控制手段，可以精确到特定的方法或类。通过自定义注解，开发者可以非常灵活地控制哪些方法需要被拦截。
>
> ### 5. 应用场景
> - **过滤器（Filter）**：适合用于处理所有HTTP请求的前置和后置处理任务，如登录验证、权限校验、编码转换等。
> - **AOP与自定义注解**：更适合用于业务逻辑层面的增强，如日志记录、性能监控、事务管理等。
>
> ### 6. 性能考量
> - **过滤器（Filter）**：因为过滤器是基于Servlet容器的标准，所以在性能方面通常是可接受的。
> - **AOP与自定义注解**：虽然现代的AOP实现（如Spring AOP）已经非常高效，但在高并发环境下，AOP可能会带来一定的性能开销。
>
> 总的来说，选择使用过滤器还是AOP与自定义注解，取决于你的具体需求。如果你需要对所有HTTP请求进行预处理或后处理，那么过滤器是一个合适的选择；如果你需要对特定的方法或类进行增强，则AOP与自定义注解更为合适。在实际开发中，这两种技术也常常被结合使用，以达到最佳的效果。

离职还是在职、为什么选择离职、对新工作有什么期待



# 字节2面

9.12 19:00 ~ 20:30

自我介绍

目前有没有拿到别的offer

为什么选择离开招行、为什么没有在职找工作

聊项目、聊架构、项目实现细节

产品异动监控系统实现、监控方案有多少、有没有什么性能瓶颈

> 方案几十万个，T+1批处理，目前没什么瓶颈

java八股文

反射、具体有什么方法、怎么使用、提到了hutool，追问hutool里面具体用了什么反射相关的方法

> https://javabetter.cn/common-tool/hutool.html
>
> https://www.hutool.cn/docs/#/core/%E5%B7%A5%E5%85%B7%E7%B1%BB/%E5%8F%8D%E5%B0%84%E5%B7%A5%E5%85%B7-ReflectUtil

引发OOM的有哪些情况，结合例子具体聊聊

> https://developer.aliyun.com/article/1443878#:~:text=%E5%9C%A8%E7%BA%BF%E5%88%86%E6%9E%90Java%20O

stackoverflow什么情况下会发生、结合具体例子说明

> https://developer.baidu.com/article/details/2830076#:~:text=java.lang.

类加载机制、双亲委派、为什么要有双亲委派（https://xie.infoq.cn/article/2c5bd1469ea9473b44a1de7e7）

> https://www.cnblogs.com/crazymakercircle/p/15554725.html
>
> ## 双亲委派的作用
>
> 　　①**防止加载同一个.class。**通过委托去询问上级是否已经加载过该.class，如果加载过了，则不需要重新加载。保证了数据安全。
>
> 　　②**保证核心.class不被篡改。**通过委托的方式，保证核心.class不被篡改，即使被篡改也不会被加载，即使被加载也不会是同一个class对象，因为不同的加载器加载同一个.class也不是同一个Class对象。这样则保证了Class的执行安全。
>
> 
>
> https://www.cnblogs.com/shoshana-kong/p/17005974.html
>
> 双亲委派保证类加载器，自下而上的委派，又自上而下的加载，保证每一个类在各个类加载器中都是同一个类。
>
> 一个非常明显的目的就是保证java官方的类库<JAVA_HOME>\lib和扩展类库<JAVA_HOME>\lib\ext的加载安全性，不会被开发者覆盖。
>
> 例如类java.lang.Object，它存放在rt.jar之中，无论哪个类加载器要加载这个类，最终都是委派给启动类加载器加载，因此Object类在程序的各种类加载器环境中都是同一个类。
>
> 如果开发者自己开发开源框架，也可以自定义类加载器，利用双亲委派模型，保护自己框架需要加载的类不被应用程序覆盖。
>
> https://xie.infoq.cn/article/2c5bd1469ea9473b44a1de7e7

有a和b两个jar包，a和b中相同类路径下具有相同的方法，如何在main方法种调用这两个方法

> https://www.cnblogs.com/guzb/p/18379639/load-contains-same-name-classes-jars-by-classloader#:~:text=%E8%A6%81%E5%9C%A8%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E4%B8%AD%E4%BB%A5%E6%99%AE%E9%80%9A
>
> https://wenku.csdn.net/answer/6hbkki7bpn#:~:text=%E5%A6%82%E6%9E%9C%E4%B8%A4%E4%B8%AAjar%E5%8C%85%E9%87%8C%E9%9D%A2
>
> >
> >
> >在Java中，如果两个JAR包中有相同的类路径下的相同类，并且这些类的方法需要同时加载并执行，这通常是一个挑战，因为Java的类加载机制默认不会允许两个同名的类同时存在于类路径中。当两个JAR包包含相同的类名时，类加载器只会加载其中一个版本的类。
> >
> >为了达到你的目的，一种可能的方式是利用Java的类加载器的特性来分别加载两个不同的类版本。具体来说，你可以使用自定义的类加载器来加载不同的类。这种方式相对复杂，因为它涉及到类加载器的实现细节，但这是可行的。
> >
> >下面是一个简化的示例，展示如何通过自定义类加载器来加载不同JAR包中的相同类，并调用它们的方法：
> >
> >1. 首先，确保你的JAR包已经被放置在一个可以被访问的位置。
> >2. 使用自定义类加载器来加载每个JAR包中的类。
> >3. 调用每个类的方法。
> >
> >这里是一个简化版的示例代码：
> >
> >```java
> >import java.io.InputStream;
> >import java.lang.reflect.Method;
> >
> >public class CustomClassLoader extends ClassLoader {
> >
> >    private String jarPath;
> >
> >    public CustomClassLoader(String jarPath) {
> >        this.jarPath = jarPath;
> >    }
> >
> >    @Override
> >    public Class<?> findClass(String name) throws ClassNotFoundException {
> >        try (InputStream is = getClass().getResourceAsStream(jarPath + "/" + name.replace('.', '/') + ".class")) {
> >            if (is == null) {
> >                throw new ClassNotFoundException("Class not found: " + name);
> >            }
> >            byte[] b = is.readAllBytes();
> >            return defineClass(name, b, 0, b.length);
> >        } catch (Exception e) {
> >            throw new ClassNotFoundException("Failed to load class: " + name, e);
> >        }
> >    }
> >    
> >    public void callMethod(String className, String methodName) throws Exception {
> >        Class<?> clazz = findClass(className);
> >        Object instance = clazz.getDeclaredConstructor().newInstance();
> >        Method method = clazz.getMethod(methodName);
> >        method.invoke(instance);
> >    }
> >}
> >
> >public class Main {
> >
> >    public static void main(String[] args) {
> >        String pathToJarA = "/path/to/a.jar";
> >        String pathToJarB = "/path/to/b.jar";
> >
> >        CustomClassLoader loaderA = new CustomClassLoader(pathToJarA);
> >        CustomClassLoader loaderB = new CustomClassLoader(pathToJarB);
> >
> >        try {
> >            loaderA.callMethod("com.example.MyClass", "doSomething");
> >            loaderB.callMethod("com.example.MyClass", "doSomething");
> >        } catch (Exception e) {
> >            e.printStackTrace();
> >        }
> >    }
> >}
> >```
> >
> >请注意：
> >- 这个示例假设你已经知道类名和方法名，并且类有一个无参构造函数。
> >- 你需要确保路径`/path/to/a.jar` 和 `/path/to/b.jar` 是正确的，并且这两个JAR文件包含你想要调用其方法的类。
> >- `CustomClassLoader` 中的 `findClass` 方法仅用于演示目的，并未实现完整的类加载逻辑，实际应用中可能需要处理更多细节。
> >- 上面的示例代码未处理类文件的读取和转换为字节数组的过程，实际应用中需要使用合适的流处理技术来读取类文件。
> >
> >这种方法虽然可以让你同时加载和执行两个JAR包中的相同方法，但它要求你对类加载机制有深入的理解，并且可能需要根据实际情况进行更多的调整。如果你的应用场景允许的话，更好的做法是重构代码以避免类名冲突。

类加载相关的具体方法

> https://javabetter.cn/jvm/class-load.html#%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8

实现一个方法，入参是List<Integer> list1，返回值一个新的list2，list2和list1内容完全一样，但是list2不能添加元素，添加元素抛出异常

（immutable是如何实现的）

> 

java immutable list是如何实现的

> https://blog.csdn.net/swadian2008/article/details/130529918
>
> https://javabetter.cn/basic-extra-meal/immutable.html
>
> https://www.cnblogs.com/gaoqiao/p/10972169.html
>
> ![image-20240915012303179](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240915012303179.png)

岗位目前杭州没有HC了，考虑北京吗、买房了吗、绩效情况

> 绩效正常，背调提供薪资流水，去年的年终奖正常，前年比去年多是因为拿了2022年度优秀项目创新奖

反问：



算法题：

接雨水：手撕出来了，测试案例也跑过了

```java
public int calcWaterSum(int[] nums) {
    int len = nums.length;
    int[] leftMaxHeight = new int[len];
    int[] rightMaxHeight = new int[len];
    int curLeftMax = 0;
    for(int i = 0; i < len; i++) {
        curLeftMax = Math.max(curLeftMax, nums[i]);
        leftMaxHeight[i] = curLeftMax;
    }
    int curRightMax = 0;
    for(int i = len - 1; i >= 0; i--) {
        curRightMax = Math.max(curRightMax, nums[i]);
        rightMaxHeight[i] = curRightMax;
    }
    int ans = 0;
    for(int i = 0; i < len; i++) {
        int temp = Math.min(leftMaxHeight[i], rightMaxHeight[i]);
        if(temp > nums[i]) {
            ans += temp - nums[i];
        }
    }
    return ans;
}
```

# 菜鸟1面

9.13 14:55 ~ 16:00，面试中遇到点小插曲，手撕代码环节，发过来的链接点进去看不到题目，平台bug，最后题目截图发到钉钉，我写完截图钉钉发给面试官。还有个小插曲，vpn挂了，没网了，搞了一会儿才好使。

自我介绍

聊项目、数据量、ES

项目是微服务架构吗

代码有修改，是热部署，还是重新走上线流程部署？

> https://segmentfault.com/a/1190000042091660#:~:text=%E7%83%AD%E9%83%A8%E7%BD%B2%E5%92%8C%E7%83%AD%E5%8A%A0%E8%BD%BD%E6%98%AF%E5%9C%A8%E5%BA%94
>
> https://xie.infoq.cn/article/a248d96a0fbbec6710146a64c#:~:text=%E7%83%AD%E9%83%A8%E7%BD%B2%E5%8E%9F%E7%90%86%E4%B8%8A%EF%BC%8C%E5%85%B6%E5%AE%9E%E6%98%AF

对产品异动监控比较感兴趣，具体是怎么实现的

sentinel什么场景下使用的、怎么设置的

sentinel怎么做限流的、用的什么算法

> https://developer.aliyun.com/article/1392703

限流算法有哪几种、分别适用于什么场景

> 限流算法是用来控制服务在一定时间内接收的请求数量，以防止过载、保证系统的稳定性和可用性。常见的限流算法有以下几种，每种算法都有其适用场景：
>
> ### 1. 固定窗口限流算法（Fixed Window）
>
> - **原理**：在固定的时间窗口内，只允许特定数量的请求通过。
> - **优点**：实现简单。
> - **缺点**：在时间窗口切换时可能会出现请求突发（即在时间窗口的边界处请求会被大量拒绝）。
> - **适用场景**：适用于对精确度要求不高，且请求量相对平稳的场景。
>
> ### 2. 滑动窗口限流算法（Sliding Window）
>
> - **原理**：将固定窗口分割成更小的时间片，每个时间片记录请求的数量，通过这些时间片的累加来判断是否达到限流阈值。
> - **优点**：可以平滑限流效果，减少请求突发现象。
> - **缺点**：实现相对复杂。
> - **适用场景**：适用于请求量波动较大，需要更精细控制的场景。
>
> ### 3. 计数器限流算法（Counter Limit）
>
> - **原理**：类似于固定窗口限流算法，但在某些实现中可能会使用环形数组或其他数据结构来优化。
> - **优点**：实现简单，易于理解。
> - **缺点**：与固定窗口类似，会有突发情况。
> - **适用场景**：适用于简单的限流需求。
>
> ### 4. 漏桶算法（Leaky Bucket）
>
> - **原理**：请求进入一个被称为“桶”的队列中，桶以恒定的速度“滴水”（处理请求）。如果桶满了，新的请求就会被丢弃。
> - **优点**：能够平滑请求的处理速度，确保系统不会过载。
> - **缺点**：在高并发的情况下可能会丢失很多请求。
> - **适用场景**：适用于需要平滑流量，保护后端服务的场景。
>
> ### 5. 令牌桶算法（Token Bucket）
>
> - **原理**：系统以一定的速率向桶中添加令牌，桶有一个固定的容量。每次请求需要从桶中取出一个令牌，如果没有足够的令牌，则拒绝该请求。
> - **优点**：可以在一定程度上容忍突发流量，提供了一定的突发能力。
> - **缺点**：实现相对复杂。
> - **适用场景**：适用于需要支持突发流量，并且希望限制流量峰值的场景。
>
> ### 6. 基于数据库或缓存系统的限流
>
> - **原理**：利用数据库或缓存系统（如Redis）来存储和管理请求计数信息，实现分布式环境下的限流。
> - **优点**：适合分布式系统，可以做到全局限流。
> - **缺点**：引入了外部依赖，增加了系统的复杂性。
> - **适用场景**：适用于需要在分布式环境中实现统一限流策略的场景。
>
> 在实际应用中，可以根据系统的特性和需求选择合适的限流算法。通常，为了更好地适应各种情况，也会将多种算法组合使用，比如结合固定窗口和滑动窗口算法来实现更为灵活的限流方案。
>
> https://segmentfault.com/a/1190000044152792#:~:text=%E5%B8%B8%E7%94%A8%E7%9A%84%E9%99%90%E6%B5%81%E6%96%B9%E5%BC%8F%E5%92%8C%E5%9C%BA%E6%99%AF
>
> Guava RateLimiter 令牌桶限流实现：https://segmentfault.com/a/1190000044488582#:~:text=Guava%E7%9A%84Rate
>
> https://developer.aliyun.com/article/1569949?spm=a2c6h.12873639.article-detail.30.58862ad3TR1WrK&scm=20140722.ID_community@@article@@1569949._.ID_community@@article@@1569949-OR_rec-V_1-RL_community@@article@@1565054

redis分布式锁怎么实现的

> setnx
>
> redission watchdog 锁续约
>
> https://blog.csdn.net/fuzhongmin05/article/details/119251590



手撕代码：

> ![eedea33855179c9c5439fe651e8f826f](C:\Users\jingdyangx\AppData\Roaming\DingTalk\2004202947_v2\resource_cache\ee\eedea33855179c9c5439fe651e8f826f.png)
>
> 暴力解法：
>
> **![1726213696136_9B163367-F53A-48b8-92CE-D4DD17EE55B0](C:\Users\jingdyangx\AppData\Roaming\DingTalk\2004202947_v2\ImageFiles\1726213696136_9B163367-F53A-48b8-92CE-D4DD17EE55B0.png)**
>
> 双指针优化：
>
> ![ad2ad2afad0862c7ab4767bb41aed945](C:\Users\jingdyangx\AppData\Roaming\DingTalk\2004202947_v2\resource_cache\ad\ad2ad2afad0862c7ab4767bb41aed945.png)
>
> ![image-20240913161910208](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240913161910208.png)
>
> ![image-20240913161929702](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240913161929702.png)
>
> 居然是力扣上的原题：[16. 最接近的三数之和](https://leetcode.cn/problems/3sum-closest/)
>
> https://leetcode.cn/problems/3sum-closest/description/
>
> 写对了，在力扣上提交，全部case都能通过

是否离职状态、有没有拿到别的offer、什么offer、要求什么时候入职、表示尽快安排后续流程

> 离职，目前有拿到offer，不在杭州，优先级还是菜鸟优先

反问：

业务线和技术栈：入职的话负责压测平台建设，部门负责工程效能平台建设、从需求到测试、上线部署交付全流程，压力测试平台。

技术栈java那一套，有些中间件使用阿里内部的。

# 携程1面

9.13 17:00 ~ 18:00

自我介绍

聊项目、就数据侧聊的特别详细

数据任务有没有多线程的处理

数据量大分批次处理，怎么知道当前批次已经执行完成？

项目中线程池各个参数怎么设置的、核心线程数、最大线程数、阻塞队列是什么

> 数据处理计算密集型
>
> 核心线程数 + 1
>
> https://developer.aliyun.com/article/1329414

为什么选择使用线程池、没有使用ForkJoin

操作系统层面有哪些IO模型、有什么区别、select、poll、epoll的水平触发和边缘触发

> https://xiaolincoding.com/os/8_network_system/selete_poll_epoll.html#%E6%9C%80%E5%9F%BA%E6%9C%AC%E7%9A%84-socket-%E6%A8%A1%E5%9E%8B
>
> https://pdai.tech/md/interview/x-interview.html#_4-2-5%E7%A7%8Dio%E6%A8%A1%E5%9E%8B

同步、异步、阻塞、非阻塞的区别

> https://developer.aliyun.com/article/1432287#:~:text=1%E3%80%81%E6%A6%82%E5%BF%B5%E8%A7%A3%E9%87%8A.%20%E5%90%8C%E6%AD%A5
>
> https://www.yuque.com/tulingzhouyu/db22bv/xawecw91rvkqgiil#sSVSo
>
> 同步（Synchronous）、异步（Asynchronous）、阻塞（Blocking）和非阻塞（Non-blocking）是描述程序设计中不同执行模式的概念。下面详细解释它们的区别：
>
> ### 同步（Synchronous）
>
> - **定义**：同步操作意味着调用者必须等待操作完成，并且在操作完成之前不能继续执行其他的任务。
> - **示例**：当你在程序中执行一个函数调用，然后等待该函数返回结果之后才能继续执行后面的代码。
> - **特点**：简单直观，容易理解和实现。
> - **缺点**：如果同步操作花费的时间很长，那么调用者将会被阻塞，导致效率低下。
>
> ### 异步（Asynchronous）
>
> - **定义**：异步操作意味着调用者发出操作请求后可以立即返回去做其他事情，当操作完成时，操作系统会通过某种方式（如回调函数、事件、信号等）通知调用者。
> - **示例**：发起一个网络请求后，你的程序不会等待响应，而是继续执行其他任务，当响应到达时，通过回调函数来处理结果。
> - **特点**：提高了程序的响应性和吞吐量。
> - **缺点**：编程模型较为复杂，错误处理和逻辑控制更加困难。
>
> ### 阻塞（Blocking）
>
> - **定义**：在执行某个操作时，当前线程会被挂起直到操作完成。在这个期间，线程无法做任何其他的事情。
> - **示例**：当你在程序中执行一个文件读取操作，并且等待读取完成之后才能继续执行后续代码。
> - **特点**：简单直接，但是可能导致资源浪费（如长时间阻塞会导致线程闲置）。
> - **适用场景**：适合于单线程环境或操作非常快的情况。
>
> ### 非阻塞（Non-blocking）
>
> - **定义**：非阻塞操作允许调用者在发出请求后立即返回，如果操作还没有完成，它会立即返回而不是等待。
> - **示例**：尝试连接到一个远程服务器，如果连接尚未建立成功，就立即返回而不是等待连接建立。
> - **特点**：可以提高程序的效率，因为线程不会被长时间占用。
> - **缺点**：需要频繁检查操作状态，可能增加CPU负担。
>
> ### 关系
>
> - **同步 vs 异步**：主要区别在于是否需要等待操作的结果。同步操作需要等待，而异步操作则不需要等待，而是通过其他方式得到结果的通知。
> - **阻塞 vs 非阻塞**：主要区别在于操作过程中是否会让当前线程停滞。阻塞操作会让线程停滞，而非阻塞操作则不会让线程停滞。

Java的IO模型有哪些、java NIO的实现、和操作系统层面的IO有什么关系

> https://javaguide.cn/java/io/io-model.html

NIO的零拷贝

> https://xiaolincoding.com/os/8_network_system/zero_copy.html
>
> https://javaguide.cn/java/io/io-model.html
>
> https://javaguide.cn/java/io/nio-basis.html
>
> https://pdai.tech/md/interview/x-interview.html#_4-3-%E9%9B%B6%E6%8B%B7%E8%B4%9D

Java线程安全集合有哪些，看过哪些源码

> https://javaguide.cn/java/collection/concurrent-hash-map-source-code.html

CopyOnWriteArrayList

> https://javaguide.cn/java/collection/copyonwritearraylist-source-code.html

Reentrancelock原理、底层实现、只有一个队列吗？可冲入怎么实现的

> https://cloud.tencent.com/developer/article/2400380
>
> https://blog.csdn.net/qq_45725126/article/details/119718711
>
> https://javaguide.cn/java/concurrent/aqs.html

异步了解吗、项目中使用过异步吗、CompletableFuture

> https://segmentfault.com/a/1190000044543793

异步和在主线程中新开一个线程处理有什么区别

> https://blog.csdn.net/wo541075754/article/details/113815077
>
> 在 Java 中，“异步”和“在主线程中新开一个线程处理”都是处理并发任务的两种方式，它们之间的区别主要体现在以下几个方面：
>
> ### 异步（Asynchronous）
>
> 异步处理通常是指程序中的某个操作不会阻塞当前线程的执行。当一个异步操作启动后，调用者可以继续执行后续代码，而不需要等待这个操作完成。异步操作通常会在完成后通过回调函数、Future、CompletableFuture 或其他机制来通知调用者其已完成。
>
> #### 优点：
> - **非阻塞**：不会阻塞主线程，这对于提高应用程序的响应性和性能特别重要。
> - **资源利用率高**：特别是在 I/O 密集型的操作中，如网络请求或文件读写等，异步可以让程序在等待 I/O 操作完成的同时去做其他事情。
> - **可维护性好**：通过事件循环和回调机制，可以更容易地管理多个并发任务。
>
> #### 缺点：
> - **复杂性**：编写和理解异步代码可能比同步代码更难，因为需要处理回调地狱（Callback Hell）或者使用 CompletableFuture 等技术来简化。
> - **调试困难**：错误处理在异步代码中通常更加复杂。
>
> ### 在主线程中新开一个线程处理
>
> 这种方法通常指的是多线程或多进程编程，在这种情况下，程序创建一个新的线程来并行执行一些任务。新线程将独立于主线程运行，并且可以同时执行不同的任务。
>
> #### 优点：
> - **并发执行**：可以实现真正的并行执行，对于 CPU 密集型任务特别有用。
> - **隔离性**：不同线程间的数据是隔离的，这有助于避免数据竞争。
>
> #### 缺点：
> - **资源开销**：创建和销毁线程有一定的开销，过多的线程可能会消耗大量的系统资源。
> - **同步问题**：需要小心处理线程之间的同步问题，以防止数据竞争和死锁。
> - **上下文切换**：线程间的上下文切换会带来额外的开销。
>
> ### 实现方式
>
> #### 异步
> 在 Java 中，异步可以通过以下几种方式实现：
> - **CompletableFuture**：提供了异步编程的支持，可以链式调用，并且支持异常处理。
> - **Future 和 Executor Service**：通过 Future 可以获取异步操作的结果，而 Executor Service 提供了线程池管理。
> - **Reactor 模型**：使用事件驱动的方式来处理大量并发连接，例如 Netty 框架。
>
> #### 多线程
> 在 Java 中，多线程可以通过以下几种方式实现：
> - **Thread 类**：直接继承 Thread 类并重写 run 方法。
> - **Runnable 接口**：实现 Runnable 接口，然后通过 Thread 对象来执行。
> - **Callable 和 Future**：使用 Callable 接口来执行任务，并通过 Future 获取结果。
> - **Executor Service**：使用线程池来管理线程，减少线程创建和销毁的开销。
>
> ### 适用场景
>
> - **异步**：适用于 I/O 密集型任务，如网络请求、文件读写等，这些任务的特点是等待时间较长，但 CPU 使用率较低。
> - **多线程**：适用于 CPU 密集型任务，如复杂的计算任务，这些任务的特点是需要大量的 CPU 资源。
>
> 总之，选择异步还是多线程取决于具体的应用场景、性能需求以及系统的复杂度。在设计并发处理时，应根据任务特性合理选择合适的并发模式。

使用的JDK版本、知道现在到什么版本了吗、知道哪些新特性

> https://javaguide.cn/java/new-features/java8-common-new-features.html

字节码增强技术、实现原理

JDK动态代理和Cglib动态代理、为什么jdk动态代理需要代理对象实现接口

> https://blog.csdn.net/zwwhnly/article/details/130192925
>
> https://blog.csdn.net/weixin_45967584/article/details/136848759
>
> ![img](https://i-blog.csdnimg.cn/blog_migrate/e25caf740868bfd779bac6f00cf469bf.png)

算法题：选择最便宜的航班

```java
class Flight {
	String from;	// 出发点
	String to;	  // 目的地
	int price;		// 费用
    String name;	// 航班名称
}

// set航班集合，from：出发点，to：目的地
// 计算从from到to的可行方案中费用最少的并返回航班集合，如果无法从from到to，则返回空列表
List<Flight> calc(Set<Flight> set, String from, String to) {
    
}
```

类似题目：力扣787：https://leetcode.cn/problems/cheapest-flights-within-k-stops/description/



# 菜鸟2面

9.14 16:00 ~ 16:45

1面同事、2面领导、3面总监、4面HR

自我介绍

聊项目、聊架构

为什么用ES、是你引入的吗、自己部署的吗

> 多维度的分析聚合、ES比MySQL更适合
>
> 不是在我入职之前首页筛选已经用上了，我的场景更复杂、选择ES做多维度的分析聚合比较合适
>
> 不是自己部署，行内基础设施建设很完善，需要什么资源在云平台上申请对接就可以使用了

平台建设有几个人、你的角色、负责内容

> 6个人，负责基金、理财、保险、存款、贵金属等品类，作为项目经理，从需求分析、开发设计、架构评审、项目实施、测试支持、上线投产交付、运维全生命周期。从源数据ETL开始，到平台功能构建、数字化赋能输出、产品异动监控等。

近期有没有遇到难题和技术挑战

> 实话实说，并没有，对整个业务开发流程很熟悉，没有多少新的东西，轻车熟路
>
> 这也是自己想跳出去看看外面机会的原因之一，技术成长遇到瓶颈

用什么语言

> 主要Java，少量数据处理Python

对前端了解、有前端经验、需要前端开发

> 讲了在技术预研的时候有前端的经验
>
> JS+HTML+CSS+VUE+ElementUI+Webpack
>
> TS+HTML+CSS+React+antd+vite
>
> 让我不用担心前端问题，公司内部资源很丰富、培训体系也很晚上

面相C端系统的高可用等级，说了小数点后4个9，让说一下具体表示的含义

> 当时算了下不可用的时间，一年不可用时间：86400 * 0.0001 * 365
>
> 具体结果没口算出来，面试官说对这个数字还是不够敏感，给出了具体数字，一年中不可用的时间52min
>
> https://www.cnblogs.com/wjoyxt/p/7477095.html

说出3个技术优势

> 1、数据侧积累的经验：面对复杂数据ETL处理，数据流转的能力;（目前招聘岗位业务需要面向复杂数据处理的能力，考虑招我进去到哪个岗位比较合适）
>
> 2、数字化赋能：构建高并发、高可用系统的能力；
>
> 3、产品异动监控系统：产品异动监控将由自然语言表示的监控方案进行拆解、抽象、进行建模到平台功能的构建、结果的生成和告警推送，对自然语言进行拆解、抽象、建模的能力，业务sense强。
>
> 技术引领业务，不仅懂技术也懂业务

项目中有没有大模型经验的落地

> 实话实说目前没有，后续规划可能会有吧

有没有理财投资经验可以分享？哈哈哈

> 自己收益率惨不忍睹

招银的工作强度、有没有准备好、来这边不是躺平的、来这边追求的是什么、技术的成长、大厂的平台

> 引用IBM前女CEO的名言，`Growth and comfort never coexists.`

在杭州吗、住在哪、哪个区、结婚了吗、有没有女朋友

> 滨江距离菜鸟优点距离，可能得换个房子

拿到了什么offer、要求什么时候入职

> 和之前做的差不多、证券、期货之类的，优先级没有菜鸟高
>
> 要求月底入职

一面面试算法题感觉怎么样，难度如何

> 我说难度还好，面试官说那就行，太难了的话让下次不要那么过，简单的话让下次难一些，哈哈哈

之前也有招银的跳过去，干了两年半调到恒生了，很优秀，学历背景都很好

聊了公司发展前景，最近对阿里监管解除限制，从股价市值也能看出来，未来前景好

前面的面试官有没有介绍岗位的细节，有什么想问的，除了技术细节（哈哈哈，作为部门领导脱离一线，很正常）

主要业务线：质量平台建设，Devops工具链建设，压力测试平台建设

# 追觅4面 HR沟通

9.14，本来上周四就约我面试，约的9.9这周1 ，然后HR有绩效考核要处理鸽了，拖到了今天。

大概聊了一会儿，聊薪资、入职时间

目前薪资、构成、预期薪资

表示追多20%的涨幅

个税APP上截图发给HR，后面再聊聊



# 字节3面

9.18 15:00 ~ 15:54

自我介绍

ES的底层实现、架构

> Elasticsearch的架构是分布式的，包括多个节点，每个节点可以是主节点或数据节点。主节点负责集群管理和负载均衡等任务，数据节点负责存储和检索数据。每个节点都可以自由加入或退出集群，具有自动发现和自动平衡功能。Elasticsearch还具有**分片和副本机制**，可以**将一个索引分成多个部分，每个部分称为一个分片，每个分片可以有多个副本，以提高数据冗余和可用性**。
>
> > ### 5. 解释Elasticsearch的分布式架构。
> >
> > Elasticsearch采用了**主从式的分布式架构，其中节点（Node）是最基本的运行单位，每个节点都是一台物理服务器或虚拟机，运行着Elasticsearch服务**。这些**节点可以组成一个集群（Cluster），共同存储和索引数据**。
> >
> > - 集群（Cluster）：由一个或多个节点组成，共享数据和配置信息。**每个集群有一个唯一的名字，默认名为`elasticsearch`**。
> > - 主节点（Master Node）：**负责集群的管理和协调工作，如创建或删除索引、跟踪节点状态、平衡分片等**。任何节点都可以成为主节点，但通常推荐配置特定的节点作为主节点候选，以提高稳定性。
> > - 数据节点（Data Node）：**存储实际的数据分片**，并参与数据的索引和搜索操作。在一个大型集群中，通常会有多个数据节点以分散存储和处理负载。
> > - 客户端节点（Client Node）：**不存储数据，仅用于接收客户端请求，然后转发给合适的节点执行**。它可以帮助减轻数据节点的压力，并提供更好的**负载均衡**。
> >
> > Elasticsearch通过自动发现机制让新加入的节点自动加入集群，并通过一致性哈希算法动态分配和重新分配分片，以实现数据的均衡分布和故障恢复。

金融模型

项目有没有涉及到资损

分布式事务

分布式事务实现转账功能

mysql开启一个事务后，一直没有提交会发生什么

> https://blog.51cto.com/u_16213461/8886302#:~:text=%E5%BD%93%E4%B8%80%E4%B8%AA%E4%BA%8B%E5%8A%A1%E4%B8%8D%E6%8F%90%E4%BA%A4%E6%88%96%E5%9B%9E%E6%BB%9A%E6%97%B6%EF%BC%8C%E5%AE%83%E5%B0%86%E5%8D%A0%E7%94%A8%E9%94%81%E8%B5%84%E6%BA%90%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%85%B6%E4%BB%96%E4%BA%8B%E5%8A%A1%E6%97%A0%E6%B3%95%E5%B9%B6%E5%8F%91%E6%89%A7%E8%A1%8C%EF%BC%8C%E4%BB%8E%E8%80%8C%E9%99%8D%E4%BD%8E%E4%BA%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%B9%B6%E5%8F%91%E6%80%A7%E8%83%BD%E3%80%82,%E7%BB%BC%E4%B8%8A%E6%89%80%E8%BF%B0%EF%BC%8CMySQL%20%E4%BA%8B%E5%8A%A1%E5%BC%80%E5%90%AF%E4%B9%8B%E5%90%8E%E4%B8%8D%E6%8F%90%E4%BA%A4%E6%88%96%E5%9B%9E%E6%BB%9A%EF%BC%8C%E4%BC%9A%E5%AF%BC%E8%87%B4%E6%95%B0%E6%8D%AE%E4%B8%80%E7%9B%B4%E8%A2%AB%E9%94%81%E5%AE%9A%EF%BC%8C%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%EF%BC%8C%E5%8D%A0%E7%94%A8%E6%95%B0%E6%8D%AE%E5%BA%93%E8%B5%84%E6%BA%90%E4%BB%A5%E5%8F%8A%E9%99%8D%E4%BD%8E%E5%B9%B6%E5%8F%91%E6%80%A7%E8%83%BD%E7%AD%89%E9%97%AE%E9%A2%98%E3%80%82
>
> > 在 MySQL 中，如果你开启了一个事务但没有提交（`COMMIT`）也没有回滚（`ROLLBACK`），可能会发生以下几种情况：
> >
> > 1. **锁定资源**：
> >    - 在某些隔离级别下，例如可重复读（Repeatable Read）或者更高，未提交的事务可能会锁定某些行或范围，阻止其他事务修改这些数据直到当前事务结束。这可能导致其他事务需要等待，甚至可能引起死锁。
> >
> > 2. **占用内存**：
> >    - 事务会占用系统资源，包括内存和其他计算资源。长时间未提交的事务可能会导致服务器内存使用增加。
> >
> > 3. **阻塞其他操作**：
> >    - 其他事务或查询可能因为等待锁定释放而被阻塞，这会影响数据库的整体性能。
> >
> > 4. **持久化问题**：
> >    - 数据库中的更改不会被永久保存到磁盘上，只有当事务提交后才会真正写入到磁盘。如果事务一直不提交，那么在发生系统崩溃的情况下，这部分数据将无法恢复。
> >
> > 5. **连接超时**：
> >    - 如果一个连接长时间保持打开状态并且没有活动，MySQL 的默认配置可能会导致连接超时。此时，你需要重新建立连接，并且之前的事务会被自动回滚。
> >
> > 6. **事务大小限制**：
> >    - MySQL 对事务的大小也有一定的限制。如果一个事务运行得太久，可能会超出系统能够支持的最大事务大小。
> >
> > 为了避免上述问题的发生，你应该尽量缩短事务的持续时间，确保所有事务最终都被正确地提交或回滚。此外，合理使用事务隔离级别，根据应用程序的需求选择合适的级别，可以减少锁定的影响。
> >
> > 如果你发现事务长时间未提交，应该检查应用程序是否有逻辑错误或者异常情况，并确保事务逻辑正确无误。在开发过程中，也应该加入适当的异常处理机制，确保事务能够在发生错误时及时回滚。

有没有涉及到资损

项目中难点、最具有挑战的项目

> 项目中提到使用sentinel、jetcache、MySQL MGR，面试官说这些都是使用第三方的组件，你做了什么
>
> 内心OS：不使用第三方中间件，自己造轮子吗，问得好奇怪

怎么保证系统高可用、有什么方法论

算法题：合并N个有序数组，数组平均长度为K，时间复杂度尽可能小

```
public class MergeArrays {
 
    public static void main(String[] args) {
        int[][] nums = {{1, 4, 8, 9}, {2, 3, 6, 10}, {5, 18, 20, 36}};
        for (int num : mergeArray(nums)) {
            System.out.println(num);
        }
    }
 
    第一种方法：
     static class Node implements Comparable<Node> {
        int val;
        int index;
        int next;
        public Node(int val, int index, int next) {
            this.val = val;
            this.index = index;
            this.next = next;
        }
        @Override
        public int compareTo(Node o) {
            return this.val - o.val;
        }
    }
 
    //合并N个M长度的有序数组
    public static int[] mergeSortedArrays(int[][] arrays) throws Exception {
        if (arrays == null || arrays.length == 0) {
            throw new Exception("数组为空");
        }
        if (arrays.length == 1) {
            return arrays[0];
        }
        int n = arrays.length;
        int k = arrays[0].length;
        int[] res = new int[n * k];
        PriorityQueue<Node> pq = new PriorityQueue<>();
        for (int i = 0; i < n; i++) {
            pq.offer(new Node(arrays[i][0], i, 0));
        }
        int index = 0;
        while (index < n * k) {
            Node temp = pq.poll();
            if (temp != null) {
                res[index++] = temp.val;
                if (temp.next + 1 < k) {
                    pq.offer(new Node(arrays[temp.index][temp.next + 1], temp.index, temp.next + 1));
                }
            }
        }
        return res;
    }
 
    第二中方法：
    public static int[] mergeArray(int[][] nums) {
        if (nums == null || nums.length == 0) {
            return null;
        }
 
        if (nums.length == 1) {
            return nums[0];
        }
        int len = nums.length;
        boolean flag = len % 2 != 0;
        int half = flag == true ? len / 2 + 1 : len / 2;
        int[][] resultNums = new int[half][];
        for (int i = 0; i < len+2; i += 2) {
            if (flag && i == len - 1) {
                resultNums[i / 2] = nums[i];
            } else {
                resultNums[i / 2] = mergeTwoArray(nums[i], nums[i + 1]);
            }
        }
        return mergeArray(resultNums);
 
    }
 
    public static int[] mergeTwoArray(int[] num1, int[] num2) {
        int len1 = num1.length;
        int len2 = num2.length;
        int index1 = 0;
        int index2 = 0;
        int index = 0;
        int[] resultArray = new int[len1 + len2];
        while (index1 < len1 && index2 < len2) {
            if (num1[index1] < num2[index2]) {
                resultArray[index++] = num1[index1++];
            } else {
                resultArray[index++] = num2[index2++];
            }
        }
 
        while (index1 < len1) {
            resultArray[index++] = num1[index1++];
        }
 
        while (index2 < len2) {
            resultArray[index++] = num2[index2++];
        }
        return resultArray;
    }
 
}
```







-------



https://youle.zhipin.com/articles/c0404134be3751f0qxB72dq6Ew~~.html



-------



#  菜鸟1面补面

9.22 11:00 ~ 12:17

自我介绍

聊项目

项目中难点、挑战

之前的工作项目迭代快吗

数据、前端、后端感谢哪些，想做哪部分？

> 回答都有做过，都OK，后面说菜鸟都是全栈，数据、前后端都写，和在nt一样，都写。。。。。。。



mysql联合索引和单独索引、最左匹配原则、什么情况下建立联合索引、什么情况下使用单独索引

> 

项目中遇到过什么生产问题

CPU、内存占用率高

提到遇到过OOM，追问OOM怎么定位问题、怎么解决的



数据ETL怎么做的、开窗函数了解吗

> https://blog.csdn.net/baomingshu/article/details/137017141
>
> https://www.cnblogs.com/FatalFlower/p/17626994.html
>
> https://mysql.net.cn/doc/refman/8.0/en/window-functions-usage.html#google_vignette



mysql行转列、列转行

> ```
> 这三行
> 
> key value
> 
> 1  2
> 
> 1  3
> 
> 1  4
> 
> 如果转化成一行？ 
> 
> 1 2 3 4
> 
> 通俗 就是 行转列 你有什么思路和方法？
> ```
>
> https://www.cnblogs.com/jingyu-zhang/p/16693189.html
>
> https://www.cnblogs.com/ministep/p/17291823.html
>
> https://blog.csdn.net/qq_57492774/article/details/131668936
>
> https://www.bilibili.com/video/BV1184y1q7DU/?spm_id_from=333.337.search-card.all.click&vd_source=d88527641a2c5d74436ce88743d2fd55
>
> https://www.bilibili.com/video/BV1ZF41187sp/?spm_id_from=333.337.search-card.all.click&vd_source=d88527641a2c5d74436ce88743d2fd55
>
> https://www.cnblogs.com/jingyu-zhang/p/16693189.html



刷没刷过力扣

> 刷过
>
> 问：刷过哪些？
>
> 答：hot100
>
> 问：全都刷过一遍吗
>
> 答：大部分刷过

做了两道算法题。。。。。。面试没开空调，出汗了，有点紧张，还好最后写出来了。。。。。。

算法题：

![image-20240920122418530](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240920122418530.png)

> 这三行
>
> key value
>
> 1  2
>
> 1  3
>
> 1  4
>
> 如果转化成一行？ 
>
> 1 2 3 4
>
> 通俗 就是 行转列 你有什么思路和方法？
>
> 给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串 s ，判断字符串是否有效。
>
> 有效字符串需满足：
>
> 左括号必须用相同类型的右括号闭合。
>
> 左括号必须以正确的顺序闭合。
>
> 每个右括号都有一个对应的相同类型的左括号。
>
> 示例 1：
>
> 输入：s = "()"
>
> 输出：true
>
> 示例 2：
>
> 输入：s = "()[]{}"
>
> 输出：true
>
> 示例 3：
>
> 输入：s = "(]"
>
> 输出：false
>
> 示例 4：
>
> 输入：s = "([])"
>
> 输出：true
>
> 提示：
>
> 1 <= s.length <= 104
>
> s 仅由括号 '()[]{}' 组成
>
> class Solution {
>
> public boolean checkBracket(String str) {
>
> if(str == null || str.length == 0) {
>
> return false;
>
> }
>
> int len = str.length;
>
> Map<Character, Character> mp = new HashMap<>();
>
> mp.put('(', ')');
>
> mp.put('[', ']');
>
> mp.put('{', '}');
>
> Deque<Character> dq = new LinkedList<>();
>
> for(int i = 0; i < len; i++) {
>
> char ch = str.charAt(i);
>
> // 右括号
>
> if(!mp.containsKey(ch) {
>
> ​    if(dp.isEmpty()) {
>
> ​     return false;
>
> ​    }
>
> ​    char top = dp.pop();
>
> ​    if(mp.get(top) != ch) {
>
> ​     return false;
>
> ​    }
>
> }
>
> // 左括号 
>
> else {
>
> ​    dp.push(ch);
>
> }
>
> }
>
> return dp.isEmpty();
>
> }
>
> }
>
> 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。如果可以利用字典中出现的一个或多个单词拼接出 s 则返回 true。
>
> 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。
>
> 示例 1：
>
> 输入: s = "leetcode", wordDict = ["leet", "code"]
>
> 输出: true
>
> 解释: 返回 true 因为 "leetcode" 可以由 "leet" 和 "code" 拼接成。
>
> 示例 2：
>
> 输入: s = "applepenapple", wordDict = ["apple", "pen"]
>
> 输出: true
>
> 解释: 返回 true 因为 "applepenapple" 可以由 "apple" "pen" "apple" 拼接成。
>
> 注意，你可以重复使用字典中的单词。
>
> 示例 3：
>
> 输入: s = "catsandog", wordDict = ["cats", "dog", "sand", "and", "cat"]
>
> 输出: false
>
> 
>
> 提示：
>
> 
>
> 1 <= s.length <= 300
>
> 1 <= wordDict.length <= 1000
>
> 1 <= wordDict[i].length <= 20
>
> s 和 wordDict[i] 仅由小写英文字母组成
>
> wordDict 中的所有字符串 互不相同
>
> class Solution {
>
> public boolean checkString(string str, string[] wordDict) {
>
> int len = str.length;
>
> int start = 0;
>
> int end = 0;
>
> Set<String> set = new HashSet<>();
>
> for(String word : wordDict) {
>
> set.add(word);
>
> }
>
> for(int i = 0; i < len; i++) {
>
> end = i;
>
> String subString = str.substring(start, end);
>
> if(set.contains(subString)) {
>
> ​    start = i + 1;
>
> ​    end = i + 1;
>
> }
>
> }
>
> return start == len;
>
> }
>
> }
>
> ```java
> 这三行
> 
> key value
> 
> 1  2
> 
> 1  3
> 
> 1  4
> 
> 如果转化成一行？ 
> 
> 1 2 3 4
> 
> 通俗 就是 行转列 你有什么思路和方法？
> 
> 给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串 s ，判断字符串是否有效。
> 
> 有效字符串需满足：
> 
> 左括号必须用相同类型的右括号闭合。
> 
> 左括号必须以正确的顺序闭合。
> 
> 每个右括号都有一个对应的相同类型的左括号。
> 
> 示例 1：
> 
> 输入：s = "()"
> 
> 输出：true
> 
> 示例 2：
> 
> 输入：s = "()[]{}"
> 
> 输出：true
> 
> 示例 3：
> 
> 输入：s = "(]"
> 
> 输出：false
> 
> 示例 4：
> 
> 输入：s = "([])"
> 
> 输出：true
> 
> 提示：
> 
> 1 <= s.length <= 104
> 
> s 仅由括号 '()[]{}' 组成
> 
> class Solution {
>     public boolean isValid(String s) {
>         if (s == null || s.length() == 0) {
>             return false;
>         }
>         int len = s.length();
>         Map<Character, Character> mp = new HashMap<>();
>         mp.put('(', ')');
>         mp.put('[', ']');
>         mp.put('{', '}');
>         Deque<Character> dq = new LinkedList<>();
>         for (int i = 0; i < len; i++) {
>             char ch = s.charAt(i);
>             // 右括号
>             if (!mp.containsKey(ch)) {
>                 if (dq.isEmpty()) {
>                     return false;
>                 }
>                 char top = dq.pop();
>                 if (mp.get(top) != ch) {
>                     return false;
>                 }
>             }
>             // 左括号
>             else {
>                 dq.push(ch);
>             }
>         }
>         return dq.isEmpty();
>     }
> }
> ```
>
> ```java
> 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。如果可以利用字典中出现的一个或多个单词拼接出 s 则返回 true。
> 
> 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。
> 
> 示例 1：
> 
> 输入: s = "leetcode", wordDict = ["leet", "code"]
> 
> 输出: true
> 
> 解释: 返回 true 因为 "leetcode" 可以由 "leet" 和 "code" 拼接成。
> 
> 示例 2：
> 
> 输入: s = "applepenapple", wordDict = ["apple", "pen"]
> 
> 输出: true
> 
> 解释: 返回 true 因为 "applepenapple" 可以由 "apple" "pen" "apple" 拼接成。
> 
> 注意，你可以重复使用字典中的单词。
> 
> 示例 3：
> 
> 输入: s = "catsandog", wordDict = ["cats", "dog", "sand", "and", "cat"]
> 
> 输出: false
> 
> class LC139 {
>     public boolean checkString(String str, String[] wordDict) {
>         int len = str.length();
>         int start = 0;
>         int end = 0;
>         Set<String> set = new HashSet<>();
>         for (String word : wordDict) {
>             set.add(word);
>         }
>         for (int i = 0; i < len; i++) {
>             end = i;
>             String subString = str.substring(start, end);
>             if (set.contains(subString)) {
>                 start = i + 1;
>                 end = i + 1;
>             }
>         }
>         return start == len;
>     }
> }
> ```
>
> 



反问：

菜鸟ETL都在做什么

> 主要是Devops工具链过程中产生的数据，工单数据等

数据、前端、后端都做，全栈，前端用React



# 阿里云1面

9.20 14:00 ~ 14:50 阿里云部门Team Leader

自我介绍

聊项目、聊架构

简历了写了很多业务相关的、介绍下技术、架构

sentinel、jetcache使用场景

redis缓存击穿、怎么解决、布隆过滤器原理

布隆过滤器怎么设置

QPS多少、多少台机器、平均RT多少

> 峰值QPS接近1W，HTTP连接池400，平均RT100ms，每台机器每秒能够处理400 ÷ 0.1 = 4000个请求，
>
> 6台机器，6 * 4000 = 24000，能够承受24000的QPS
>
> 面试官表示压力可能有点大，回答说那是之前股市、债市表现比较好的时候，最近关注的人少了不少，流量小了，而且行内对于资源利用率，比如CPU、内存等的使用率有要求，使用率低了要求整改，缩资源（降本增效无处不在。。。。。）

架构是分布式的吗

> 没有做微服务的拆分，应用容器化集群部署

有做压力测试吗

压力测试接口有瓶颈怎么处理

> 首先想到横向扩展、加资源

 除了使用sentinel、jetcache等框架、还做了什么对系统进行优化

如果一个接口的RT的响应时间很长，比如500ms，应该怎么定位排查问题

团队多少人？你是项目owner，是你主导负责的吗

> 招银的开发模式是这样的，我负责基金、理财、保险、存款、贵金属等产品品类，作为项目经理，全面负责从需求分析到开发设计、到实施交付、到持续运维，按产品品类和业务场景进行划分归属

小组都是围绕这个平台开展的吗，除了这个还负责什么产品

是正式员工吗？

职级、有没有晋升



面试官表示应该先找好工作再离职的，最好不要gap，不然HR可能觉得你哪里大礼包可能是公司裁员、被动离开的

技术上没问题，学历、经历各方面都很优秀，对于GAP的事情，在最终候选人排序的时候，HR可能比较介意

2个hc，面试的很多，排序录取。。。。。。。



# 滴滴HR面

9.20 17:00 ~ 17:30

自我介绍

困难和挑战、技术上的成长

在不在杭州、为什么选择来杭州、打算在杭州长期发展吗

简历上没有写年龄，年龄多少

怎么写出高质量代码，说是前几天听到的问题，让我谈谈

离职状态、为什么离职

> 业务很熟悉了，没有多少新的东西，技术成长遇到瓶颈
>
> 和身边同学、朋友对比，当初毕业做出来不同选择，现在发现差距开始拉大，peer pressure，朋辈压力
>
> 银行业承压、薪资待遇福利收窄
>
> 提升自身能力，拓宽技术视野，寻求自身成长

本来面的是二面面试官刘胜琦负责的部门下的岗位，三面面试官薛博(真是博士)作为交叉面，现在胜琦部门招到人了，岗位变成薛博部门的了，负责换电方面的工作，主要和宁德时代合作

小桔能源有两个博士，薛博是其中一个

是不是这个？不确定是不是这个岗位

![image-20240921220521715](C:\Users\jingdyangx\AppData\Roaming\Typora\typora-user-images\image-20240921220521715.png)



> https://blog.csdn.net/2301_77835649/article/details/132767496
>
> https://developer.aliyun.com/article/1425844
>
> https://blog.csdn.net/Roinli/article/details/139605837





9.23 上午，问期望薪资，表示期望总包40+，hr表示预算达不到，帮我申请下，看能达到多少，接受的话，就进入offer和背调环节，问10.1之前还是之后入职，

15薪资

我说的拿到了券商的offer，地点不在杭州，自己比较倾向于留着杭州

之前的offer，地点不在杭州，之前拒掉了，



---------

下午两点半薪资沟通：

23 * 15 = 345

每周2，周4入职

试用期6个月，试用期薪资100%，薪酬待遇都一样

饭补30，

换电业务，定级D6

年假8天，病假6天

12%公积金，

不打卡，十点上班7点下班，中午休息2小时

等级S，A，B，C，D

B及其以上都能拿到3个月，S难拿，百分之95以上能拿到B

C出现生产事故，



每年一次调薪，6月之前，不同部门不一样，可能3、4、5月

-----------

9.25发正式书面offer，填写背调信息和入职信息

9.26收到背调公司(全景求是)电话，开启背调，说是3个工作日之内完成背调

-----------



# 菜鸟3面（算上2次一面，这次是4面）部门负责人面

9.23 13:00 ~ 14:13

自我介绍

聊项目、聊架构、结合具体例子讲讲实现细节

产品风险移动监控系统

sentinel使用

应用怎么部署的？

mysql知道哪些使用问题

招银和招商银行的关系

为什么出来看机会

> 自身成长发展有限，业务很熟悉了，技术上没有比较多的挑战和新的东西，
>
> 部分来自和同学朋友交流的peer pressure(朋辈压力)，当初做出了不同的选择，现在有些差距且差距有继续拉大的趋势，
>
> 回想起前IBM CEO的一句话，growth and comfort never coexists，成长和舒适不能共存，想趁自己还年轻的时候，多积累和成长，
>
> 圈里面待久了，成长和积累比较慢比较有限，成长是一个熵增的过程，从无序到有序的一个过程，比如记忆从胡乱到清晰，从手生到手熟悉，都是从无序到有序的，熵增的过程是需要做工的，舒适圈里呆久了成长积累需要做的功就少了
>
> 做技术的，没有在阿里这样的大厂历练过，总觉得自己的职业生涯缺少了点什么，希望和领域内最优秀的一批人一起工作，多历练自己，拓宽技术视野，多积累和成长

阿里这边目前还有没有在面试

> 阿里云

反问：开发模式

> 没有专门的测试同学，自己测试，没有prd，自己画原型，没有产品，自己规划

# 1

最有成就感的事情

> 2022年参与负责的项目拿到招行信息技术部优秀项目创新奖，收到业务感谢信，自己的工作创造了价值，让行业一线客户经理能够更好的服务一线客户，作为底层能力向手机银行输出，让数以万计的一线用户得到服务，自己做的工作有获得感，能服务于很多人

遇到的最大的困难和挑战

> 复杂金融数据ETL清洗加工，复杂的金融数据流转，基础指标、衍生指标、金融模型实现，保证其准确性、时效性
>
> 面相C端手机银行App输出，高并发场景下系统的高可用保障
>
> 面对困难和挑战，并解决，也得到了响应的成长

为什么选择离职

> 自身成长发展有限，业务很熟悉了，技术上没有比较多的挑战和新的东西，
>
> 部分来自和同学朋友交流的peer pressure(朋辈压力)，当初做出了不同的选择，现在有些差距且差距有继续拉大的趋势，
>
> 回想起前IBM CEO的一句话，growth and comfort never coexists，成长和舒适不能共存，想趁自己还年轻的时候，多积累和成长，
>
> 在舒适圈里面待久了，成长和积累比较慢比较有限，成长是一个熵增的过程，从无序到有序的一个过程，比如记忆从胡乱到清晰，从手生到手熟悉，都是从无序到有序的，熵增的过程是需要做工的，舒适圈里呆久了成长积累需要做的功就少了
>
> 做技术的，没有在阿里这样的大厂历练过，总觉得自己的职业生涯缺少了点什么，希望和领域内最优秀的一批人一起工作，多历练自己，拓宽技术视野，多积累和成长
>
> 

优缺点

> 优点：工作认真、积极、责任心强、细心、有耐心（复杂的金融数据ETL处理），学习能力强、适应能力强
>
> 缺点：不太会拒绝、包揽了自身职责意外的事情，没有做到权责对等，做好了没有显性收益，如果没做好可能担责，可能影响的不止是自己，还会影响小组的绩效表现
>
> 被好朋友吐槽过，让我不要做一个滥好人，不要对谁都好
>
> 学会拒绝是成长的必经之路，自己也在改变

绩效、薪资构成、总包

> 绩效正常
>
> 职级 初2，入职的时候初3，升了1级，21年新员工不参加绩效考核，22年升级，23年升级因为阳了在家养病错过了认证考试，导致23年没有升级
>
> 目前拿到比阿里第一级别的二线大厂offer

# 菜鸟HR面

9.27 17:00



17:26 ~ 17:58 一共32min

自我介绍

这几年的成长有哪些

面的岗位，自身有什么优势



为什么离职

有什么

有没有女朋友、是否异地、看稳定性

家是哪里的、来杭州这么远家里人同意吗



为什么当时选择离职

有没有晋升

> 21年新员工，不参与绩效考核
>
> 22年晋升
>
> 23年绩效正常，认证考试因为阳了没有参加，没有晋升

绩效情况

> 

问我拿了什么offer，什么职级，薪资

> 滴滴，D6，35

最晚9.30给我答复过没过，10.8入职来不及，还要背调和体检



-----

菜鸟

聊薪资

阿里

22 * 16

P5

4.1 ~ 来年3.31发年终奖，考勤周期4.1~来年3.31

(12 + 1 + 3) * 16，

1个月年底和当月工资一起发，年底双薪一起发，3个月3.31发

试用期3个月

90%可以拿到3个月，20%~30%拿到3个月及以上



7天年假，3年后每年加2天，最多15天，5天病假，独生子女5天陪护假





--------





反问：组织结构调整，对我这个岗位有什么影响吗

流程什么时候能走完，手上的offer要求节后第一天10.8入职



> **常见的 HRG/HRD 面试问题参考**
>
> **社招：**
>
> 1) 你为什么离职?
> 2) 你为什么对我们公司感兴趣？
> 3) 你对我们目前的业务了解有多少？你觉得你来做这块业务的优势和劣势分别是什么？
>
> 4) 你对现在的经济形式怎么看？
> 5) 过往项目中遇到的最大的挑战是什么？你是如何解决的？
> 6) 你过往公司的和我们的差异点有哪些？
> 7) 你的薪资是怎么组成的？期望薪资是？
> 8) 你怎么看待自己的职涯？
> 9) 最快什么时候能够到岗
> 10) 是否介意出差？家里人是否支持？
>
> 11)你还有什么需要问我的吗？



算法题





![img](https://www.gov.cn/zhengce/jiedu/tujie/202310/W020231025341289510133_ORIGIN.jpg)



# 风控模型检测平台：

> 蚂蚁金服：
>
> 蚂蚁金服智能风控引擎AlphaRisk介绍 http://giac-history.msup.com.cn/2019/schedule/course?id=13964
>
> 度小满金融：
>
> 



# 数据规模

> 数据截止到20240704
>
> 
> 理财：
> 招行代销理财产品数量：总计3970(包含募集期，封闭运作中，在售(开放期)，已到期)
> 其中已到期产品1142只
>
> 
> 全市场理财产品数量：总计101838只产品，包含未到期和已到期产品。
> 其中未到期产品67966只
>
> ---------------分割线-------------
> 保研产品：
> 总计1447只
> 其中投连险40只(投连险子账户会根据发布净值序列计算衍生指标)
>
> 
> ---------------分割线-------------
> 实物金：
> 总计9721只产品
>
> ---------------分割线-------------
> 存款产品
> 享定存：(在售表示处于销售期的产品，不可售表示不在销售期或已下架的产品)
> 在售：55只
> 即将发售：0只
> 不可售：284只
>
> 大额存单：(在售表示处于销售期的产品，不可售表示不在销售期或已下架的产品)
> 在售：22只
> 即将发售：0只
> 不可售：7563只
>
> 
> 结构性存款：(在售表示处于销售期的产品，不可售表示不在销售期或已下架的产品)
> 在售：34只
> 即将发售：7只
> 不可售：2748只
>
> ---------------分割线-------------
> 基金
> 代销基金(招行手机银行app可以买的)：12442只
> 未代销：8876只

# 分库分表

对于基金、理财产品：

净值表和收益率表：基金5w个产品 * 250 * 5，5万支基金，一年250个交易日，5年会产生6125w行数据。

分库分表：

基金净值表：fund_nav_2023、fund_nav_2024，表主键：fund_id + date

基金收益率表：fund_yield_2023、fund_yield_2024，表主键：fund_id + date

理财净值表：wmp_nav_2023、wmp_nav_2024，表主键：wmp_id + date

理财收益率表：wmp_yield_2023、wmp_yield_2024，表主键：wmp_id + date

根据日期的位数将数据分散到10个表中：fund_nav_0 ~ fund_nav_9

对已经退市的产品数据进行归档，退市产品净值或万分收益不再更新，指标不需要再继续执行日跟任务。





redis缓存什么情况下设置本地、远程和都设置的场景 :star:



> 本地和远程同时使用：
>
> 结合本地缓存：访问速度快、jvm内存容量有限、分布式环境下一致性保证
>
> 远程缓存：
>
> 在使用Redis缓存时，选择设置本地缓存、远程缓存或两者结合的场景主要取决于应用的需求、**性能要求和数据一致性**等因素。以下是一些常见的场景：
>
> 1. 本地缓存：
>
>    \- 场景：适用于对性能要求极高的应用，尤其是需要快速访问的数据。
>    \- 优点：减少网络延迟，提升数据访问速度，适合频繁读取的数据。
>    \- 示例：用户会话信息、热点数据等。
>
> 2. 远程缓存：
>
>    \- 场景：适用于需要共享数据的分布式系统，或者数据量较大，无法全部存储在本地的情况。
>    \- 优点：可以在多个实例之间共享数据，适合需要**高可用性和一致性**的场景。
>    \- 示例：全局配置、用户信息等。
>
> 3. 本地和远程结合：
>
>    \- 场景：适用于需要兼顾性能和一致性的应用。
>    \- 优点：本地缓存可以**快速响应请求**，而远程缓存则确保数据的**一致性和持久性**。
>    \- 示例：电商网站的商品信息，先从本地缓存读取，如果未命中再从远程缓存获取。
>
> 本地缓存：



chatgpt回答：

## :star:

>在使用Redis作为缓存系统时，根据应用场景的不同，可以选择设置本地缓存、远程缓存或者两者结合使用。以下是各种情况的具体说明：
>
>### 本地缓存
>
>- **场景**: 当应用需要**快速访问缓存数据且总体数据量不大时**，可以使用本地缓存。
>- 优势
>  - 极低的延迟，因为数据缓存于进程内。
>  - **减少网络请求**，降低带宽使用。
>- 适用情况
>  - **小规模的单实例应用**。
>  - **对访问速度要求极高的数据**。
>
>### 远程缓存
>
>- **场景**: 适合**需要共享缓存的数据**，或者**在多台服务器上运行的分布式应用场景**。
>- 优势
>  - **所有实例都可以访问同一个缓存数据**。
>  - **解决了本地缓存无法分享的问题**。
>- 适用情况
>  - **大规模、多服务器的分布式架构**。
>  - 需要一致性与共享的数据。
>
>### 本地与远程结合使用
>
>- **场景**: 在多节点分布式系统中，**同时使用本地缓存和远程缓存以获得最佳性能**。
>- 优势
>  - **本地缓存可以快速响应热点数据请求，减少网络延迟**。
>  - **远程缓存提供了一致性和数据共享**。
>- 适用情况
>  - 大流量的网站或应用。
>  - **热数据频繁访问，同时又需要确保新数据及时更新**。
>
>### 总结
>
>从使用场景来看，选择本地、远程或两者结合的缓存策略应基于如下因素：**数据的访问频率、系统架构、网络延迟的影响以及应用对一致性的要求**。

**本地和远程：数据更新比较频繁、热点数据、主机提供给我们，我们包主机的接口，比如盈利概率，每五分钟刷新一次，接口请求量大、并发高，更新频率快，使用二级缓存，本地缓存保证接口快速响应，redis缓存保证分布式环境下数据的一致性和可靠性，本地缓存在应用故障重启之后就没有了，且本地缓存受限于jvm内存，容量有限，而redis则不会。二级缓存架构在提高性能的同时，保证数据的可靠性和一致性。**

一般不是接口性能要求特别高、并发量特别大、 对性能要求特别高的话，redis缓存就足够了。

> https://chatgptfrancais.org/zh-CN/chatgpt-en-ligne/#google_vignette
>
> redis缓存什么情况下设置本地、远程和都设置的场景，使用中文回答：
>
> 在使用Redis缓存时，选择设置本地缓存、远程缓存或两者结合的场景主要取决于应用的需求、**性能要求和数据一致性**等因素。以下是一些常见的场景：
>
> 1. 本地缓存：
>
>    \- 场景：适用于对性能要求极高的应用，尤其是需要快速访问的数据。
>    \- 优点：减少网络延迟，提升数据访问速度，适合频繁读取的数据。
>    \- 示例：用户会话信息、热点数据等。
>
> 2. 远程缓存：
>
>    \- 场景：适用于需要共享数据的分布式系统，或者数据量较大，无法全部存储在本地的情况。
>    \- 优点：可以在多个实例之间共享数据，适合需要高可用性和一致性的场景。
>    \- 示例：全局配置、用户信息等。
>
> 3. 本地和远程结合：
>
>    \- 场景：适用于需要兼顾性能和一致性的应用。
>    \- 优点：本地缓存可以**快速响应请求**，而远程缓存则确保数据的**一致性和持久性**。
>    \- 示例：电商网站的商品信息，先从本地缓存读取，如果未命中再从远程缓存获取。
>
> 选择合适的缓存策略可以有效提升应用的性能和用户体验。希望这些信息对你有所帮助！如果你有其他问题，随时问我哦！
>
> https://console.volcengine.com/ark/region:ark+cn-beijing/experience：
>
> 豆包大模型回答：
>
> 在考虑是否使用本地缓存、远程缓存（如 Redis）或两者都使用时，需要根据具体的应用场景和需求来决定。以下是一些常见的情况和设置场景的考虑因素：
>
> 1. **本地缓存**：
>    - **高并发读场景**：如果应用中有大量的读请求，且这些数据的访问频率很高，并且在一段时间内不会发生变化，那么可以考虑使用本地缓存来减少对远程缓存或数据库的访问压力。例如，一些静态配置信息、频繁使用的字典数据等。
>    - **数据一致性要求不高**：如果数据的一致性要求不是非常严格，允许在一定时间内存在数据的不一致，那么本地缓存可以提供更快的响应速度。
>    - **性能优化**：本地缓存可以减少网络延迟和数据传输开销，提高应用的整体性能。
> 2. **远程缓存（Redis）**：
>    - **分布式系统**：在分布式环境中，多个应用实例需要共享数据时，Redis 作为远程缓存可以提供集中式的数据存储和访问，确保数据的一致性和共享性。
>    - **数据更新频繁**：如果数据需要频繁更新，并且需要在多个应用实例之间保持同步，那么 Redis 可以作为一个有效的缓存解决方案，通过发布/订阅或其他机制来实现数据的同步。
>    - **数据量较大**：当本地缓存无法容纳所有需要缓存的数据时，Redis 可以提供更大的存储容量，并且可以通过分布式架构来扩展存储能力。
>    - **缓存失效策略**：Redis 提供了丰富的缓存失效策略，可以根据业务需求灵活设置缓存的过期时间，确保数据的及时性和有效性。
> 3. **本地缓存和远程缓存都设置**：
>    - **分层缓存架构**：结合本地缓存和远程缓存可以构建一个分层的缓存架构。本地缓存作为第一层缓存，**用于快速响应常见的读请求，减少对远程缓存的访问**。远程缓存作为第二层缓存，**用于存储更多的数据，并在本地缓存未命中时提供数据支持。这种架构可以在提高性能的同时，保证数据的可靠性和一致性**。
>    - **热点数据缓存**：对于一些热点数据，可以将其同时缓存在本地和远程缓存中。本地缓存用于快速响应本地的读请求，而远程缓存用于在多个应用实例之间共享热点数据，避免重复读取数据库。
>    - **数据预热**：在应用启动时，可以从数据库中加载一些常用的数据到本地缓存和远程缓存中，以提高应用的启动速度和初始性能。
>
> 需要注意的是，在实际应用中，需要根据具体的业务需求、数据特点、性能要求和系统架构来综合考虑选择合适的缓存策略。同时，还需要合理设置缓存的过期时间、更新策略和数据一致性机制，以确保缓存的有效性和可靠性。
>
> 以下是一个简单的示例场景来说明不同缓存设置的情况：
>
> 假设我们有一个电商网站，其中商品信息是经常被查询的，但商品信息的更新并不是非常频繁。在这种情况下，我们可以考虑以下缓存设置：
>
> - **本地缓存**：在每个应用服务器的本地内存中，缓存一些热门商品的信息。这些热门商品的信息可以根据历史访问数据进行筛选和确定。当用户发起商品查询请求时，首先在本地缓存中查找，如果命中，则直接返回结果，避免了对远程缓存或数据库的访问，提高了响应速度。
> - **远程缓存（Redis）**：将所有商品的信息存储在 Redis 中。当本地缓存未命中时，从 Redis  中读取商品信息并返回给用户。同时，Redis  中的商品信息可以根据一定的策略进行更新，例如定时从数据库中同步最新的商品信息，或者在商品信息发生变化时及时更新 Redis 中的缓存。
> - **数据库**：作为数据的最终来源，当 Redis 中也未命中商品信息时，从数据库中读取数据并更新到 Redis 和本地缓存中，以保证数据的完整性和准确性。
>
> 通过这种本地缓存和远程缓存相结合的方式，可以在提高系统性能的同时，保证数据的一致性和可靠性，满足电商网站对商品信息查询的需求。当然，具体的缓存设置还需要根据实际情况进行调整和优化，以达到最佳的性能和效果。
>
> 
>
> 通义千问：
>
> JetCache 是一个 Java 缓存框架，它支持多种缓存类型，包括本地缓存、远程缓存（如 Redis）以及组合缓存。JetCache 的设计目标之一就是简化缓存的使用，并通过配置实现缓存的一致性。
>
> ### 保证本地缓存与 Redis 缓存的一致性
>
> 要保证 JetCache 中本地缓存和 Redis 缓存的一致性，通常采用以下几种策略：
>
> 1. **写穿透** (Write Through): 当更新缓存数据时，同时更新本地缓存和远程缓存（Redis）。这种方式可以保证数据的一致性，但可能会增加系统负载，因为每次写操作都需要访问远程缓存。
> 2. **写旁路** (Write Bypass): 更新时仅更新远程缓存（Redis），并清除本地缓存中对应的条目。下次读取时会从远程缓存获取数据并填充到本地缓存。这种方式降低了系统的写负载，但可能会出现短暂的数据不一致。
> 3. **读修复** (Read Repair): 当本地缓存失效或没有命中时，从远程缓存获取数据并更新本地缓存。这种方式不需要额外的操作来保证一致性，但在本地缓存失效后会有一定的延迟。
> 4. **定时刷新** (Scheduled Refresh): 定期同步本地缓存和远程缓存中的数据。这种方式适用于数据更新频率较低的场景。
> 5. **主动失效** (Proactive Eviction): 当远程缓存（Redis）中的数据被更新时，主动清除本地缓存中的对应条目，以确保下次读取时能够从远程缓存中获取最新的数据。
>
> ### 多级缓存的选择
>
> 在 JetCache 中，可以通过配置实现多级缓存，通常分为本地缓存和远程缓存两层。根据不同的场景，可以选择不同的缓存策略：
>
> 1. **高并发、低延迟场景**:
>    - 使用本地缓存作为第一级缓存，减少远程调用次数。
>    - Redis 作为第二级缓存，用于持久化数据和容灾。
> 2. **数据一致性要求高的场景**:
>    - 使用 Redis 作为主缓存，所有读写操作都直接作用于 Redis。
>    - 可选地使用本地缓存作为辅助缓存，提高读取性能。
> 3. **数据量大且需要高可用性的场景**:
>    - 使用 Redis 集群作为主缓存，提供高可用性和水平扩展能力。
>    - 使用本地缓存减少对 Redis 的访问频率。
> 4. **内存限制严格的场景**:
>    - 如果服务器内存有限，则主要依赖 Redis 进行缓存，减少本地缓存的使用。
> 5. **二级缓存**:
>    - 在某些情况下，可能还需要考虑更深层次的缓存，比如使用数据库作为第三级缓存，这通常是为了数据的安全性和持久性。
>
> ### 实际配置示例
>
> 以下是一个简单的配置示例，展示如何设置本地缓存和 Redis 缓存的组合：
>
> 在这个示例中，我们配置了一个名为 `exampleCache` 的缓存，使用 Redis 作为远程缓存，并设置了本地缓存为 Caffeine，当本地缓存失效时会从 Redis 中读取数据。这种配置既减少了 Redis 的访问压力，也提高了数据的读取速度。



jetcache如何保证本地缓存和redis缓存保持一致性？jetcache多级缓存在不同场景下如何选择本地缓存或是redis缓存或者是二级缓存？

> JetCache 是一个缓存框架，用于在应用中实现本地缓存和远程缓存（如 Redis）的管理。要保证本地缓存和 Redis 缓存的一致性，可以考虑以下几种方式：
>
> 1. 缓存更新策略：JetCache 可以设置合适的缓存更新策略。当数据发生变化时，及时更新本地缓存和 Redis 缓存。例如，可以在数据更新操作后，同时将更新后的数据写入到本地缓存和 Redis 中，确保两者的数据一致性。
> 2. 缓存失效机制：设置合理的缓存失效时间，当数据过期时，重新从数据源（如数据库）获取数据并更新到本地缓存和 Redis 中。通过定期的失效和更新操作，可以保持缓存数据的新鲜度和一致性。
> 3. 事件通知或消息队列：当数据发生变化时，可以通过事件通知或消息队列的方式通知缓存系统进行更新。这样可以确保本地缓存和 Redis 缓存能够及时响应数据的变化。
> 4. 数据同步机制：可以定期进行本地缓存和 Redis 缓存的数据同步，以确保两者的数据一致性。例如，可以设置一个定时任务，将 Redis 中的数据同步到本地缓存，或者将本地缓存中的数据同步到 Redis 中。
>
> 关于 JetCache 多级缓存在不同场景下的选择，可以考虑以下因素：
>
> 1. 数据访问频率：**如果数据的访问频率非常高，并且对响应时间要求很高，可以优先考虑使用本地缓存。本地缓存可以提供更快的访问速度，减少网络延迟和数据传输开销**。
> 2. 数据一致性要求：**如果数据的一致性要求非常严格，建议优先使用 Redis 缓存**。Redis 作为分布式缓存，可以在多个应用实例之间保持数据的一致性，确保数据的准确性和可靠性。
> 3. 数据量和存储容量：**如果数据量较大，本地缓存可能无法容纳所有数据，此时可以考虑使用 Redis 缓存来扩展存储容量**。同时，如果需要在多个应用实例之间共享大量数据，Redis 也是一个更好的选择。
> 4. 网络环境：如果应用部署在分布式环境中，网络环境可能不太稳定，此时使用 Redis 缓存可以减少对本地网络的依赖，提高系统的可靠性和稳定性。
> 5. 缓存更新频率：如果数据的更新频率较高，需要及时将更新后的数据反映到缓存中，此时可以根据实际情况选择本地缓存和 Redis 缓存的组合。例如，可以**将频繁更新的数据缓存在 Redis 中，以确保数据的及时更新，而将一些相对稳定的数据缓存在本地缓存中，以提高访问速度**。
>
> 综合考虑以上因素，可以根据具体的业务需求和场景来选择合适的缓存策略。在一些情况下，可能需要同时使用本地缓存和 Redis 缓存，以实现更好的性能和数据一致性。例如，**对于一些热点数据，可以将其同时缓存在本地缓存和 Redis 中，本地缓存用于快速响应本地的读请求，Redis 缓存用于在多个应用实例之间共享数据和保证数据的一致性**。在其他情况下，可能只需要使用本地缓存或 Redis 缓存中的一种，具体取决于数据的特点和应用的需求。



jetcache如何保证本地缓存和redis缓存保持一致性？jetcache多级缓存在不同场景下如何选择本地缓存或是redis缓存或者是二级缓存？

通义千问

> JetCache 是一个 Java 缓存框架，它支持多种缓存类型，包括本地缓存、远程缓存（如 Redis）以及组合缓存。JetCache 的设计目标之一就是简化缓存的使用，并通过配置实现缓存的一致性。
>
> ### 保证本地缓存与 Redis 缓存的一致性
>
> 要保证 JetCache 中本地缓存和 Redis 缓存的一致性，通常采用以下几种策略：
>
> 1. **写穿透** (Write Through): 当更新缓存数据时，同时更新本地缓存和远程缓存（Redis）。这种方式可以保证数据的一致性，但可能会增加系统负载，因为每次写操作都需要访问远程缓存。
>
> 2. **写旁路** (Write Bypass): 更新时仅更新远程缓存（Redis），并清除本地缓存中对应的条目。下次读取时会从远程缓存获取数据并填充到本地缓存。这种方式降低了系统的写负载，但可能会出现短暂的数据不一致。
>
> 3. **读修复** (Read Repair): 当本地缓存失效或没有命中时，从远程缓存获取数据并更新本地缓存。这种方式不需要额外的操作来保证一致性，但在本地缓存失效后会有一定的延迟。
>
> 4. **定时刷新** (Scheduled Refresh): 定期同步本地缓存和远程缓存中的数据。这种方式适用于数据更新频率较低的场景。
>
> 5. **主动失效** (Proactive Eviction): 当远程缓存（Redis）中的数据被更新时，主动清除本地缓存中的对应条目，以确保下次读取时能够从远程缓存中获取最新的数据。
>
> ### 多级缓存的选择
>
> 在 JetCache 中，可以通过配置实现多级缓存，通常分为本地缓存和远程缓存两层。根据不同的场景，可以选择不同的缓存策略：
>
> 1. **高并发、低延迟场景**:
>    - 使用本地缓存作为第一级缓存，减少远程调用次数。
>    - Redis 作为第二级缓存，用于持久化数据和容灾。
>
> 2. **数据一致性要求高的场景**:
>    - 使用 Redis 作为主缓存，所有读写操作都直接作用于 Redis。
>    - 可选地使用本地缓存作为辅助缓存，提高读取性能。
>
> 3. **数据量大且需要高可用性的场景**:
>    - 使用 Redis 集群作为主缓存，提供高可用性和水平扩展能力。
>    - 使用本地缓存减少对 Redis 的访问频率。
>
> 4. **内存限制严格的场景**:
>    - 如果服务器内存有限，则主要依赖 Redis 进行缓存，减少本地缓存的使用。
>
> 5. **二级缓存**:
>    - 在某些情况下，可能还需要考虑更深层次的缓存，比如使用数据库作为第三级缓存，这通常是为了数据的安全性和持久性。
>
> ### 实际配置示例
>
> 以下是一个简单的配置示例，展示如何设置本地缓存和 Redis 缓存的组合：
>
> 在这个示例中，我们配置了一个名为 `exampleCache` 的缓存，使用 Redis 作为远程缓存，并设置了本地缓存为 Caffeine，当本地缓存失效时会从 Redis 中读取数据。这种配置既减少了 Redis 的访问压力，也提高了数据的读取速度。



> 分别说明只使用本地缓存、只使用Redis缓存和同时使用本地缓存和redis缓存的场景
>
> 



二级缓存适用场景

缓存命中率？

本地jvm内存缓存最大的问题是很难保证多实例的一致性，本地缓存主要还是解决redis的网络io开销

-------------

亮点、难点：

> 业务层面：、结合招行零售实际业务场景
>
> 技术层面：多源异构数据清洗加工、衍生指标计算、金融模型实现比较繁琐和困难。
>
> 将数据ETL进行拆分、无依赖关系的独立子任务之间并行计算，优先缩短计算时间。通过RabbitMQ，将业务逻辑、任务依赖判断逻辑、任务分发逻辑实现解耦，有效提高开发效率、缩短开发周期。
>
> MySQL和ES的双写一致性问题。
>
> 使用策略模式、责任链模式，对代码进行优化。
>
> MySQL和Redis双写一致性
>
> 分布式集群架构下，构建高并发、高可用分布式系统，缓存、MySQL、Redis集群模式、主从复制、读写分离，db和cache一致性问题。



> 凤凰架构：https://icyfenix.cn/architect-perspective/general-architecture/diversion-system/cache-middleware.html
>
> 分布式缓存与进程内缓存各有所长，也有各有局限，它们是互补而非竞争的关系，如有需要，完全可以同时把进程内缓存和分布式缓存互相搭配，构成透明多级缓存（Transparent Multilevel Cache，TMC），如图 4-14 所示。先不考虑“透明”的话，多级缓存是很好理解的，使用进程内缓存做一级缓存，分布式缓存做二级缓存，如果能在一级缓存中查询到结果就直接返回，否则便到二级缓存中去查询，再将二级缓存中的结果回填到一级缓存，以后再访问该数据就没有网络请求了。如果二级缓存也查询不到，就发起对最终数据源的查询，将结果回填到一、二级缓存中去。
>
> ![img](data:image/png;base64,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)图 4-14 多级缓存
>
> 尽管多级缓存结合了进程内缓存和分布式缓存的优点，但它的代码侵入性较大，需要由开发者承担多次查询、多次回填的工作，也不便于管理，如超时、刷新等策略都要设置多遍，数据更新更是麻烦，很容易会出现各个节点的一级缓存、以及二级缓存里数据互相不一致的问题。必须“透明”地解决以上问题，多级缓存才具有实用的价值。一种常见的设计原则是变更以分布式缓存中的数据为准，访问以进程内缓存的数据优先。大致做法是**当数据发生变动时，在集群内发送推送通知（简单点的话可采用 Redis 的 PUB/SUB，求严谨的话引入 ZooKeeper 或 Etcd 来处理），让各个节点的一级缓存自动失效掉相应数据**。当访问缓存时，提供统一封装好的一、二级缓存联合查询接口，接口外部是只查询一次，接口内部自动实现优先查询一级缓存，未获取到数据再自动查询二级缓存的逻辑。



>  分布式系统下，本地缓存的使用情况
>
> https://www.cnblogs.com/yaochunhui/p/14378776.html









---------

# 技术面试杂谈（15）你的项目有什么亮点

原创倾城 

编码专家

*2024年01月15日 21:00* *广东*

技术面试官主要关注简历上的两点内容：专业技能、项目经历。有些面试官喜欢问纯粹的技术问题，比如HashMap的实现、线程池的使用等等。但是资深的面试官一定结合项目提问，穿插基础技术问题。如果不能说出项目的亮点或难点，无法体现解决问题的能力，面试官就不会给高分。

大部分人不在大厂工作或者处在大厂的边缘项目，经常抱怨自己的项目平平无奇。其实，只要深度挖掘总能找到一些亮点，建议从以下3个方面入手：

- 系统设计：结合业务需求，设计低成本高效的解决方案
- 性能调优：解决性能瓶颈，提高系统总体吞吐量和稳定性
- 故障排除：快速准确的处理线上故障，并且进行故障复盘

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/TqicVvibZnyDTfolTI8aezS2RbI66KJajLmJwcah75sABGWJpl3P4QiatJSaxH895czfGwkT4knKZCL6T5ian46NTg/640?wx_fmt=png&from=appmsg&wxfrom=13)

在日常开发中就要总结项目的亮点，早点为面试存储一些材料。描述项目亮点要注意技巧，把面试官引入自己熟悉的知识点上去，看看下面的模拟对话：

```
面试官：这个项目有什么亮点，或者有哪些让你印象深刻的东西？
候选人：我解决了一个核心接口的性能瓶颈，接口耗时从500 ms降低到100 ms，解决的过程是这样的......

面试官：你的简历上提到采用DDD进行了系统设计，能够详细说一下吗？
候选人：我对DDD的理解比较粗浅，但是参与过分库分表的大活儿，您愿意听我说一下吗？
面试官：可以，请说吧！
```

以下内容以Java技术体系为例，罗列了一些项目中值得说一说的亮点。

### 1 系统设计

架构师负责的系统设计一般维度较高，服务整个技术部门。业务部门开发复杂需求时，也要权衡研发成本、系统扩展性等因素，设计出低成本高效的方案。

#### 1.1 运用设计模式

大部分Java项目都是基于Spring Boot 构建，只要按照规约填充业务代码，不需要运用太多设计模式，但是部分场景可能用到单例模式、外观模式、责任链模式。

- 单例模式（Singleton Pattern）

单例模式确保一个类只有一个实例，并提供全局访问点，常常用于管理受限的资源，使用场景如下：
（1）数据库连接池：单例模式确保在只有一个数据库连接池实例，避免出现重复连接的问题。
（2）日志处理器：在大多数情况一个日志处理器实例就足够了，单例可以减少资源的浪费。
（3）性能管理器：单例模式可以确保性能管理器的整个生命周期中只有一个实例运行。
（4）线程池：采用单例模式创建线程池，可以避免内存使用过高。

- 外观模式（Facade Pattern）

外观模式为子系统定义了一个统一的接口，客户端无须关心子系统的工作细节，通过外观角色即可调用相关功能，简化了内外通信的调用规则。典型的外观角色代码：

```
public class Facade
{
    private SubSystemA obj1 = new SubSystemA();
    private SubSystemB obj2 = new SubSystemB();
    private SubSystemC obj3 = new SubSystemC();
    
    public void method()
    {
        obj1.method();
        obj2.method();
        obj3.method();
    }
}
```

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/TqicVvibZnyDTfolTI8aezS2RbI66KJajLOzicUF7m33XWYeLJUpjSBYS0NxIVaQsWnanGkdeazecnyFFBTic1I7Dw/640?wx_fmt=jpeg&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
外观模式

外观模式的缺点是违背了“开闭原则“，增加新的子系统需要修改外观类或客户端的代码。

- 责任链模式（Responsibility Chain Pattern）

责任链模式指责任请求沿着处理者链前进， 每个处理者要么处理请求，要么传递给链上的下个处理者。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/TqicVvibZnyDTfolTI8aezS2RbI66KJajL1KYPliaiaehSd1Heml5KzwhKfEF5yNhk1eUmoFw0AK2EbZwYSGJYoZbQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
责任链模式

责任链的使用场景有3个：

- 流程判断：实现多用户系统的权限控制，比如先校验用户身份，再校验功能授权。
- 业务审批：实现业务流程的层层审批，比如审批经过总经理、人事经理、项目经理。
- 过滤器：实现Web请求过滤，比如编码过滤、判断用户的登陆状态，典型的案例是Java Servlet的过滤器实现。

#### 1.2 分布式ID解决方案

在分布式系统中有多个计算节点，不能依赖单节点生成生成唯一标识符，要保证在全局范围内每个ID是唯一的，这就是分布式ID。分布式ID的需求有5点：

- 全局唯一：无论哪个节点生成ID，都保证全局范围内唯一。
- 趋势递增：分布式ID最终要写入数据库，趋势递增的特性可以保证数据库查询效率。
- 业务含义：分布式ID携带业务含义，比如创建时间、业务类型等。
- 高性能：生成效率高，不能存在严重的性能瓶颈。
- 高可用性：个别节点宕机后，依然能够获取到ID。

常见的分布式ID生成策略有：UUID、号段模式、雪花算法、基于Redis的ID生成方案等。具体选择哪种方案，需要根据系统的实际需求来决定。

- UUID

UUID（Universally Unique Identifier）是指在一台机器上生成的数字，保证对在同一时空中的所有机器都是唯一的。算法用到了以太网卡地址、纳秒级时间、芯片ID码和随机数字。

```
30c0c01c-2b6e-4e91-8de1-5e8a06ea214a
```

UUID的优点是生成效率高、唯一性高，一般适用仅做唯一标识的场景，如：MQ消息的ID，分布式追踪的Trace ID等等。UUID不适合做入库的业务主键，有几个原因：

（1）存储成本高：UUID是36个字节的字符串，部分场景需要短一点。
（2）信息不安全： 基于MAC地址生成的UUID算法暴露MAC地址。
（3）没有趋势递增： 关系性数据库希望主键越短越好，无序性也会造成索（4）引数据位置频繁变动，影响插入和查询性能。

- 号段模式

号段模式是指每次从数据库取出一个号段范围，加载到服务内存中，调用方在这个范围递增取值。例如 (1,1000] 代表1000个ID，具体的业务服务将本号段生成1~1000的自增ID，数据表结构如下：

```
CREATE TABLE id_generator (
  id int(10) NOT NULL,
  max_id bigint(20) NOT NULL COMMENT '当前最大id',
  step int(20) NOT NULL COMMENT '号段的长度',
  biz_type    int(20) NOT NULL COMMENT '业务类型',
  version int(20) NOT NULL COMMENT '乐观锁版本号',
  PRIMARY KEY (`id`)
)
```

等这批号段ID用完，再次向数据库申请新号段，对max_id字段做一次update操作，新的号段范围是 (max_id ,max_id + step)。多业务端可能同时操作，通常采用乐观锁方式更新。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
号段模式

号段模式的高可用依赖数据库的高可用架构，当应用重启的时候部分ID会被浪费，不过这是小问题。

- Redis INCR

Redis INCR 命令用于对存储在 Redis 中的键进行自增操作。它适用于存储为整数的值，每次执行 INCR 命令，键的值将会增加1，并返回自增后的值。如果键不存在，则会创建一个新的键，并将其初始值设置为0。演示命令如下：

```
INCR key
```

借助 Redis 极强的并发特性，这个方案既能够实现高并发又能保证单调递增。Redis主从之间是异步复制的，如果Master宕机之前，还有数据没有同步到Slave，而 Slave被选举为 Master，就会出现ID重复的问题。

- 雪花算法

雪花算法是Twitter开源的分布式ID生成算法，以划分命名空间的方式将64bit位分割成了多个部分，每个部分都有具体的不同含义，如下所示：

第一部分： 占用1bit，第一位为符号位 第二部分：41位的时间戳，41bit位可以表示241个数，每个数代表的是毫秒，那么雪花算法的时间年限是(241)/(1000×60×60×24×365)=69年 第三部分：10bit表示是机器数，即 2^ 10 = 1024台机器，通常不会部署这么多机器 第四部分：12bit位是自增序列，可以表示2^12=4096个数，一秒内可以生成4096个ID，理论上snowflake方案的QPS约为409.6 w/s。

雪花算法强依赖机器时钟，如果机器上时钟回拨会导致发号重复，通常通过记录最后使用时间处理该问题。

#### 1.3 实现接口幂等性

接口幂等性，是指多次请求某一个资源对于资源本身应该具有同样的结果。业务开发中，经常会遇到重复提交的情况，比如用户在APP上连续点击了多次提交订单，后台应该只产生一个订单。实现接口幂等性有很多种方案，不同的方案的成本不同，要根据实际情况实施。

- 数据库唯一索引

唯一索引适用于插入数据时的幂等性，保证一张表中只能存在一条符合唯一索引的记录。唯一索引可以是主键，也可是组合唯一索引。组合唯一索引是将重复的条件判定放在了数据库层。

- 防重令牌

调用方在调用接口的时候先向后端请求一个全局 Token，请求的时候携带这个全局 Token 一起请求（Token 最好将其放到 Headers 中），后端需要对这个 Token 作为 Key，用户信息作为 Value 到 Redis 中进行键值内容校验，如果 Key 存在且 Value 匹配就执行删除命令，然后正常执行后面的业务逻辑。

### 2 性能调优

#### 2.1 降低接口耗时

过早优化是万恶之源，一定要优化接口耗时的话，要估算收益是否大于成本，具体的优化步骤：
（1）接口调研：根据日志确定耗时高是常态，不是偶发性。结合监控系统、链路跟踪系统等工具定位性能瓶颈。
（2）制定方案：结合调用方的需求和开发人员的经验确定优化方案，比如调用方希望耗时在3秒内或者接受异步通知。
（3）实施方案：通过代码重构、引入中间件等方式改造接口，最好包含灰度方案或者动态切换新老逻辑的开关。

常规业务系统譬如电商、物流都属于数据密集型系统，即数据是系统成败的决定性因素，包括数据的规模、数据的复杂度、数据产生与变化的速率。这类系统对IO的操作频率远远高于CPU，减少IO操作、提高CPU利用率是性能优化的大方向。排除掉网络质量问题，导致接口性能问题的原因很多。

- 数据库读写性能差

系统发展到一定的阶段，单实例的数据库一定无法支撑高并发的读写，优先考虑的应该是索引优化和冷数据归档，分库分表是最后的大招。 
（1） **冷数据归档**：将用户不太关注的历史数据从单表中迁移走，前端提示用户只提供最近N个月数据。保证每个表的数据在一千万左右，查询耗时在0.5秒以内。
（2）**索引优化**：索引可以大大提高数据的查询速度。索引优化需要重点关注索引失效的原因。如果单表的数据量过大，优化索引也无法改善性能。
（3） **数据缓存**：读多写少、弱实时性的场景，尝试缓存数据。缓存的介质常常是内存，查询速度远高于数据库的磁盘，提升性能的效果非常明显。常用分布式缓存组件是Redis，二级缓存组件有Guava Cache、Caffeine、Encache，Spring Cache可以集成使用这三者。

- 业务逻辑复杂

（1）**异步调用接口**：接口异步调用是最直接的方案，耗时都在网络请求和RPC连接上，耗时肯定在1秒内。由于无法同步返回数据，调用方要异步处理结果，增加了系统复杂性。
（2）**异步处理部分业务**：如果不能接受整个接口异步调用，考虑将部分非核心流程异步执行。比如下单接口包含查库存、生成订单、发送短信三个步骤，发送短信不是核心流程，可以改为发送MQ消息触发短信，能省下一点耗时。
（3）**多业务并行处理**：在满足业务逻辑的前提下，将没有关联的步骤由串行改为并行执行。比如有A和B两个步骤，分别耗时200ms和100ms，并行执行后最大耗时就是A的200ms。以下代码演示了Java语言的并行处理，将“查询满100减10信息”和“查询可用优惠券信息”的结果汇总返回给接口。
（4）**调整业务流程**：如果从技术角度上找到更好的方案，可以尝试调整业务流程。将一个接口做完的事情，拆成两三个接口来做，每个接口的耗时自然就减少了。再通过校验业务数据，保证拆分后的接口的顺序调用。

#### 2.2 系统限流策略

任何系统的性能都有一个上限，当并发量超过这个上限之后，可能会对系统造成毁灭性地打击。因此必须限制并发请求数量不能超过某个阈值，限流就是为了完成这一目的。

- 计数器算法

规定在1分钟内访问接口次数不能超过1000个，设置一个计数器，对固定时间窗口1分钟进行计数，每有一个请求，计数器就+1，如果请求数超过了阈值，则舍弃该请求，当时间窗口结束时，重置计数器为0。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
计数器算法

计数器算法虽然简单，但是有临界问题。假设有一个用户在0:59时，瞬间发送了1000个请求，并且1:01又发送了1000个请求，那么其实用户在 2秒里面，瞬间发送了2000个请求。用户在时间窗口的重置节点处突发请求， 可以瞬间超过速率限制，压垮应用。

- 滑动窗口算法

滑动窗口算法解决了计数器算法的缺点。计数器的时间窗口是固定的，而滑动窗口的时间窗口是动态的。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
滑动窗口算法

橙色的矩形框表示一个时间窗口，一个时间窗口就是一分钟。滑动窗口划成了4格，每格代表的是15秒钟。每过15秒钟，时间窗口就会往右滑动一格。每一个格子都有自己独立的计数器，比如当一个请求在0:30秒的时候到达，那么0:30~0:44对应的计数器就会加1。那么滑动窗口怎么解决刚才的临界问题的呢？0:59到达的1000个请求落在格子中，而1:01到达的请求会落在后面的格子中。当时间到达1:00时，窗口会往右移动一格，此时时间窗口内的总请求数量一共是2000个，超过了限定的1000个，此时能够检测出来触发限流。

- 漏桶算法

想想一个场景：一个水桶的桶底下有一个小孔，水以固定的频率流出，水龙头以任意速率流入水，当水超过桶则”溢出“。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
漏桶算法

漏桶算法保证了固定的流出速率，这是优点也是缺点。始终恒定的处理速率有时候并不一定是好事情，对于突发的请求洪峰，在保证服务安全的前提下，应该尽最大努力去响应，这个时候漏桶算法显得有些呆滞，最终可能导致水位溢出，丢弃请求。

- 令牌桶算法

很多限流场景除了限制数据的平均传输速率，还要允许某种程度的突发传输。这时候漏桶算法可能就不合适了，令牌桶算法更为适合。令牌桶算法的原理是系统以恒定的速率产生令牌，然后把令牌放到令牌桶中，令牌桶有一个容量，当令牌桶满了的时候，再向其中放令牌，那么多余的令牌会被丢弃；当想要处理一个请求的时候，需要从令牌桶中取出一个令牌，如果此时令牌桶中没有令牌，那么则拒绝该请求。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)
令牌桶算法

令牌桶算法的缺点是，需要根据以往的系统性能、用户习惯等判断令牌数量和生成速度，难以预知实际的限流数。

### 3 故障排除

#### 3.1 Java微服务OOM

在生产环境中，JVM内存溢出是很常见的故障，通常分5类错误：

- 堆溢出 java.lang.OutOfMemoryError: Java heap space
- 栈溢出 java.lang.StackOverFlowError
- 元信息溢出 java.lang.OutOfMemoryError: Metaspace
- 直接内存溢出 java.lang.OutOfMemoryError: Direct buffer memory
- GC超限 java.lang.OutOfMemoryError: GC overhead limit exceeded

造成JVM内存溢出通常有4个原因：

- 加载太多数据进入内存，如一次从数据库取出过多数据
- 资源使用之后没有及时关闭，相关对象无法被GC回收
- 代码中存在死循环或循环产生过多重复的对象实体
- JVM启动参数设置不当，内存分配过小

在JVM启动之前，配置OOM的dump日志文件输出路径，如下所示：

```
-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/jvm/outofmemory/ -Xms1024m -Xmx1024m
```

当发生OOM时，JVM在路径 /usr/jvm/outofmemory/ 输出名为 java_pidXXXX.hprof 的文件。使用JProfiler工具分析该文件，就可以定位有问题的源码位置。

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

#### 3.2 RocketMQ消息堆积

当消息消费者的处理能力跟不上消息产生的速度时，消息会积压在消息队列中，这就是MQ消息堆积。一旦出现消息堆积，直接的影响是新消息无法进入队列，用户的反应是系统有延迟，体验不好。

通过RocketMQ的管理控制台可以查阅 topic 的消息堆积情况，如下图所示：

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

业务系统需求不一样，但是造成消息堆积的原因主要有四点：

（1）新上线的消费者代码有问题，无法正常消费消息。

（2）消费者实例宕机，或者无法同Broker建立连接。

（3）生产者短时间内推送大量消息到Broker，消费者消费能力不足。

（4）生产者没有感知Broker消费堆积，持续向Broker推送消息。

无论属于哪一种情况，针对MQ的监控和预警是首要工作，当消息堆积达到一定阈值时，发出预警及时处理。针对以上四点原因，解决方案是：

（1）灰度发布：新功能上线前，选取一定比例的消费实例做灰度，若出现问题及时回滚；若消费者消费正常，平稳运行一段时间后，再升级其它实例。如果需要按规则选出一部分账号做灰度，则需要做好消息过滤，让正常消费实例排除灰度消息，让灰度消费实例过滤出灰度消息。

（2）多活：极端情况下，一个IDC内消费实例全部宕机时，让其他IDC内的消费实例正常消费消息。若一个IDC内Broker全部宕机，生产者要将消息发送至其它IDC的Broker。

（3）增强消费能力：增加消费者线程数或增加消费者实例个数。增加消费者线程数要注意消费者及其下游服务的消费能力，上线前就要将线程池参数调至最优状态。增加消费者实例个数，要注意Queue数量，消费实例的数量要与Queue数量相同，如果消费实例数量超过Queue数量，多出的消费实例分不到Queue，只增加消费实例是没用的，如果消费实例数量比Queue数量少，每个消费实例承载的流量是不同的。

（4）熔断与隔离：当一个Broker的队列出现消息积压时，要对其熔断隔离，将新消息发送至其它队列，过一定的时间，再解除其隔离。



-------

# [生产环境如何排查和优化JVM](https://segmentfault.com/a/1190000042558533)

## 前言

通过前面的介绍，对 JVM 的理论及实践等相关知识有了一个大体的印象。 Java  虚拟机的排查工具是一个合格程序员必备的技能，使用它我们可以很方便地定位出问题的所在，尤其在团队合作的今天，每个人各守一摊很容易出现隐藏的  bug（缺陷）。因此使用这些排查功能可以帮我们快速地定位并解决问题。本文时将重点讲解 JVM 的排查与优化，这样就会对 JVM  的知识点有一个完整的认识，从而可以更好地应用于实际工作或者面试了。

相关的面试题有以下这些：

- 生产环境如何排查问题？
- 除了比较实用的命令行工具之外，有没有方便一点的排查工具？
- JVM 常见的调优手段有哪些？

## 生产环境如何排查问题？

如果是在生产环境中直接排查 JVM 的话，最简单的做法就是使用 JDK 自带的 6  个非常实用的命令行工具来排查。它们分别是：jps、jstat、jinfo、jmap、jhat 和 jstack，它们都位于 JDK 的 bin  目录下，可以使用命令行工具直接运行，其目录如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558535)

接下来我们来看看这些工具的具体使用。

### 1. jps（虚拟机进程状况工具）

jps（JVM Process Status tool，虚拟机进程状况工具）它的功能和 Linux 中的 ps 命令比较类似，用于列出正在运行的 JVM 的  LVMID（Local Virtual Machine IDentifier，本地虚拟机唯一 ID），以及 JVM 的执行主类、JVM  启动参数等信息。语法如下：

```shell
jps [options] [hostid]
```

常用的 options 选项：

```diff
-l：用于输出运行主类的全名，如果是 jar 包，则输出 jar 包的路径；

-q：用于输出 LVMID（Local Virtual Machine Identifier，虚拟机唯一 ID）；

-m：用于输出虚拟机启动时传递给主类 main() 方法的参数；

-v：用于输出启动时的 JVM 参数。
```

使用实例：

**jps -l**

```plaintext
68848
40085 org.jetbrains.jps.cmdline.Launcher
40086 com.example.optimize.NativeOptimize
40109 jdk.jcmd/sun.tools.jps.Jps
68879 org.jetbrains.idea.maven.server.RemoteMavenServer36
```

**jps -q**

```plaintext
40368
68848
40085
40086
68879
```

**jps -m**

```plaintext
68848
40085 Launcher /Applications/IntelliJ IDEA2.app/Contents/lib/idea_rt.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/oro-2.0.8.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/resources_en.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/maven-model-3.6.1.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/qdox-2.0-M10.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/plexus-component-annotations-1.7.1.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/httpcore-4.4.13.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/maven-resolver-api-1.3.3.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/netty-common-4.1.47.Final.jar:/Applications/IntelliJ IDEA2.app/Contents/plugins/java/lib/maven-resolver-connector-basic-1.3.3.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/maven-artifact-3.6.1.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/plexus-utils-3.2.0.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/netty-resolver-4.1.47.Final.jar:/Applications/IntelliJ IDEA2.app/Contents/lib/guava-28.2-
40086 NativeOptimize
68879 RemoteMavenServer36
```

**jps -v**

```plaintext
68848  -Xms128m -Xmx2048m -XX:ReservedCodeCacheSize=240m -XX:+UseCompressedOops -Dfile.encoding=UTF-8 -XX:+UseConcMarkSweepGC -XX:SoftRefLRUPolicyMSPerMB=50 -ea -XX:CICompilerCount=2 -Dsun.io.useCanonPrefixCache=false -Djava.net.preferIPv4Stack=true -Djdk.http.auth.tunneling.disabledSchemes="" -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -Djdk.attach.allowAttachSelf -Dkotlinx.coroutines.debug=off -Djdk.module.illegalAccess.silent=true -Xverify:none -XX:ErrorFile=/Users/admin/java_error_in_idea_%p.log -XX:HeapDumpPath=/Users/admin/java_error_in_idea.hprof -javaagent:/Users/admin/.jetbrains/jetbrains-agent-v3.2.0.de72.619 -Djb.vmOptionsFile=/Users/admin/Library/Application Support/JetBrains/IntelliJIdea2020.1/idea.vmoptions -Didea.paths.selector=IntelliJIdea2020.1 -Didea.executable=idea -Didea.home.path=/Applications/IntelliJ IDEA2.app/Contents -Didea.vendor.name=JetBrains
40085 Launcher -Xmx700m -Djava.awt.headless=true -Djava.endorsed.dirs="" -Djdt.compiler.useSingleThread=true -Dpreload.project.path=/Users/admin/github/blog-example/blog-example -Dpreload.config.path=/Users/admin/Library/Application Support/JetBrains/IntelliJIdea2020.1/options -Dcompile.parallel=false -Drebuild.on.dependency.change=true -Djava.net.preferIPv4Stack=true -Dio.netty.initialSeedUniquifier=1366842080359982660 -Dfile.encoding=UTF-8 -Duser.language=zh -Duser.country=CN -Didea.paths.selector=IntelliJIdea2020.1 -Didea.home.path=/Applications/IntelliJ IDEA2.app/Contents -Didea.config.path=/Users/admin/Library/Application Support/JetBrains/IntelliJIdea2020.1 -Didea.plugins.path=/Users/admin/Library/Application Support/JetBrains/IntelliJIdea2020.1/plugins -Djps.log.dir=/Users/admin/Library/Logs/JetBrains/IntelliJIdea2020.1/build-log -Djps.fallback.jdk.home=/Applications/IntelliJ IDEA2.app/Contents/jbr/Contents/Home -Djps.fallback.jdk.version=11.0.6 -Dio.netty.noUnsafe=true -Djava.io.tmpdir=/Users/admin/Library/Caches/Je
40086 NativeOptimize -Dfile.encoding=UTF-8
40425 Jps -Dapplication.home=/Users/admin/Library/Java/JavaVirtualMachines/openjdk-14/Contents/Home -Xms8m -Djdk.module.main=jdk.jcmd
68879 RemoteMavenServer36 -Djava.awt.headless=true -Dmaven.defaultProjectBuilder.disableGlobalModelCache=true -Xmx768m -Didea.maven.embedder.version=3.6.1 -Dmaven.ext.class.path=/Applications/IntelliJ IDEA2.app/Contents/plugins/maven/lib/maven-event-listener.jar -Dfile.encoding=UTF-8
```

### 2. jstat（虚拟机统计信息监视工具）

jstat（JVM Statistics Monitoring Tool，虚拟机统计信息监视工具）用于监控虚拟机的运行状态信息。

例如，我们用它来查询某个 Java 进程的垃圾收集情况，示例如下：

**jstat -gc 43704**

```plaintext
S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT    CGC    CGCT     GCT
10752.0 10752.0  0.0    0.0   65536.0   5243.4   175104.0     0.0     4480.0 774.0  384.0   75.8       0    0.000   0      0.000   -          -    0.000
```

参数说明如下所示：

- S0C     年轻代中第一个存活区的大小
- S1C     年轻代中第二个存活区的大小
- S0U     年轻代中第一个存活区已使用的空间（字节）
- S1U     年轻代中第二个存活区已使用的空间（字节）
- EC     Eden 区大小
- EU     年轻代中 Eden 区已使用的空间（字节）
- OC     老年代大小
- OU     老年代已使用的空间（字节）
- YGC     从应用程序启动到采样时 young gc 的次数
- YGCT     从应用程序启动到采样时 young gc 的所用的时间（s）
- FGC     从应用程序启动到采样时 full gc 的次数
- FGCT     从应用程序启动到采样时 full gc 的所用的时间
- GCT     从应用程序启动到采样时整个 gc 所用的时间

> 注意：年轻代的 Eden 区满了会触发 young gc，老年代满了会触发 old gc。full gc 指的是清除整个堆，包括 young 区 和 old 区。

jstat 常用的查询参数有：

- -class，查询类加载器信息；
- -compiler，JIT 相关信息；
- -gc，GC 堆状态；
- -gcnew，新生代统计信息；
- -gcutil，GC 堆统计汇总信息。

### 3. jinfo（查询虚拟机参数配置工具）

jinfo（Configuration Info for Java）用于查看和调整虚拟机各项参数。语法如下：

```shell
jinfo <option> <pid>
```

查看 JVM 参数示例如下：

**jinfo -flags 45129**

```plaintext
VM Flags:
-XX:CICompilerCount=3 -XX:InitialHeapSize=268435456 -XX:MaxHeapSize=4294967296 -XX:MaxNewSize=1431306240 -XX:MinHeapDeltaBytes=524288 -XX:NewSize=89128960 -XX:OldSize=179306496 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseFastUnorderedTimeStamps -XX:+UseParallelGC
```

其中 45129 是使用 jps 查询的 LVMID。
我们可以通过 jinfo -flag [+/-]name 来修改虚拟机的参数值，比如下面的示例：

```shell
➜ jinfo -flag PrintGC 45129 # 查询是否开启 GC 打印
➜  jinfo -flag +PrintGC 45129 # 开启 GC 打印
➜  jinfo -flag PrintGC 45129 # 查询是否开启 GC 打印
➜  jinfo -flag -PrintGC 45129 # 关闭 GC 打印
➜  jinfo -flag PrintGC 45129 # 查询是否开启 GC 打印
```

### 4. jmap（堆快照生成工具）

jmap（Memory Map for Java）用于查询堆的快照信息。

查询堆信息示例如下：

**jmap -heap 45129**

```plaintext
Attaching to process ID 45129, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.101-b13
using thread-local object allocation.
Parallel GC with 6 thread(s)
Heap Configuration:
MinHeapFreeRatio         = 0
MaxHeapFreeRatio         = 100
MaxHeapSize              = 4294967296 (4096.0MB)
NewSize                  = 89128960 (85.0MB)
MaxNewSize               = 1431306240 (1365.0MB)
OldSize                  = 179306496 (171.0MB)
NewRatio                 = 2
SurvivorRatio            = 8
MetaspaceSize            = 21807104 (20.796875MB)
CompressedClassSpaceSize = 1073741824 (1024.0MB)
MaxMetaspaceSize         = 17592186044415 MB
G1HeapRegionSize         = 0 (0.0MB)
Heap Usage:
PS Young Generation
Eden Space:
capacity = 67108864 (64.0MB)
used     = 5369232 (5.1204986572265625MB)
free     = 61739632 (58.87950134277344MB)
8.000779151916504% used
From Space:
capacity = 11010048 (10.5MB)
used     = 0 (0.0MB)
free     = 11010048 (10.5MB)
0.0% used
To Space:
capacity = 11010048 (10.5MB)
used     = 0 (0.0MB)
free     = 11010048 (10.5MB)
0.0% used
PS Old Generation
capacity = 179306496 (171.0MB)
used     = 0 (0.0MB)
free     = 179306496 (171.0MB)
0.0% used

2158 interned Strings occupying 152472 bytes.
```

我们也可以直接生成堆快照文件，示例如下：

**jmap -dump:format=b,file=/Users/admin/Documents/2020.dump 47380**

```shell
Dumping heap to /Users/admin/Documents/2020.dump ...
Heap dump file created
```

### 5. jhat（堆快照分析功能）

jhat（JVM Heap Analysis Tool，堆快照分析工具）和 jmap 搭配使用，用于启动一个 web 站点来分析 jmap 生成的快照文件。

执行示例如下：

**jhat /Users/admin/Documents/2020.dump**

```plaintext
Reading from /Users/admin/Documents/2020.dump...
Dump file created Tue May 26 16:12:41 CST 2020
Snapshot read, resolving...
Resolving 17797 objects...
Chasing references, expect 3 dots...
Eliminating duplicate references...
Snapshot resolved.
Started HTTP server on port 7000
Server is ready.
```

上述信息表示 jhat 启动了一个 http 的服务器端口为 7000 的站点来展示信息，此时我们在浏览器中输入：[http://localhost:7000/](https://link.segmentfault.com/?enc=VQjcVyO2J2xiSBQYfM%2Fhsg%3D%3D.%2Bw8E2UAXc06x8Og1MDU%2FaeqdFJEoFMxCyKlMYzYwDCk%3D)，会看到如下图所示的信息：

![img](https://segmentfault.com/img/remote/1460000042558536)

### 6. jstack（查询虚拟机当前的线程快照信息）

jstack（Stack Trace for Java）用于查看当前虚拟机的线程快照，用它可以排查线程的执行状况，例如排查死锁、死循环等问题。

比如，我们先写一段死锁的代码：

```java
public class NativeOptimize {
    private static Object obj1 = new Object();
    private static Object obj2 = new Object();
    public static void main(String[] args) {
        new Thread(new Runnable() {
            @Override
            public void run() {
                synchronized (obj2) {
                    System.out.println(Thread.currentThread().getName() + "锁住 obj2");
                    try {
                        Thread.sleep(1000);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                    synchronized (obj1) {
                        // 执行不到这里
                        System.out.println("1秒钟后，" + Thread.currentThread().getName()
                                + "锁住 obj1");
                    }
                }
            }
        }).start();
        synchronized (obj1) {
            System.out.println(Thread.currentThread().getName() + "锁住 obj1");
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            synchronized (obj2) {
                // 执行不到这里
                System.out.println("1秒钟后，" + Thread.currentThread().getName()
                        + "锁住 obj2");
            }
        }
    }
}
```

以上程序的执行结果如下：
`text
main：锁住 obj1
Thread-0：锁住 obj2
`

此时我们使用 jstack 工具打印一下当前线程的快照信息，结果如下：

**jstack -l 50016**

```plaintext
2020-05-26 18:01:41
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.101-b13 mixed mode):
"Attach Listener" #10 daemon prio=9 os_prio=31 tid=0x00007f8c00840800 nid=0x3c03 waiting on condition [0x0000000000000000]
java.lang.Thread.State: RUNNABLE
Locked ownable synchronizers:
- None
"Thread-0" #9 prio=5 os_prio=31 tid=0x00007f8c00840000 nid=0x3e03 waiting for monitor entry [0x00007000100c8000]
java.lang.Thread.State: BLOCKED (on object monitor)
at com.example.optimize.NativeOptimize$1.run(NativeOptimize.java:25)
- waiting to lock <0x000000076abb62d0> (a java.lang.Object)
- locked <0x000000076abb62e0> (a java.lang.Object)
at java.lang.Thread.run(Thread.java:745)
Locked ownable synchronizers:
- None
"Service Thread" #8 daemon prio=9 os_prio=31 tid=0x00007f8c01814800 nid=0x4103 runnable [0x0000000000000000]
java.lang.Thread.State: RUNNABLE
Locked ownable synchronizers:
- None
"C1 CompilerThread2" #7 daemon prio=9 os_prio=31 tid=0x00007f8c0283c800 nid=0x4303 waiting on condition [0x0000000000000000]
java.lang.Thread.State: RUNNABLE
Locked ownable synchronizers:
- None
"C2 CompilerThread1" #6 daemon prio=9 os_prio=31 tid=0x00007f8c0300a800 nid=0x4403 waiting on condition [0x0000000000000000]
java.lang.Thread.State: RUNNABLE
Locked ownable synchronizers:
- None
"C2 CompilerThread0" #5 daemon prio=9 os_prio=31 tid=0x00007f8c0283c000 nid=0x3603 waiting on condition [0x0000000000000000]
java.lang.Thread.State: RUNNABLE
Locked ownable synchronizers:
- None
"Signal Dispatcher" #4 daemon prio=9 os_prio=31 tid=0x00007f8c0283b000 nid=0x4603 runnable [0x0000000000000000]
java.lang.Thread.State: RUNNABLE
Locked ownable synchronizers:
- None
"Finalizer" #3 daemon prio=8 os_prio=31 tid=0x00007f8c03001000 nid=0x5003 in Object.wait() [0x000070000f8ad000]
java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
- waiting on <0x000000076ab08ee0> (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
- locked <0x000000076ab08ee0> (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
Locked ownable synchronizers:
- None
"Reference Handler" #2 daemon prio=10 os_prio=31 tid=0x00007f8c03000000 nid=0x2f03 in Object.wait() [0x000070000f7aa000]
java.lang.Thread.State: WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
- waiting on <0x000000076ab06b50> (a java.lang.ref.Reference$Lock)
at java.lang.Object.wait(Object.java:502)
at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
- locked <0x000000076ab06b50> (a java.lang.ref.Reference$Lock)
at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)
Locked ownable synchronizers:
- None
"main" #1 prio=5 os_prio=31 tid=0x00007f8c00802800 nid=0x1003 waiting for monitor entry [0x000070000ef92000]
java.lang.Thread.State: BLOCKED (on object monitor)
at com.example.optimize.NativeOptimize.main(NativeOptimize.java:41)
- waiting to lock <0x000000076abb62e0> (a java.lang.Object)
- locked <0x000000076abb62d0> (a java.lang.Object)
Locked ownable synchronizers:
- None
"VM Thread" os_prio=31 tid=0x00007f8c01008800 nid=0x2e03 runnable
"GC task thread#0 (ParallelGC)" os_prio=31 tid=0x00007f8c00803000 nid=0x2007 runnable

"GC task thread#1 (ParallelGC)" os_prio=31 tid=0x00007f8c00006800 nid=0x2403 runnable

"GC task thread#2 (ParallelGC)" os_prio=31 tid=0x00007f8c01800800 nid=0x2303 runnable
"GC task thread#3 (ParallelGC)" os_prio=31 tid=0x00007f8c01801800 nid=0x2a03 runnable
"GC task thread#4 (ParallelGC)" os_prio=31 tid=0x00007f8c01802000 nid=0x5403 runnable
"GC task thread#5 (ParallelGC)" os_prio=31 tid=0x00007f8c01006800 nid=0x2d03 runnable
"VM Periodic Task Thread" os_prio=31 tid=0x00007f8c00010800 nid=0x3803 waiting on condition
JNI global references: 6
Found one Java-level deadlock:
=============================
"Thread-0":
waiting to lock monitor 0x00007f8c000102a8 (object 0x000000076abb62d0, a java.lang.Object),
which is held by "main"
"main":
waiting to lock monitor 0x00007f8c0000ed58 (object 0x000000076abb62e0, a java.lang.Object),
which is held by "Thread-0"

Java stack information for the threads listed above:
===================================================
"Thread-0":
at com.example.optimize.NativeOptimize$1.run(NativeOptimize.java:25)
- waiting to lock <0x000000076abb62d0> (a java.lang.Object)
- locked <0x000000076abb62e0> (a java.lang.Object)
at java.lang.Thread.run(Thread.java:745)
"main":
at com.example.optimize.NativeOptimize.main(NativeOptimize.java:41)
- waiting to lock <0x000000076abb62e0> (a java.lang.Object)
- locked <0x000000076abb62d0> (a java.lang.Object)

Found 1 deadlock.
```

从上述信息可以看出使用 jstack ，可以很方便地排查出代码中出现“deadlock”（死锁）的问题。

## 知识扩展

### 可视化排查工具

JVM 除了上面的 6 个基础命令行工具之外，还有两个重要的视图调试工具，即 JConsole 和 JVisualVM，它们相比于命令行工具使用更方便、操作更简单、结果展现也更直观。

JConsole 和 JVisualVM 都位于 JDK 的 bin 目录下，JConsole（Java Monitoring and Management Console）是最早期的视图调试工具，其启动页面如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558537)

可以看出我们可以用它来连接远程的服务器，或者是直接调试本机，这样就可以在不消耗生产环境的性能下，从本机启动 JConsole 来连接服务器。选择了调试的进程之后，运行界面如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558538)

从上图可以看出，使用 JConsole 可以监控线程、CPU、类、堆以及 VM 的相关信息，同样我们可以通过线程这一页的信息，发现之前我们故意写的死锁问题，如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558539)

可以看到 main（主线程）和 Thread-0 线程处于死锁状态。

JVisualVM 的启动图如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558540)

由上图可知，JVisualVM 既可以调试本地也可以调试远程服务器，当我们选择了相关的进程之后，运行如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558541)

可以看出 JVisualVM 除了包含了 JConsole 的信息之外，还有更多的详细信息，并且更加智能。例如，线程死锁检查的这页内容如下图所示：

![img](https://segmentfault.com/img/remote/1460000042558542)

可以看出 JVisualVM 会直接给你一个死锁的提示，而 JConsole 则需要程序员自己分析。

## JVM 调优

JVM 调优主要是根据实际的硬件配置信息重新设置 JVM 参数来进行调优的，例如，硬件的内存配置很高，但 JVM  因为是默认参数，所以最大内存和初始化堆内存很小，这样就不能更好地利用本地的硬件优势了。因此，需要调整这些参数，让 JVM  在固定的配置下发挥最大的价值。

JVM 常见调优参数包含以下这些：

- -Xmx，设置最大堆内存大小；
- -Xms，设置初始堆内存大小；
- -XX:MaxNewSize，设置新生代的最大内存；
- -XX:MaxTenuringThreshold，设置新生代对象经过一定的次数晋升到老生代；
- -XX:PretrnureSizeThreshold，设置大对象的值，超过这个值的对象会直接进入老生代；
- -XX:NewRatio，设置分代垃圾回收器新生代和老生代内存占比；
- -XX:SurvivorRatio，设置新生代 Eden、Form Survivor、To Survivor 占比。

我们要根据自己的业务场景和硬件配置来设置这些值。例如，当我们的业务场景会有很多大的临时对象产生时，因为这些大对象只有很短的生命周期，因此需要把“-XX:MaxNewSize”的值设置的尽量大一些，否则就会造成大量短生命周期的大对象进入老生代，从而很快消耗掉了老生代的内存，这样就会频繁地触发 full gc，从而影响了业务的正常运行。

## 总结

本文讲了 JVM 排查的 6 个基本命令行工具：jps、jstat、jinfo、jmap、jhat、jstack，以及 2 个视图排查工具：JConsole 和 JVisualVM；同时还讲了 JVM 的常见调优参数，希望本文的内容可以切实的帮助到你。****



-----

# JAVA 线上故障排查完整套路！牛掰！

fredalxin 

[Java架构师之路](javascript:void(0);)

 *2020年05月25日 15:29*

> 作者：fredalxin
>
> 来源：https://fredal.xin/java-error-check

------

线上故障主要会包括 CPU、磁盘、内存以及网络问题，而大多数故障可能会包含不止一个层面的问题，所以进行排查时候尽量四个方面依次排查一遍。同时例如 jstack、jmap 等工具也是不囿于一个方面的问题的，基本上出问题就是 df、free、top 三连，然后依次 jstack、jmap 伺候，具体问题具体分析即可。

# CPU

一般来讲我们首先会排查 CPU 方面的问题。CPU 异常往往还是比较好定位的。原因包括业务逻辑问题(死循环)、频繁 gc 以及上下文切换过多。而最常见的往往是业务逻辑(或者框架逻辑)导致的，可以使用 jstack 来分析对应的堆栈情况。

使用 jstack 分析 CPU 问题

我们先用 ps 命令找到对应进程的 pid(如果你有好几个目标进程，可以先用 top 看一下哪个占用比较高)。

接着用top -H -p pid来找到 CPU 使用率比较高的一些线程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9cURh8AKRDIfFFrJc88aSe1QvuMy3pYtAWe6IPzeexXmbfltDqcsJhw/640?wx_fmt=jpeg&wxfrom=13&tp=wxpic)

然后将占用最高的 pid 转换为 16 进制printf '%x\n' pid得到 nid

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9u29OGiaxibnAl1XEuW2WzzYUBnTpogloiar4ibSDjp8qGoNdw39Q0icNHgg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

接着直接在 jstack 中找到相应的堆栈信息jstack pid |grep 'nid' -C5 –color

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9XAdiaXt9pouPhUGLPMfAwUqF5mSAHOcnzc8NVwvFP3fialoZB7JAuXSQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到我们已经找到了 nid 为 0x42 的堆栈信息，接着只要仔细分析一番即可。

当然更常见的是我们对整个 jstack 文件进行分析，通常我们会比较关注 WAITING 和 TIMED_WAITING 的部分，BLOCKED 就不用说了。我们可以使用命令cat jstack.log | grep "java.lang.Thread.State" | sort -nr | uniq -c来对 jstack 的状态有一个整体的把握，如果 WAITING 之类的特别多，那么多半是有问题啦。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9zpsDC09P5ww0K7GTAYZbhxfc6VfyucR5Lf7TGY2mbfBN14UicSbOPIQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

频繁 gc

当然我们还是会使用 jstack 来分析问题，但有时候我们可以先确定下 gc 是不是太频繁，使用jstat -gc pid 1000命令来对 gc 分代变化情况进行观察，1000 表示采样间隔(ms)，S0C/S1C、S0U/S1U、EC/EU、OC/OU、MC/MU 分别代表两个 Survivor 区、Eden 区、老年代、元数据区的容量和使用量。YGC/YGT、FGC/FGCT、GCT 则代表 YoungGc、FullGc 的耗时和次数以及总耗时。如果看到 gc 比较频繁，再针对 gc 方面做进一步分析，具体可以参考一下 gc 章节的描述。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9MKs9HzKAziao22GmRcTGHArZ0vdmRianvicN58y2sM2Ne3mhZfb3Vg0oA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

上下文切换

针对频繁上下文问题，我们可以使用vmstat命令来进行查看（https://www.cnblogs.com/ggjucheng/archive/2012/01/05/2312625.html）

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9P2IZ2fVgz8z03pyhRm5cIqch7ytg9hmpoO64iapPlUjjiaUZ1PuRJd8Q/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

cs(context switch)一列则代表了上下文切换的次数。

如果我们希望对特定的 pid 进行监控那么可以使用 pidstat -w pid命令，cswch 和 nvcswch 表示自愿及非自愿切换。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9clBLQdNkmNResib4bKMzSjClHpTXHJSaBWia0buP1uoahp4qVcgx2oDg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

# 磁盘

磁盘问题和 CPU 一样是属于比较基础的。首先是磁盘空间方面，我们直接使用df -hl来查看文件系统状态

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs91rAhM7DCqdDZ3TPIj3qicJ9XBZvh5gs6hUqFbmFuG3BjqD3VEgzO5KA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

更多时候，磁盘问题还是性能上的问题。我们可以通过 iostat。iostat -d -k -x来进行分析

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9kicWZHgeMphDonPQtaIpZvgsDtMXxXI6Thxj73jjuthFFYtsNscMlTw/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

最后一列%util可以看到每块磁盘写入的程度，而rrqpm/s以及wrqm/s分别表示读写速度，一般就能帮助定位到具体哪块磁盘出现问题了。

另外我们还需要知道是哪个进程在进行读写，一般来说开发自己心里有数，或者用 iotop 命令来进行定位文件读写的来源。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9UOwicNicwS7ep640BTUJSqvBthicFV3Aqnf5icOHzEzpojGxhTapRrdVug/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

不过这边拿到的是 tid，我们要转换成 pid，可以通过 readlink 来找到 pid。readlink -f /proc/*/task/tid/../..。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9QWIicSI5EmMFHkIlR9yAl1WrPHdiafIOuiaVGiaEt4JuF8b6mcGQXVsmOQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

找到 pid 之后就可以看这个进程具体的读写情况cat /proc/pid/io

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9AHPZScGmI5wr9EfaLHibwO5ovVL4Yiccb6E03KuOSGiaM55vPC2jrCTLA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

我们还可以通过 lsof 命令来确定具体的文件读写情况lsof -p pid

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs977LolSkIQ1coxHc5ibhfpQFiakUeWs0IYMDmib207Vx6aCK8tDyelW1rQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

# **内存**

内存问题排查起来相对比 CPU 麻烦一些，场景也比较多。主要包括 OOM、GC 问题和堆外内存。一般来讲，我们会先用free命令先来检查一发内存的各种情况。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9gR3via3V6ebgfOicYMjKPMQ3iaLjs0icnM97myJbYVibXfiaiaThhzbgHetrA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

堆内内存

内存问题大多还都是堆内内存问题。表象上主要分为 OOM 和 Stack Overflow。

OOM

JMV 中的内存不足，OOM 大致可以分为以下几种：

**Exception in thread "main" java.lang.OutOfMemoryError: unable to create new native thread**

这个意思是没有足够的内存空间给线程分配 Java 栈，基本上还是线程池代码写的有问题，比如说忘记 shutdown，所以说应该首先从代码层面来寻找问题，使用 jstack 或者 jmap。如果一切都正常，JVM 方面可以通过指定Xss来减少单个 thread stack 的大小。另外也可以在系统层面，可以通过修改/etc/security/limits.confnofile 和 nproc 来增大 os 对线程的限制

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9hpBEFNxR6KgLN9R8sI8ZrnichpqPmVlVvsPcH84O4tWTAv2diaoEFMNg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

**Exception in thread "main" java.lang.OutOfMemoryError: Java heap space**

这个意思是堆的内存占用已经达到-Xmx 设置的最大值，应该是最常见的 OOM 错误了。解决思路仍然是先应该在代码中找，怀疑存在内存泄漏，通过 jstack 和 jmap 去定位问题。如果说一切都正常，才需要通过调整Xmx的值来扩大内存。

**Caused by: java.lang.OutOfMemoryError: Meta space**

这个意思是元数据区的内存占用已经达到XX:MaxMetaspaceSize设置的最大值，排查思路和上面的一致，参数方面可以通过XX:MaxPermSize来进行调整(这里就不说 1.8 以前的永久代了)。

Stack Overflow

栈内存溢出，这个大家见到也比较多。

**Exception in thread "main" java.lang.StackOverflowError**

表示线程栈需要的内存大于 Xss 值，同样也是先进行排查，参数方面通过Xss来调整，但调整的太大可能又会引起 OOM。

使用 JMAP 定位代码内存泄漏

上述关于 OOM 和 Stack Overflow 的代码排查方面，我们一般使用 JMAP jmap -dump:format=b,file=filename pid来导出 dump 文件

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9e8gQWnOAvmXjvHOCSHcyicIstYlacBsMW2YcHxeOepTnvXVNfYCp9Pg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

通过 mat(Eclipse Memory Analysis Tools)导入 dump 文件进行分析，内存泄漏问题一般我们直接选 Leak Suspects 即可，mat 给出了内存泄漏的建议。另外也可以选择 Top Consumers 来查看最大对象报告。和线程相关的问题可以选择 thread overview 进行分析。除此之外就是选择 Histogram 类概览来自己慢慢分析，大家可以搜搜 mat 的相关教程。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9oRjPiazyEkL5xF1xnvbIfibnBz5JTnkw1vyuNficNuIpNZvYg5JpXiauFw/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

日常开发中，代码产生内存泄漏是比较常见的事，并且比较隐蔽，需要开发者更加关注细节。比如说每次请求都 new 对象，导致大量重复创建对象；进行文件流操作但未正确关闭；手动不当触发 gc；ByteBuffer 缓存分配不合理等都会造成代码 OOM。

另一方面，我们可以在启动参数中指定-XX:+HeapDumpOnOutOfMemoryError来保存 OOM 时的 dump 文件。

gc 问题和线程

gc 问题除了影响 CPU 也会影响内存，排查思路也是一致的。一般先使用 jstat 来查看分代变化情况，比如 youngGC 或者 fullGC 次数是不是太多呀；EU、OU 等指标增长是不是异常呀等。

线程的话太多而且不被及时 gc 也会引发 oom，大部分就是之前说的unable to create new native thread。除了 jstack 细细分析 dump 文件外，我们一般先会看下总体线程，通过pstreee -p pid |wc -l。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9Qiby29R0TM1vleXDqBVlgok2fvBEbwDrbxSzvibE1W38R52MhcF4XSCg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

或者直接通过查看/proc/pid/task的数量即为线程数量。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9ELzXlh7ibibJgEiakg91XynOmiaKETIpA4YAh2IR0aBW2e9TdczzXCDiauw/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

堆外内存

如果碰到堆外内存溢出，那可真是太不幸了。首先堆外内存溢出表现就是物理常驻内存增长快，报错的话视使用方式都不确定，如果由于使用 Netty 导致的，那错误日志里可能会出现OutOfDirectMemoryError错误，如果直接是 DirectByteBuffer，那会报OutOfMemoryError: Direct buffer memory。

堆外内存溢出往往是和 NIO 的使用相关，一般我们先通过 pmap 来查看下进程占用的内存情况pmap -x pid | sort -rn -k3 | head -30，这段意思是查看对应 pid 倒序前 30 大的内存段。这边可以再一段时间后再跑一次命令看看内存增长情况，或者和正常机器比较可疑的内存段在哪里。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9wPvdqWzauQRPCon769dudGQP6B0zj7ucdX5xBnlLjqrQzukrT3N5Cw/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

我们如果确定有可疑的内存端，需要通过 gdb 来分析gdb --batch --pid {pid} -ex "dump memory filename.dump {内存起始地址} {内存起始地址+内存块大小}"

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9XHyuhia3QJankbxnDNwJ11yKX7NQ6VlicWmIB1miblPHeKNH6b58wJtAg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

获取 dump 文件后可用 heaxdump 进行查看hexdump -C filename | less，不过大多数看到的都是二进制乱码。

NMT 是 Java7U40 引入的 HotSpot 新特性，配合 jcmd 命令我们就可以看到具体内存组成了。需要在启动参数中加入 -XX:NativeMemoryTracking=summary 或者 -XX:NativeMemoryTracking=detail，会有略微性能损耗。

一般对于堆外内存缓慢增长直到爆炸的情况来说，可以先设一个基线jcmd pid VM.native_memory baseline。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs982vnIibTNvCxibgdt8z16Hj1M1VRxxwlgxjtXvn9VEHQMXWx4qic7wvJQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

然后等放一段时间后再去看看内存增长的情况，通过jcmd pid VM.native_memory detail.diff(summary.diff)做一下 summary 或者 detail 级别的 diff。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs93mricZdIwEGYzKKeiacqOeibDDy4bJbwA59kV6ianWQHiaHqDYoq5EL4ricA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9D5UwssOfdZoKY6583NPxPxudbLo9YTIZ9zBVGbeL71ChVw6GSPrsfw/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到 jcmd 分析出来的内存十分详细，包括堆内、线程以及 gc(所以上述其他内存异常其实都可以用 nmt 来分析)，这边堆外内存我们重点关注 Internal 的内存增长，如果增长十分明显的话那就是有问题了。

detail 级别的话还会有具体内存段的增长情况，如下图。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9LUccDnQ3onwXfgibP0icNftOx6U9Q5acw8iaD34l3DMJVYgUBSpSJM0tg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

此外在系统层面，我们还可以使用 strace 命令来监控内存分配 strace -f -e "brk,mmap,munmap" -p pid

这边内存分配信息主要包括了 pid 和内存地址。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9fv1wibbYCBLLIalwWQ442BQmEdagRw0Boob3tmo1Rq1TNb1QQpQxqLQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

不过其实上面那些操作也很难定位到具体的问题点，关键还是要看错误日志栈，找到可疑的对象，搞清楚它的回收机制，然后去分析对应的对象。比如 DirectByteBuffer 分配内存的话，是需要 full GC 或者手动 system.gc 来进行回收的(所以最好不要使用-XX:+DisableExplicitGC)。那么其实我们可以跟踪一下 DirectByteBuffer 对象的内存情况，通过jmap -histo:live pid手动触发 fullGC 来看看堆外内存有没有被回收。如果被回收了，那么大概率是堆外内存本身分配的太小了，通过-XX:MaxDirectMemorySize进行调整。如果没有什么变化，那就要使用 jmap 去分析那些不能被 gc 的对象，以及和 DirectByteBuffer 之间的引用关系了。

# **GC 问题**

堆内内存泄漏总是和 GC 异常相伴。不过 GC 问题不只是和内存问题相关，还有可能引起 CPU 负载、网络问题等系列并发症，只是相对来说和内存联系紧密些，所以我们在此单独总结一下 GC 相关问题。

我们在 CPU 章介绍了使用 jstat 来获取当前 GC 分代变化信息。而更多时候，我们是通过 GC 日志来排查问题的，在启动参数中加上-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps来开启 GC 日志。

常见的 Young GC、Full GC 日志含义在此就不做赘述了。

针对 gc 日志，我们就能大致推断出 youngGC 与 fullGC 是否过于频繁或者耗时过长，从而对症下药。我们下面将对 G1 垃圾收集器来做分析，这边也建议大家使用 G1-XX:+UseG1GC。

**youngGC 过频繁**

youngGC 频繁一般是短周期小对象较多，先考虑是不是 Eden 区/新生代设置的太小了，看能否通过调整-Xmn、-XX:SurvivorRatio 等参数设置来解决问题。如果参数正常，但是 young gc 频率还是太高，就需要使用 Jmap 和 MAT 对 dump 文件进行进一步排查了。

**youngGC 耗时过长**

耗时过长问题就要看 GC 日志里耗时耗在哪一块了。以 G1 日志为例，可以关注 Root Scanning、Object Copy、Ref Proc 等阶段。Ref Proc 耗时长，就要注意引用相关的对象。Root Scanning 耗时长，就要注意线程数、跨代引用。Object Copy 则需要关注对象生存周期。而且耗时分析它需要横向比较，就是和其他项目或者正常时间段的耗时比较。比如说图中的 Root Scanning 和正常时间段比增长较多，那就是起的线程太多了。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs95F2hYyL4XzKclBfIYcA1tZOrXqSXrJLystsoULld4PhyWPrJsaEXzQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

**触发 fullGC**

G1 中更多的还是 mixedGC，但 mixedGC 可以和 youngGC 思路一样去排查。触发 fullGC 了一般都会有问题，G1 会退化使用 Serial 收集器来完成垃圾的清理工作，暂停时长达到秒级别，可以说是半跪了。

fullGC 的原因可能包括以下这些，以及参数调整方面的一些思路：

- 并发阶段失败：在并发标记阶段，MixGC 之前老年代就被填满了，那么这时候 G1 就会放弃标记周期。这种情况，可能就需要增加堆大小，或者调整并发标记线程数-XX:ConcGCThreads。
- 晋升失败：在 GC 的时候没有足够的内存供存活/晋升对象使用，所以触发了 Full GC。这时候可以通过-XX:G1ReservePercent来增加预留内存百分比，减少-XX:InitiatingHeapOccupancyPercent来提前启动标记，-XX:ConcGCThreads来增加标记线程数也是可以的。
- 大对象分配失败：大对象找不到合适的 region 空间进行分配，就会进行 fullGC，这种情况下可以增大内存或者增大-XX:G1HeapRegionSize。
- 程序主动执行 System.gc()：不要随便写就对了。

另外，我们可以在启动参数中配置-XX:HeapDumpPath=/xxx/dump.hprof来 dump fullGC 相关的文件，并通过 jinfo 来进行 gc 前后的 dump

jinfo -flag +HeapDumpBeforeFullGC pid

jinfo -flag +HeapDumpAfterFullGC pid

jinfo -flag +HeapDumpBeforeFullGC pid

jinfo -flag +HeapDumpAfterFullGC pid

这样得到 2 份 dump 文件，对比后主要关注被 gc 掉的问题对象来定位问题。

# 网络

涉及到网络层面的问题一般都比较复杂，场景多，定位难，成为了大多数开发的噩梦，应该是最复杂的了。这里会举一些例子，并从 tcp 层、应用层以及工具的使用等方面进行阐述。

超时

超时错误大部分处在应用层面，所以这块着重理解概念。超时大体可以分为连接超时和读写超时，某些使用连接池的客户端框架还会存在获取连接超时和空闲连接清理超时。

- 读写超时。readTimeout/writeTimeout，有些框架叫做 so_timeout 或者 socketTimeout，均指的是数据读写超时。注意这边的超时大部分是指逻辑上的超时。soa 的超时指的也是读超时。读写超时一般都只针对客户端设置。
- 连接超时。connectionTimeout，客户端通常指与服务端建立连接的最大时间。服务端这边 connectionTimeout 就有些五花八门了，Jetty 中表示空闲连接清理时间，Tomcat 则表示连接维持的最大时间。
- 其他。包括连接获取超时 connectionAcquireTimeout 和空闲连接清理超时 idleConnectionTimeout。多用于使用连接池或队列的客户端或服务端框架。

我们在设置各种超时时间中，需要确认的是尽量保持客户端的超时小于服务端的超时，以保证连接正常结束。

在实际开发中，我们关心最多的应该是接口的读写超时了。

如何设置合理的接口超时是一个问题。如果接口超时设置的过长，那么有可能会过多地占用服务端的 tcp 连接。而如果接口设置的过短，那么接口超时就会非常频繁。

服务端接口明明 rt 降低，但客户端仍然一直超时又是另一个问题。这个问题其实很简单，客户端到服务端的链路包括网络传输、排队以及服务处理等，每一个环节都可能是耗时的原因。

TCP 队列溢出

tcp 队列溢出是个相对底层的错误，它可能会造成超时、rst 等更表层的错误。因此错误也更隐蔽，所以我们单独说一说。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9ueiaWMwEibnlkQFibePsaOaoLJ963hJibxPmWzrHM1SiaN6Yy3CV7NDgwEQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，这里有两个队列：syns queue(半连接队列）、accept queue（全连接队列）。三次握手，在 server 收到 client 的 syn 后，把消息放到 syns queue，回复 syn+ack 给 client，server 收到 client 的 ack，如果这时 accept queue 没满，那就从 syns queue 拿出暂存的信息放入 accept queue 中，否则按 tcp_abort_on_overflow 指示的执行。

tcp_abort_on_overflow 0 表示如果三次握手第三步的时候 accept queue 满了那么 server 扔掉 client 发过来的 ack。tcp_abort_on_overflow 1 则表示第三步的时候如果全连接队列满了，server 发送一个 rst 包给 client，表示废掉这个握手过程和这个连接，意味着日志里可能会有很多connection reset / connection reset by peer。

那么在实际开发中，我们怎么能快速定位到 tcp 队列溢出呢？

**netstat 命令，执行 netstat -s | egrep "listen|LISTEN"**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs95rEegaooMnc2icOmZC1icYTsv9QdJR5QAoaaavN5WDSvSFqpDKvsdwibg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，overflowed 表示全连接队列溢出的次数，sockets dropped 表示半连接队列溢出的次数。

**ss 命令，执行 ss -lnt**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9hORTeGqWHPiblNhIGjQbMq8D30G7N102kzibKDPvBuLicn9ibGOVghZ0FQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

上面看到 Send-Q 表示第三列的 listen 端口上的全连接队列最大为 5，第一列 Recv-Q 为全连接队列当前使用了多少。

接着我们看看怎么设置全连接、半连接队列大小吧：

全连接队列的大小取决于 min(backlog, somaxconn)。backlog 是在 socket 创建的时候传入的，somaxconn 是一个 os 级别的系统参数。而半连接队列的大小取决于 max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog)。

在日常开发中，我们往往使用 servlet 容器作为服务端，所以我们有时候也需要关注容器的连接队列大小。在 Tomcat 中 backlog 叫做acceptCount，在 Jetty 里面则是acceptQueueSize。

RST 异常

RST 包表示连接重置，用于关闭一些无用的连接，通常表示异常关闭，区别于四次挥手。

在实际开发中，我们往往会看到connection reset / connection reset by peer错误，这种情况就是 RST 包导致的。

**端口不存在**

如果像不存在的端口发出建立连接 SYN 请求，那么服务端发现自己并没有这个端口则会直接返回一个 RST 报文，用于中断连接。

**主动代替 FIN 终止连接**

一般来说，正常的连接关闭都是需要通过 FIN 报文实现，然而我们也可以用 RST 报文来代替 FIN，表示直接终止连接。实际开发中，可设置 SO_LINGER 数值来控制，这种往往是故意的，来跳过 TIMED_WAIT，提供交互效率，不闲就慎用。

**客户端或服务端有一边发生了异常，该方向对端发送 RST 以告知关闭连接**

我们上面讲的 tcp 队列溢出发送 RST 包其实也是属于这一种。这种往往是由于某些原因，一方无法再能正常处理请求连接了(比如程序崩了，队列满了)，从而告知另一方关闭连接。

**接收到的 TCP 报文不在已知的 TCP 连接内**

比如，一方机器由于网络实在太差 TCP 报文失踪了，另一方关闭了该连接，然后过了许久收到了之前失踪的 TCP 报文，但由于对应的 TCP 连接已不存在，那么会直接发一个 RST 包以便开启新的连接。

**一方长期未收到另一方的确认报文，在一定时间或重传次数后发出 RST 报文**

这种大多也和网络环境相关了，网络环境差可能会导致更多的 RST 报文。

之前说过 RST 报文多会导致程序报错，在一个已关闭的连接上读操作会报connection reset，而在一个已关闭的连接上写操作则会报connection reset by peer。通常我们可能还会看到broken pipe错误，这是管道层面的错误，表示对已关闭的管道进行读写，往往是在收到 RST，报出connection reset错后继续读写数据报的错，这个在 glibc 源码注释中也有介绍。

我们在排查故障时候怎么确定有 RST 包的存在呢？当然是使用 tcpdump 命令进行抓包，并使用 wireshark 进行简单分析了。tcpdump -i en0 tcp -w xxx.cap，en0 表示监听的网卡。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9COFn2RA5jjh48hSA3fV2XEktBejp2icUX9aOe50ES9KquSs7CSPTNqg/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

接下来我们通过 wireshark 打开抓到的包，可能就能看到如下图所示，红色的就表示 RST 包了。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs92HUlM0egemTGj5w6viaZadhXQicjAaCY5ngzX2VdNUxw33uSV6WLlMaQ/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

TIME_WAIT 和 CLOSE_WAIT

TIME_WAIT 和 CLOSE_WAIT 是啥意思相信大家都知道。

在线上时，我们可以直接用命令netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'来查看 time-wait 和 close_wait 的数量

用 ss 命令会更快ss -ant | awk '{++S[$1]} END {for(a in S) print a, S[a]}'

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/WwPkUCFX4x4q4SxZeO5N1RicXwYTjxYs9MOYdcUgic9TrAcAKZVYoG6pRdoEUoTibuOcIEHpQJYdoXTzqh7Q8XzdA/640?wx_fmt=jpeg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

TIME_WAIT

time_wait 的存在一是为了丢失的数据包被后面连接复用，二是为了在 2MSL 的时间范围内正常关闭连接。它的存在其实会大大减少 RST 包的出现。

过多的 time_wait 在短连接频繁的场景比较容易出现。这种情况可以在服务端做一些内核参数调优:

\#表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭

net.ipv4.tcp_tw_reuse = 1

\#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭

net.ipv4.tcp_tw_recycle = 1

\#表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭

net.ipv4.tcp_tw_reuse = 1

\#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭

net.ipv4.tcp_tw_recycle = 1

当然我们不要忘记在 NAT 环境下因为时间戳错乱导致数据包被拒绝的坑了，另外的办法就是改小tcp_max_tw_buckets，超过这个数的 time_wait 都会被干掉，不过这也会导致报time wait bucket table overflow的错。

CLOSE_WAIT

close_wait 往往都是因为应用程序写的有问题，没有在 ACK 后再次发起 FIN 报文。close_wait 出现的概率甚至比 time_wait 要更高，后果也更严重。往往是由于某个地方阻塞住了，没有正常关闭连接，从而渐渐地消耗完所有的线程。

想要定位这类问题，最好是通过 jstack 来分析线程堆栈来排查问题，具体可参考上述章节。这里仅举一个例子。

开发同学说应用上线后 CLOSE_WAIT 就一直增多，直到挂掉为止，jstack 后找到比较可疑的堆栈是大部分线程都卡在了countdownlatch.await方法，找开发同学了解后得知使用了多线程但是确没有 catch 异常，修改后发现异常仅仅是最简单的升级 sdk 后常出现的class not found。





-----------

# JVM调优实战（一）

原创 码易有道 

[码易有道](javascript:void(0);)

 *2024年02月26日 00:19* *北京*

> 关键词：JVM、OOM、CPU、Full GC

## **背景**

  Java虚拟机（JVM）在Java技术体系中占据着核心地位。JVM调优能够显著`提升应用的响应速度、吞吐量以及资源利用效率`，从而保障系统的稳定高效运行。本章为JVM调优前置知识，主要介绍：什么时候需要调优？JVM如何监控和诊断性能、如何设置参数、常用的JVM参数等。

## **文章导读**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQHwgsiaF3mAibkgicYVEIhLxIGB2nRLlr0QaAfRFVa7xpQKwqaXlk8Pa9Q/640?wx_fmt=png&from=appmsg&wxfrom=13)

## **什么时候需要调优？**

  在进行JVM调优时，需要综合考虑JVM的`内存管理、垃圾回收、线程管理`等方面，通过合理的配置和参数调整，实现最佳的性能和资源利用效果。

通常出现以下情况就需要JVM调优：

- `性能问题`：例如应用程序的响应时间过长、吞吐量低、频繁发生垃圾回收等情况，可能需要进行JVM调优。
- `内存问题`：当应用程序经常发生内存溢出错误或持续占用过多的内存时，表明堆内存配置不合理或垃圾回收策略需要调整。
- `并发问题`：在高并发环境下，如果应用程序出现线程竞争、死锁或阻塞等问题，可以考虑通过调整线程池大小、线程栈大小等参数来改善并发性能。

## **JVM调优监控和诊断**

那么，当生产遇到上述类似的情况时，我们如何发现问题？一般会通过系统监控来诊断问题。通常诊断的工具有两类：一种是jdk自带命令行工具；一种是借助第三方性能分析工具。

### **基础配置**

- 主机参数：4核8G
- Oracle jdk版本：1.8.0_221

### **Java命令行工具**

我们知道，jdk打娘胎出来在`${JAVA_HOME/bin}`下有很多命令行工具。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQ6TlNxdlhGugjbb6UpkUKDXu4Dvz7RBQkzCneR2UicHduV8YtjyGwZXQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

获取命令具体参数选项，可以通过`command -help` 获取。例如：

```
[root@localhost bin]# jps -help
usage: jps [-help]
       jps [-q] [-mlvV] [<hostid>]

Definitions:
    <hostid>:      <hostname>[:<port>]
```

下面我们针对一些常用的命令行工具逐一介绍。

**- jps：查看正在运行的Java进程**

------

通过`jps`命令获取Java进程的进程ID（PID）以及主类名称或JAR文件的完整路径名。

参数选项：

- `-l`：输出主类或者JAR的完全路径名。例如，运行`jps -l`命令将列出所有Java进程及其对应的主类名称或JAR文件的完整路径名。
- `-v`：输出JVM参数。列出每个Java进程的JVM参数信息。
- `-m`：输出JVM启动时传递给main()方法的参数。
- `-V`：（特定环境或版本可能支持）提供特定于该环境或版本的输出或功能。
- `<hostid>`：指定要查询 Java 进程信息的远程主机。

测试用例：

```
[root@localhost ~]# jps -mlvV
10217 demo-jvm-1.0-SNAPSHOT.jar
10285 sun.tools.jps.Jps -mlvV -Dapplication.home=/usr/local/java/jdk1.8.0_221 -Xms8m
```

结果演示：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQ5n5oId4fLYpHDkLNZUeiaKAtKoQYey9hplAjE77K6KI5zzrqw83FFtQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**- jstat：查看JVM统计信息**

------

`jstat` 是 Java 虚拟机（JVM）自带的监控工具，用于查看 HotSpot JVM 的性能统计信息。

```
jstat -<option> [-t] [-h<lines>] <vmid> [<interval> [<count>]]
```

参数选项：

基础参数：

- `-t`：展示从虚拟机运行到现在的性能数据
- `-h<lines>`：每隔lines行展示行头部信息
- `<vmid>`：JVM进程的虚拟机标识符。
- `<interval>`：两次统计信息收集之间的时间间隔（秒）。
- `<count>`：收集统计信息的次数。

性能参数：

- `-class`：显示类加载器的统计信息。
- `-compiler`：显示即时编译器的统计信息。
- `-gc`：显示垃圾收集的统计信息。
- `-gccapacity`：显示各内存池的容量和使用情况。
- `-gccause`：显示上一次或当前垃圾收集的原因及相关统计信息。
- `-gcnew`：显示新生代的垃圾收集统计信息。
- `-gcnewcapacity`：显示新生代内存池的容量和使用情况。
- `-gcold`：显示老年代的垃圾收集统计信息。
- `-gcoldcapacity`：显示老年代内存池的容量和使用情况。
- `-gcpermcapacity`：显示永久代的容量和使用情况（Java 8 及以后版本可能不再适用）。
- `-printcompilation`：输出被即时编译器编译的方法信息。

测试用例：

通过jstat工具监控指定Java进程（进程ID为10217）的垃圾收集（GC）统计信息，并限制输出结果的行数为2行。每1秒收集一次数据，总共收集3次。

```
[root@localhost ~]# jstat -gc -h 2 10217 1s 3
```

结果演示：![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQ53wsY0abDNa6ja0ViaHQbOIibIBFBeicMjAnQrXIwl4LE7CUgYDN4htxQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**- jmap：导出内存映像文件&内存使用情况**

------

`jmap` 用于生成堆内存映射或堆转储文件（heap dump）。这些文件可以用于后续的堆分析，帮助开发者诊断内存泄漏、内存溢出等问题。

```
jmap [option] <pid>
```

其中，`<pid>` 是要分析的 Java 进程的进程 ID。

参数选项：

- `-heap`：打印出堆内存的概要信息，包括各代（新生代、老年代）的使用情况、GC 配置等。
- `-histo`：打印堆内存的直方图，列出每个类的实例数量和总字节大小。帮助识别哪些类占用了大量内存。
- `-dump:<dump-options>`：生成堆转储文件。`<dump-options>` 可以是 `live`（仅转储活动的对象）、`format=b`（二进制格式）、`file=<filename>`（指定转储文件的名称）。

例如，要生成一个名为 `heapdump.hprof` 的堆转储文件，可以使用以下命令：

```
jmap -dump:format=b,file=heapdump.hprof <pid>
```

- `-finalizerinfo`：打印出正在等待 Finalizer 线程执行的对象信息。
- `-clstats`：打印类加载器的统计信息，包括加载的类数量、卸载的类数量等。
- `-printcompilation`：打印出即时编译器编译的代码信息。

> 注意: 生成堆转储文件可能会对正在运行的 JVM 产生性能影响，特别是在堆内存很大的情况下。因此，建议在系统负载较低或处于可接受的范围内时执行此操作。

**- jinfo：实时查看和修改JVM配置参数**

------

`jinfo` 用于实时查看和修改运行中的 Java 进程的 JVM 配置参数。

```
jinfo [option] <pid>
```

参数选项：

- `-flag <name>`     打印指定名称的 VM 标志的值
- `-flag [+|-]<name>`  启用或禁用指定名称的 VM 标志
- `-flag <name>=<value>`将指定名称的 VM 标志设置为给定值
- `-Flags`       打印 VM 标志
- `-sysprops`      打印 Java 系统属性
- `<no option>`     同时打印以上两者
- `-h` 或 `-help`    打印此帮助信息

> **注意**：`jinfo` 在 Java 9 及更高版本中才支持参数动态修改，并且并非所有 VM 标志都支持动态修改。修改某些标志可能需要 JVM 重启才能生效。

测试用例：

例如：查看这个进程的 JVM 标志和系统属性，例如：修改其中一个标志（比如 `-Xmx`，即最大堆内存大小）的值。

- 查看所有 JVM 标志和系统属性

```
jinfo 10217
```

- 查看特定 JVM 标志的值

```
jinfo -flag MaxHeapSize 10217
```

这个命令会输出进程 `10217` 的 `MaxHeapSize`（最大堆内存大小）标志的当前值。

- 修改特定 JVM 标志的值

  请注意，不是所有的 JVM 标志都可以在运行时修改。而且，修改某些标志可能需要 JVM 重启。下面的命令尝试将最大堆内存大小设置为 2GB：

```
jinfo -flag MaxHeapSize=2g 10217
```

- 查看所有 JVM 标志

```
jinfo -Flags 10217
```

- 查看所有 Java 系统属性

```
jinfo -sysprops 10217
```

演示效果：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQbLYQtcT12Y7T5VZHU6CST50yKkY4V30oel0Od5icbOClHX86L3H2lMw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**- jstack：打印JVM中线程快照**

------

`jstack(JVM Stack Trace)`：用于生成虚拟机指定进程当前时刻的线程快照(虚拟机堆栈跟踪)。

线程快照就是当前虚拟机内指定进程的每一条线程正在执行的方法堆栈的集合。可用于定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等问题。就可以用jstack显示各个线程调用的堆栈情况。

```
jstack <pid>
```

参数选项：

- `-F`：强制生成线程堆栈。当 `jstack <pid>` 没有响应（进程挂起）时使用。
- `-m`：如果调用到本地方法的话，可以显示C/C++的堆栈
- `-l`：打印关于锁的额外信息。

测试用例：

```
jstack 10217
```

演示效果：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQZ4qoyrNNsIr6PMa3YsWsQJgscGzq5eC9e0HWWjdK9X419zXOgHAJicQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### **第三方JVM监控及诊断工具**

  使用命令行工具或组合能帮您获取目标Java应用性能相关的基础信息，但它们无法获取方法级别的分析数据，如方法间的调用关系、各方法的调用次数和调用时间等（这对定位应用性能瓶颈至关重要）。而且结果展示不够直观。

[![图片](https://mmbiz.qpic.cn/sz_mmbiz_gif/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQsT0cIZ07icrUiaJqmgfMsPgY468fGicgibBpicbntuAatjZE7YKdAP5D1icw/640?wx_fmt=gif&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)](https://mp.weixin.qq.com/s?__biz=MzIwNTA4NzI1Mw==&mid=2247485033&idx=1&sn=28ec26dcd5bd76c9f30029770cdb6f67&scene=21#wechat_redirect)你可以如此可爱

  为此，JDK提供了一些内存泄漏的分析工具，如`jconsole，jvisualvm`等，用于辅助开发人员定位问题，但是这些工具很多时候并不足以满足快速定位的需求。

> jvisualvm

  为方便测试，我们在window下使用Visual VM [${JAVA_HOME}/bin/jvisualvm.exe] 来演示效果（功能性详细信息，请自行发掘）

比如：我们在程序中写一个最简单OOM的例子。

```
public static void main(String[] args) {
    List<Object> list = new ArrayList<>();
    while (true) {
        list.add(new Object());
    }
}
```

运行一段时间，点击运行jvisualvm.exe，显示内存使用情况

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQpGpM9wra4aQHV6rwKBHQwicuQuRnVwibvk5VdqEv0ojficeUwSE8KJoGw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

后台运行情况：

```
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
```

尽管直观地看到CPU、线程数、对空间的直观变化。但是这些信息有时候并不能满足我们的分析和定位需求。因此，下边介绍2个重常用的第三方工具。

> eclipse MAT

`MAT: MAT(Memory Analyzer Tool)`: 基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗。

官网下载地址：https://eclipse.dev/mat/downloads.php

为演示效果，设置JVM参数：

```
java -jar -Xms8m -Xmx8m -XX:+PrintGC -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./ demo-jvm-1.0-SNAPSHOT.jar 
```

运行一段时间，出现OOM异常,并生成了转存文件：java_pid11469.hprof。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQRj6f7FXkv6uB3D4fmicpkUaShkicIbEvYZiaoPoozOEMVEu6px07ian4UA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

本地用mat工具导入并打开刚刚的转存文件，如下图：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQU5zA3TL6bHtKBEMZA0we25q3MdOdmPmZGRlMoG6iayicUcUk0hNv8NIw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

那么，如何辅助我们发现OOM具体发生在哪里呢？

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQvEOcOfLGA8DZH9JQYXAHrpm1ULStXMUiaI9QicoK0Ep3VyLsj3h38RFQ/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

事实上，这和后台进程显示是一致的。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQFGCibbz8TlTQBxrfUJyIEYsGG5has9ryvAYiaUtaFcbkgEwHx5yRdicGA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从上面的报告中，MAT工具使我们看出有内存泄漏的情况，并能够得知内存泄漏具体发生位置。

> 阿里 Arthas

`Arthas`:Alibaba开源的Java诊断工具。

官方使用教程：https://arthas.aliyun.com/

功能特性：

- 支持在线排查，无需重启
- 动态跟踪Java代码
- 实时监控JVM状态

当你遇到以下类似问题时，Arthas可以帮助你解决（根据学习资料整理）：

1. 这个类从哪个 jar 包加载的？为什么会报各种类相关的 Exception？
2. 修改的代码是否生效？难道没commit？或者分支搞错了？
3. 遇到问题无法在线上 debug，难道只能通过加日志再重新发布吗？
4. 线上遇到某个用户的数据处理有问题，但线上同样无法 debug，线下无法重现！
5. 如何以全局视角来查看系统的运行状况？
6. 有什么办法可以监控到JVM的实时运行状态？
7. 怎么快速定位应用的热点，生成火焰图？

相关诊断指令： https://arthas.aliyun.com/doc/commands.html

用户案例：https://github.com/alibaba/arthas/issues?q=label%3Auser-case

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/VhiaUjy5R5VBCKIAYyO7ViaM0bUliagX9fQ3SibjU8WxHwNrrlbG4Hxhia19xJU7RF3ibJ57J06WlTxqJWns7I7N74gg/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  具体使用和异常处理，请在官方文档提示下学习和测试。也可以再实际生产中遇到相关问题，能快速定位。

## **如何设置JVM调优参数？**

### **JVM参数类型**

1. 标准参数选项

通常用于显示版本信息或帮助文档。例如：

- `-version`：输出JVM的版本信息。

1. -X参数选项

通常用于调试、性能调优或诊断。例如：

- `-Xms<size>`：设置JVM初始堆大小。

1. -XX参数选项

用于调整JVM的内部行为、性能优化、诊断等。

- `-XX:+PrintGCDetails`：输出详细的垃圾回收日志。

### **常用的JVM参数有哪些？**

参考文章：https://articles.zsxq.com/id_3w4u6d0xqhfd.html

**1. 设置堆、栈、方法区等内存大小**

- `-Xmx4g`: 设置进程占用的最大堆空间大小为4GB，超出后会导致OutOfMemoryError。
- `-Xms2g`: 设置初始化堆空间大小为2GB。
- `-Xmn1g`: 设置年轻代大小为1GB，官方推荐配置为整个堆的3/8。
- `-XX:NewRatio=n`: 设置年轻代和老年代空间大小的比值。
- `-Xss512k`: 设置每个线程占用的内存大小为512KB。
- `-XX:SurvivorRatio=n`: 设置年轻代中Eden区与Survivor区的比值，例如n=4时，Eden和Survivor的比值为4:2。
- `-XX:MetaspaceSize=512m`: 设置元空间（Metaspace）的初始大小为512MB。
- `-XX:MaxMetaspaceSize=512m`: 设置元空间（Metaspace）增长的上限，防止无限制地使用本地内存。
- `-XX:MinMetaspaceFreeRatio=N`: 设置Metaspace GC后空闲空间的最小比例，控制Metaspace的增长速度。
- `-XX:MaxMetaspaceFreeRatio=N`: 设置Metaspace GC后空闲空间的最大比例，控制Metaspace的释放。
- `-XX:MaxMetaspaceExpansion=N`: 设置Metaspace增长时的最大幅度。

### **设置垃圾收集器**

- `-XX:+UseSerialGC`: 设置使用串行收集器。
- `-XX:+UseParallelGC`: 设置使用并行收集器。
- `-XX:+UseParalledlOldGC`: 设置使用并行年老代收集器。
- `-XX:+UseConcMarkSweepGC`: 设置使用并发收集器。
- `-XX:ParallelGCThreads=n`: 设置并行收集器使用的线程数。
- `-XX:MaxGCPauseMillis=n`: 设置并行收集的最大暂停时间。
- `-XX:GCTimeRatio=n`: 设置垃圾回收时间占程序运行时间的百分比，1/(1+n)。
- `-XX:+DisableExplicitGC`: 禁止外部调用`System.gc()`。
- `-XX:MaxTenuringThreshold`: 设置年轻代对象复制到老年代前的最大复制次数。

### **垃圾回收信息统计**

- `-XX:+PrintGC`: 打印垃圾回收信息。
- `-XX:+PrintGCDetails`: 打印详细的垃圾回收信息。
- `-XX:+PrintGCTimeStamps`: 打印每次垃圾回收前程序未中断的执行时间。
- `-Xloggc:filename`: 把GC日志存入指定文件。
- `-XX:+PrintGCApplicationStoppedTime`: 打印垃圾回收期间程序暂停的时间。
- `-XX:+PrintGCApplicationConcurrentTime`: 打印每次垃圾回收前程序未中断的执行时间。
- `-XX:+PrintHeapAtGC`: 打印GC前后的详细堆栈信息。
- `-XX:+HeapDumpOnOutOfMemoryError`: 在OutOfMemoryError时生成堆转储。
- `-XX:HeapDumpPath=/dump`: 设置堆转储文件的路径。

### **如何设置JVM运行参数？**

  通常应用一般部署在Linux系统中。设置JVM运行参数通常涉及在启动Java应用程序时通过命令行传递参数。这些参数可以直接附加在`java`命令后面，或者通过环境变量来设置。

#### **命令行设置**

例如，要设置初始堆大小为512MB和最大堆大小为1024MB，可以使用命令行：

```
java -jar -Xms512m -Xmx1024m App.jar
```

其中，`-Xms` 和 `-Xmx` 分别是设置初始堆大小和最大堆大小的参数，`App` 是你的Java应用程序的类名。

#### **环境变量设置**

可以通过设置`JAVA_OPTS`环境变量来做到这一点：

```
export JAVA_OPTS="-Xms512m -Xmx1024m"
java $JAVA_OPTS App
```

或者，如果你使用Tomcat应用服务器，可以在tomcat/bin/catalina.sh启动脚本中设置`CATALINA_OPTS`或相应的环境变量。

#### **参数选择依据**

当选择JVM参数时，可以考虑以下几点：

1. **内存需求**：根据应用程序的需求，设置`-Xms`（初始堆大小）和`-Xmx`（最大堆大小）参数。
2. **垃圾收集器**：使用`-XX:+UseConcMarkSweepGC`、`-XX:+UseParallelGC`、`-XX:+UseG1GC`等参数来选择适合的应用程序的垃圾收集器。
3. **性能调优**：使用`-XX:+PrintGC`、`-XX:+PrintGCDetails`等参数来打印垃圾收集日志，帮助了解应用程序的性能，并进行调优。
4. **诊断**：使用`-XX:+HeapDumpOnOutOfMemoryError`、`-XX:HeapDumpPath=/path/to/dump`等参数来在发生`OutOfMemoryError`时生成堆转储文件，便于后续分析。
5. **线程数**：如果你的应用程序是多线程的，考虑使用`-XX:ParallelGCThreads`和`-XX:ConcGCThreads`来设置并行和并发垃圾收集器的线程数。
6. **预测试**：在生产环境部署之前，模拟业务场景和硬件设备，在测试环境中验证所选参数的效果，确保它们能够满足应用程序的需求。
7. **官方文档和社区**：当然，可以参考Oracle官方文档和Java社区的建议，了解参数的最佳实践和使用场景。

  因此，合适的JVM参数取决于应用程序的特定需求和环境。系统参数的设置也是根据实际情况逐渐调整，而不是一蹴而就的。最佳做法是不断监视应用程序的性能，并根据需要进行调整。

## **总结**

  本文主要对：JVM调优的背景、产生原因、监控和诊断工具、常用JVM参数进行了介绍。限于篇幅，我们将在下一章具体介绍生产环境常见的JVM调优案例。通过实操的方式，结合代码和工具详细介绍如何发现问题、分析问题、解决问题。敬请期待......





----

# HR面


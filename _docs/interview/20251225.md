20251218

17:00～18：00 虾皮 **【Supply Chain】AI高级后端开发工程师**

部门为什么带AI，部门主要做什么？

聊项目，主要聊了UI自动化，托管交付简单聊了下

八股：

## 什么情况下不适合零拷贝

> 零拷贝（Zero-Copy）是一种优化 I/O 性能的技术，通过减少数据在内核空间与用户空间之间的复制次数，以及减少上下文切换，来提升系统吞吐量和降低 CPU 开销。典型应用场景包括文件传输（如 sendfile）、网络数据转发、大数据处理等。
>
> 然而，并非所有场景都适合使用零拷贝。以下是一些**不适合或无法使用零拷贝**的情况：
>
> ------
>
> ### 1. **需要对数据进行修改或处理**
>
> - **原因**：零拷贝的核心思想是“不将数据拷贝到用户空间”，直接在内核中完成数据传输（如从磁盘 → 网卡）。
>
> - **问题**：如果你需要在发送前对数据做加密、压缩、格式转换、内容过滤等操作，就必须把数据读入用户空间进行处理。
>
> - 示例
>
>   ：
>
>   - Web 服务器动态生成 HTML 并插入用户信息；
>   - 视频流服务需要根据客户端分辨率转码；
>   - API 网关需要修改 HTTP 头部或负载。
>
> > ✅ 此时必须使用传统 read/write 或 mmap + write 方式，无法使用 sendfile 或 splice 等零拷贝机制。
>
> ------
>
> ### 2. **协议或业务逻辑需要分段/拼接/重组数据**
>
> - **原因**：零拷贝通常要求数据是连续、完整的（如一个文件或一块缓冲区）。
>
> - **问题**：如果需要将多个小块数据拼接成一个消息（如多个数据库字段组装成 JSON），或从大块中提取部分内容，零拷贝难以支持。
>
> - 示例
>
>   ：
>
>   - 构造包含用户信息 + 订单信息 + 日志的复合响应；
>   - 从日志文件中只提取包含特定关键字的行并发送。
>
> > ✅ 需要用户态逻辑参与，必须拷贝到应用内存处理。
>
> ------
>
> ### 3. **目标设备或协议不支持直接 DMA 或内核级转发**
>
> - **原因**：零拷贝依赖底层硬件（如网卡支持 SG-DMA）和操作系统支持（如 Linux 的 `sendfile`, `splice`, `io_uring`）。
>
> - 问题
>
>   ：
>
>   - 某些虚拟化环境、容器网络（如旧版 Docker bridge）可能不完整支持；
>   - 目标不是文件或 socket（如写入加密存储、自定义设备驱动）；
>   - 使用高级网络协议（如 HTTPS）需 TLS 加解密，必须经过用户空间（除非使用内核 TLS，如 kTLS）。
>
> > ✅ HTTPS 默认不支持零拷贝（因为要加密），但 Linux 5.0+ 支持 **kTLS（Kernel TLS）** 可部分实现零拷贝加密传输。
>
> ------
>
> ### 4. **小文件或低频 I/O 场景**
>
> - **原因**：零拷贝的优势在高吞吐、大块数据传输时才显著。
>
> - **问题**：对于小文件（如 < 4KB）或偶尔读取的场景，设置零拷贝的开销（系统调用、页表锁定等）可能超过收益。
>
> - 示例
>
>   ：
>
>   - 读取配置文件；
>   - 偶尔上传头像图片。
>
> > ✅ 传统 read/write 更简单、开销更低。
>
> ------
>
> ### 5. **跨平台或语言运行时限制**
>
> - **原因**：零拷贝依赖操作系统特定 API（如 Linux 的 `sendfile`，FreeBSD 的 `sendfile`，Windows 的 `TransmitFile`）。
>
> - 问题
>
>   ：
>
>   - Java 的 `FileChannel.transferTo()` 在某些 OS 上会退化为普通拷贝；
>   - Python、Node.js 等高级语言对零拷贝支持有限或需特定库；
>   - 跨平台应用难以统一使用零拷贝。
>
> > ✅ 若追求可移植性，可能放弃零拷贝。
>
> ------
>
> ### 6. **需要精确控制内存生命周期或安全性**
>
> - **原因**：零拷贝可能涉及内核长时间持有用户内存（如 mmap），或绕过常规内存管理。
>
> - 问题
>
>   ：
>
>   - 安全敏感场景（如处理机密数据）希望尽快清空内存；
>   - 实时系统需要确定性的内存释放行为；
>   - 内存受限环境需避免内核缓存占用过多资源。
>
> ------
>
> ### 总结：何时**不适合**零拷贝？
>
> | 不适合场景               | 原因             |
> | ------------------------ | ---------------- |
> | 需要修改/处理数据        | 必须进入用户空间 |
> | 数据需拼接/过滤/动态生成 | 无法直接内核转发 |
> | 使用 HTTPS（无 kTLS）    | 加密需用户态参与 |
> | 小文件或低频 I/O         | 优化收益低       |
> | 跨平台/高级语言环境      | API 支持不一致   |
> | 安全/实时性要求高        | 内存控制受限     |



## mysql有一张用户表，包括用户id，姓名、公司、学校......等等几十个字段，通过id进行水平分表，分了10张表，请设计一个检索系统，比如根据name和学校进行查询的时候，怎么进行进行设计？（要求可以随意根据某个字段查询或者随意根据某几个字段联合查询，如果通过建立MYSQL索引的方式，每个查询都要建立索引，索引的数量太多了，设计系统时考虑这种情况）

> 在 MySQL 中对用户表进行水平分片（按 user_id 分成 10 张表）后，如果要支持任意字段（如 name、school、company 等）或多个字段的组合查询，而这些字段又不是分片键（user_id），那么传统的 MySQL 索引方案会面临以下问题：
>
> - 每张分表都要为各种可能的查询字段建立索引，组合爆炸（例如 name+school、name+company、school+company+city...）导致索引数量剧增；
> - 即使建了索引，跨分片查询仍需遍历所有分表，性能差；
> - 无法高效支持模糊查询、全文检索等。
>
> 因此，**建议引入外部搜索引擎或专用索引系统**来解耦“写入路径”和“查询路径”。以下是推荐的系统设计方案：
>
> ------
>
> ## ✅ 整体架构：MySQL + Elasticsearch（或 Apache Solr）
>
> ### 1. 数据写入流程
>
> - 用户数据写入 MySQL（按 user_id 路由到对应分表）；
> - 同时将**全量用户记录**（包括所有字段）同步到 **Elasticsearch（ES）** 中，ES 中不分片（或按业务逻辑分 shard，但对应用透明）；
> - 同步方式：
>   - 实时：通过 binlog（如 Canal、Debezium）监听 MySQL 变更，推送到 Kafka，再消费写入 ES；
>   - 准实时/批量：定时任务从 MySQL 抽取增量数据更新 ES。
>
> > ES 中每条文档包含 user_id，并可作为唯一 ID，便于后续回查 MySQL 获取完整信息（如敏感字段未存 ES）。
>
> ------
>
> ### 2. 查询流程
>
> - 当用户发起任意字段组合查询（如name LIKE '%张%' AND school = '清华'）：
>   - **直接查询 Elasticsearch**，利用其倒排索引、全文检索、多字段组合过滤能力；
>   - ES 返回匹配的 user_id 列表；
>   - 应用层根据 user_id 路由规则（如 `user_id % 10`）确定具体 MySQL 分表；
>   - 并行从对应 MySQL 分表中拉取完整用户详情（可选，若 ES 已存全量则无需此步）。
>
> ------
>
> ### 3. 优势
>
> | 优势             | 说明                                                 |
> | ---------------- | ---------------------------------------------------- |
> | **灵活查询**     | 支持任意字段、任意组合、模糊、范围、全文搜索         |
> | **避免索引爆炸** | 所有查询由 ES 处理，MySQL 只需维护主键和少量必要索引 |
> | **高性能**       | ES 分布式设计，查询并行化，响应快                    |
> | **扩展性强**     | 后续新增查询字段无需改 MySQL 表结构或重建索引        |
>
> ## ✅ 推荐结论
>
> > **采用 “MySQL（分库分表） + Elasticsearch（统一索引）” 架构** 是解决该问题的最佳实践。
>
> - MySQL 负责事务性写入和强一致性存储；
> - Elasticsearch 负责灵活、高性能的多维查询；
> - 两者通过可靠的数据同步机制保持最终一致。
>
> 这种架构已被大量互联网公司（如电商、社交、招聘平台）验证，能有效应对“任意字段组合查询”的需求，同时规避 MySQL 索引爆炸和跨分片查询性能问题。



## ElasticSearch原理，什么是倒排索引

> Elasticsearch（ES）是一个基于 **Apache Lucene** 构建的分布式、可扩展的全文搜索引擎。其核心能力在于**快速对海量非结构化或半结构化数据进行全文检索、聚合分析和复杂查询**。而这一切的基础，就是 **倒排索引（Inverted Index）**。
>
> ------
>
> ## 一、什么是倒排索引？（Inverted Index）
>
> ### 1. 正排索引（Forward Index） vs 倒排索引（Inverted Index）
>
> | 类型         | 结构                                  | 用途                       |
> | ------------ | ------------------------------------- | -------------------------- |
> | **正排索引** | `文档ID → 文档内容`                   | 存储原始数据（如数据库行） |
> | **倒排索引** | `词项（Term） → 包含该词的文档ID列表` | 支持“根据关键词找文档”     |
>
> ### 2. 举个例子
>
> 假设有两个文档：
>
> - **Doc 1**: "The quick brown fox"
> - **Doc 2**: "The lazy dog"
>
> #### 正排索引：
>
> text
>
> 编辑
>
> 
>
> 
>
> 
>
> ```
> Doc1 → ["the", "quick", "brown", "fox"]
> Doc2 → ["the", "lazy", "dog"]
> ```
>
> #### 倒排索引（简化版）：
>
> text
>
> 编辑
>
> 
>
> 
>
> 
>
> ```
> "brown" → [Doc1]
> "dog"   → [Doc2]
> "fox"   → [Doc1]
> "lazy"  → [Doc2]
> "quick" → [Doc1]
> "the"   → [Doc1, Doc2]
> ```
>
> > ✅ 当用户搜索 `"quick"`，ES 直接查倒排索引，立刻知道结果是 **Doc1**，无需扫描所有文档！
>
> ------
>
> ## 二、倒排索引的组成（Lucene 实现）
>
> 在 Lucene/ES 中，一个完整的倒排索引包含以下关键部分：
>
> ### 1. **词典（Term Dictionary）**
>
> - 所有唯一词项（terms）的有序列表；
> - 支持快速查找（如通过 FST — Finite State Transducer，压缩且高效）。
>
> ### 2. **倒排表（Postings List）**
>
> - 每个词项对应的文档列表，通常包含：
>   - **文档 ID（doc_id）**
>   - **词频（TF, Term Frequency）**：该词在文档中出现次数
>   - **位置（Position）**：词在文档中的偏移位置（支持短语查询）
>   - **偏移量（Offset）**：用于高亮显示
>   - **Payload（可选）**：附加元数据
>
> > 例如：
> >  `"fox"` → [(doc_id=1, tf=1, pos=[3])]
>
> ### 3. **跳表（Skip List）**
>
> - 为加速大倒排表的合并（如 AND/OR 查询），在 postings list 中建立跳表，实现 **O(log n)** 跳跃查找。
>
> ------
>
> ## 三、Elasticsearch 如何构建倒排索引？
>
> 当文档被写入 ES 时，会经历 **分析（Analysis）** 过程：
>
> text
>
> 编辑
>
> 
>
> 
>
> 
>
> ```
> 原始文本 → [Character Filter] → [Tokenizer] → [Token Filter] → 词项（Terms）
> ```
>
> ### 示例：英文文本分析
>
> 输入：`"The Quick Brown Foxes"`
>
> 1. **Character Filter**：去除 HTML 标签等（可选）
>
> 2. Tokenizer
>
>    （如 Standard Tokenizer）：
>
>    - 切分为：`["The", "Quick", "Brown", "Foxes"]`
>
> 3. Token Filters
>
>    ：
>
>    - `Lowercase` → `["the", "quick", "brown", "foxes"]`
>    - `Stemmer`（词干提取）→ `["the", "quick", "brown", "fox"]`
>
> 最终，这些 **标准化后的词项** 被写入倒排索引。
>
> > 🔧 注意：字段是否分词、使用什么分析器，由 **Mapping（映射）** 定义。
> >
> > - `text` 类型：会分词，建倒排索引（用于全文搜索）
> > - `keyword` 类型：不分词，整值存储（用于精确匹配、聚合）
>
> ------
>
> ## 四、倒排索引如何支持复杂查询？
>
> | 查询类型             | 倒排索引如何工作                                       |
> | -------------------- | ------------------------------------------------------ |
> | **Term Query**       | 直接查词项 → 返回 postings list                        |
> | **Match Query**      | 先分析查询词 → 查多个词项 → 合并结果（如 bool should） |
> | **Phrase Query**     | 查词项 + 验证位置是否连续（利用 position 信息）        |
> | **Wildcard / Fuzzy** | 需要遍历词典（性能较低，慎用）                         |
> | **Aggregation**      | 利用 doc_values（列式存储，非倒排索引）                |
>
> > ⚠️ 注意：**聚合、排序、脚本字段** 等操作 **不依赖倒排索引**，而是使用 **doc_values**（一种列式存储结构，在索引时构建）。
>
> ------
>
> ## 五、倒排索引的优缺点
>
> ### ✅ 优点：
>
> - **全文检索极快**：O(1) ~ O(log N) 定位文档；
> - **支持复杂文本查询**：短语、模糊、同义词等；
> - **天然支持相关性评分（TF-IDF / BM25）**。
>
> ### ❌ 缺点：
>
> - **无法高效支持“非”查询（NOT）或全表扫描**；
> - **更新成本高**：Lucene 的索引是**不可变的（immutable）**，更新=删除+新增；
> - **不擅长数值范围查询**（需借助 BKD Tree 等其他结构）。
>
> ------
>
> ## 六、Elasticsearch 的整体架构与倒排索引的关系
>
> - **Index（索引）** ≈ 数据库
> - **Shard（分片）** = 一个独立的 Lucene 索引 → **包含完整的倒排索引**
> - 写入时，文档被路由到某个 shard，由该 shard 构建倒排索引
> - 查询时，协调节点将请求分发到相关 shards，各 shard 用本地倒排索索引搜索，再合并结果
>
> ------
>
> ## 总结
>
> > **倒排索引是 Elasticsearch 实现高速全文检索的基石**。
> >  它将“文档 → 词”转换为“词 → 文档”，使得关键词搜索从 **O(N)** 扫描变为 **O(1)** 查找。
> >  配合分析器、doc_values、BKD Tree 等多种数据结构，ES 能同时支持 **全文搜索、聚合、地理查询、向量检索** 等丰富功能。

## kafka数据存储怎么存储的？

> Kafka 的数据存储设计是其高性能、高吞吐和持久化能力的核心。它**不使用传统数据库或文件系统随机读写方式**，而是基于**顺序追加写（append-only log）+ 分段文件（segmented log）+ 索引机制**来高效管理海量消息。以下是 Kafka 数据存储的详细原理：
>
> ------
>
> ## 一、基本存储单位：Topic → Partition → Log Segment
>
> ### 1. **Topic（主题）**
>
> - 逻辑概念，代表一类消息流（如 `user_events`）。
> - 一个 Topic 被分为多个 **Partition（分区）**，实现并行处理和水平扩展。
>
> ### 2. **Partition（分区）**
>
> - 每个 Partition 是一个**有序、不可变的消息序列**，以 **日志（Log）** 形式存储。
> - 每条消息在 Partition 内有唯一的 **offset（偏移量）**，从 0 开始递增。
> - Partition 是 Kafka 并行度的基本单位，不同 Partition 可分布在不同 Broker 上。
>
> ### 3. **Log Segment（日志分段）**
>
> - 为避免单个文件过大，每个 Partition 的日志被切分为多个 **Segment 文件**。
> - 每个 Segment 包含：
>   - **`.log` 文件**：实际存储消息数据；
>   - **`.index` 文件**：**偏移量索引**（offset → 物理位置）；
>   - **`.timeindex` 文件**：**时间戳索引**（timestamp → offset）；
>   - （可选）**`.snapshot`、`.txnindex`** 等用于事务或快照。
>
> > ✅ 默认每个 Segment 大小为 **1GB**（可通过 `log.segment.bytes` 配置），或按时间滚动（如 7 天）。
>
> ------
>
> ## 二、消息在 `.log` 文件中的存储格式
>
> 每条消息（Record）在 `.log` 文件中以**二进制格式**顺序追加写入，结构如下（Kafka 0.11+ 使用 **Record Batch** 格式）：
>
> ```text
> [Length][Attributes][Timestamp][Offset Delta][Key Length][Key][Value Length][Value][Headers...]
> ```
>
> - **批量写入**：多条消息会被打包成一个 **Record Batch**，减少网络和磁盘 I/O 开销；
> - **压缩支持**：整个 Batch 可以用 Snappy、LZ4、Zstd 等压缩（由 Producer 设置）；
> - **顺序写**：所有写入都是 **append-only**，极大提升磁盘写性能（接近内存速度）。
>
> ------
>
> ## 三、索引机制：快速定位消息
>
> ### 1. **偏移量索引（.index）**
>
> - 稀疏索引（默认每 4KB 数据建一个索引项）；
> - 存储：`offset → position（在 .log 文件中的字节偏移）`；
> - 查询时通过 **二分查找** 定位到大致位置，再顺序扫描少量数据。
>
> ### 2. **时间戳索引（.timeindex）**
>
> - 支持按时间查询（如“获取 2025-12-25 10:00 之后的消息”）；
> - 存储：`timestamp → offset`；
> - 同样是稀疏索引。
>
> > 🔍 举例：消费者请求 offset=1000000 的消息
> >  → Kafka 找到包含该 offset 的 Segment；
> >  → 在 `.index` 中二分查找最接近的 entry（如 offset=999800 → position=500MB）；
> >  → 从 .log 的 500MB 处开始顺序读，直到找到 offset=1000000。
>
> ------
>
> ## 四、数据目录结构示例
>
> 假设 Kafka 数据目录为 `/kafka/data`，一个 Topic `orders` 有 2 个 Partition：
>
> ```bash
> /kafka/data/
> ├── orders-0/
> │   ├── 00000000000000000000.index
> │   ├── 00000000000000000000.log
> │   ├── 00000000000000000000.timeindex
> │   ├── 00000000000000123456.index
> │   ├── 00000000000000123456.log
> │   └── 00000000000000123456.timeindex
> └── orders-1/
>     ├── 00000000000000000000.index
>     ├── 00000000000000000000.log
>     └── ...
> ```
>
> - 文件名中的数字是该 Segment 的**起始 offset**（如 `00000000000000123456.log` 表示从 offset=123456 开始的消息）。
>
> ------
>
> ## 五、数据清理策略（Retention）
>
> Kafka 不永久保存数据，通过以下策略自动删除旧数据：
>
> ### 1. **基于时间**（默认）
>
> - `log.retention.hours=168`（7 天）
> - 删除超过保留时间的 Segment 文件。
>
> ### 2. **基于大小**
>
> - `log.retention.bytes=1073741824`（每个 Partition 最多 1GB）
>
> ### 3. **日志压缩（Log Compaction）**
>
> - 对 key 相同的消息只保留最新 value（适用于状态变更场景，如用户资料更新）；
> - 保留每个 key 的**最新值**，形成“紧凑日志”。
>
> ------
>
> ## 六、为什么 Kafka 存储如此高效？
>
> | 特性                   | 优势                                              |
> | ---------------------- | ------------------------------------------------- |
> | **顺序追加写**         | 避免磁盘随机写，发挥 HDD/SSD 最大吞吐             |
> | **批量 + 压缩**        | 减少 I/O 和网络开销                               |
> | **零拷贝（sendfile）** | 消费时直接从 PageCache 发送到网卡，无需用户态拷贝 |
> | **分段 + 索引**        | 快速定位 + 高效清理                               |
> | **PageCache 利用**     | 读写都走操作系统缓存，避免 JVM GC 压力            |
>
> ------
>
> ## 总结
>
> Kafka 的存储本质是一个**分布式的、分段的、带索引的、可压缩的、仅追加的日志系统**。它牺牲了随机读写能力，换来了极高的写入吞吐和可扩展性，非常适合**高吞吐消息队列、日志收集、事件溯源**等场景。

## rabbitmq数据怎么存储的？

> RabbitMQ 的数据存储机制与 Kafka 有本质区别：**Kafka 是日志型存储，强调高吞吐顺序写；而 RabbitMQ 是消息队列中间件，强调可靠性、灵活路由和消息确认机制**。其存储设计围绕 **队列（Queue）** 展开，支持内存和磁盘两种模式，并针对不同使用场景做了精细优化。
>
> ------
>
> ## 一、RabbitMQ 存储的核心单位：Queue（队列）
>
> - 每个 **Queue** 是消息的最终落脚点（与 Kafka 的 Partition 类似，但语义不同）；
> - 消息从 Exchange 路由到一个或多个 Queue；
> - **Queue 的内容可以存储在内存（RAM）或磁盘（Disk）上**，取决于消息属性和配置。
>
> ------
>
> ## 二、消息存储的两种模式
>
> ### 1. **Transient Messages（瞬时消息）**
>
> - 默认情况下，消息是 **non-persistent（非持久化）** 的；
> - 全部存储在 **内存（Erlang 进程堆）** 中；
> - **Broker 重启后消息丢失**；
> - 性能高，适合临时任务、实时通知等场景。
>
> ### 2. **Persistent Messages（持久化消息）**
>
> - Producer 发送消息时设置 `delivery_mode = 2`；
> - 消息会 **同时写入内存 + 磁盘**；
> - 即使 RabbitMQ 崩溃或重启，消息也不会丢失；
> - **必须配合 durable queue（持久化队列）使用**，否则消息仍会丢失。
>
> > ✅ 只有当 **Queue 是 durable** 且 **Message 是 persistent** 时，消息才能在 Broker 重启后恢复。
>
> ------
>
> ## 三、磁盘存储机制：Message Store（消息存储引擎）
>
> RabbitMQ 使用 **内部自研的存储引擎**（基于 Erlang 实现），从 3.8 版本起默认使用 **“Quorum Queues” 和 “Classic Queues” 两种存储模型**，其中 **Classic Queues** 是传统模型，其磁盘存储结构如下：
>
> ### Classic Queue 的磁盘存储组成：
>
> #### 1. **msg_store_persistent / msg_store_transient**
>
> - 位于 RabbitMQ 数据目录（如 `/var/lib/rabbitmq/mnesia/.../msg_stores/vhosts/...`）；
> - 所有持久化（或瞬时）消息的**实际内容**存储在这里；
> - 消息以 **二进制格式** 写入 `.rdq` 文件（RabbitMQ Data Queue）；
> - 支持 **垃圾回收（GC）**：当消息被消费并确认后，空间可被回收复用。
>
> #### 2. **Queue Index（队列索引）**
>
> - 每个队列有一个独立的索引文件（如 `queues/<queue-id>/index/...`）；
> - 记录：
>   - 消息 ID（引用 msg_store 中的位置）；
>   - 消息状态（ready/unacked）；
>   - 消息顺序（FIFO）；
>   - 是否持久化；
> - 索引也存储在磁盘，保证队列结构可恢复。
>
> > 📁 示例目录结构（简化）：
>
> ```bash
> /var/lib/rabbitmq/mnesia/
> └── rabbit@node/
>     └── msg_stores/
>         └── vhosts/
>             └── <vhost-id>/
>                 ├── msg_store_persistent/
>                 │   ├── 0.rdq
>                 │   └── cleaner.idx
>                 └── queues/
>                     └── <queue-id>/
>                         └── index/
>                             ├── 0.idx
>                             └── journal.jif
> ```
>
> ------
>
> ## 四、内存 vs 磁盘：何时写入磁盘？
>
> RabbitMQ 采用 **惰性刷盘（lazy fsync）** 策略以提升性能：
>
> | 场景                       | 是否写磁盘 | 说明                                   |
> | -------------------------- | ---------- | -------------------------------------- |
> | 非持久化消息               | ❌          | 仅存内存                               |
> | 持久化消息 + durable queue | ✅          | 写入 msg_store 和 queue index          |
> | 启用 `publisher confirms`  | ⚠️          | 消息写入磁盘后才 ack 给生产者          |
> | 启用 `disk_sync_interval`  | ✅          | 定期调用 `fsync` 强制刷盘（默认 1 秒） |
>
> > 💡 注意：即使消息是持久化的，RabbitMQ **不会对每条消息立即 fsync**（除非配置 `flush_after` 或使用镜像队列+quorum），因此极端断电仍可能丢失最近几秒数据。
>
> ------
>
> ## 五、新存储模型：Quorum Queues（推荐用于高可靠场景）
>
> 从 RabbitMQ 3.8 开始引入 **Quorum Queues**（基于 Raft 共识算法）：
>
> - 所有消息**强制持久化**；
> - 数据在多个节点间复制（类似 etcd）；
> - 存储格式更紧凑，使用 **WAL（Write-Ahead Log） + Snapshot**；
> - 目录结构不同，通常位于 `quorum/r/<queue-id>/` 下；
> - **不支持瞬时消息**，所有消息都落盘；
> - 更适合需要强一致性和高可用的场景。
>
> ------
>
> ## 六、性能与可靠性权衡
>
> | 特性         | Classic Queue           | Quorum Queue      |
> | ------------ | ----------------------- | ----------------- |
> | 支持瞬时消息 | ✅                       | ❌                 |
> | 持久化可选   | ✅                       | 强制持久化        |
> | 高吞吐       | ✅（适合大量小消息）     | 中等（Raft 开销） |
> | 强一致性     | ❌（镜像队列为最终一致） | ✅（Raft 保证）    |
> | 自动故障转移 | 需镜像队列              | 内置支持          |
> | 存储效率     | 较低（索引+消息分离）   | 较高（WAL 合并）  |
>
> ------
>
> ## 七、总结：RabbitMQ 如何存储数据？
>
> - **以 Queue 为中心**，每个队列独立管理消息存储；
> - **消息可存内存或磁盘**，持久化需显式声明；
> - **磁盘存储分为消息体（msg_store）和索引（queue index）**；
> - **Classic Queue** 适合灵活、高性能场景；
> - **Quorum Queue** 适合高可靠、强一致场景；
> - **不追求极致吞吐**，而是平衡 **可靠性、灵活性、运维复杂度**。
>
> > 🔔 与 Kafka 对比：
> >
> > - Kafka：**日志即存储**，一切为高吞吐顺序读写优化；
> > - RabbitMQ：**队列即存储**，一切为消息可靠投递和灵活路由优化。

## redis的持久化方式？怎么选择？

> Redis 提供了两种主要的持久化机制：**RDB（快照）** 和 **AOF（追加文件）**，以及从 Redis 4.0 开始支持的 **混合持久化（RDB + AOF）**。它们各有优缺点，适用于不同场景。合理选择对数据安全、性能和恢复速度至关重要。
>
> ------
>
> ## 一、RDB（Redis Database Backup）—— 快照持久化
>
> ### ✅ 原理：
>
> - 在指定时间点将内存中的数据**全量快照**保存到磁盘，生成一个 `dump.rdb` 文件。
> - 通过 `fork()` 子进程完成，主进程继续处理请求（写时复制 COW 保证一致性）。
>
> ### ⚙️ 配置示例（redis.conf）：
>
> ```
> save 900 1      # 900秒内至少1次修改
> save 300 10     # 300秒内至少10次修改
> save 60 10000   # 60秒内至少10000次修改
> dbfilename dump.rdb
> dir ./data
> ```
>
> ### ✅ 优点：
>
> - **文件紧凑**：单个二进制文件，适合备份、灾难恢复；
> - **恢复速度快**：启动时直接加载 RDB 文件；
> - **对性能影响小**：子进程负责 I/O，主进程几乎无阻塞（除非内存很大导致 fork 慢）。
>
> ### ❌ 缺点：
>
> - **可能丢失数据**：两次快照之间的写入会丢失（如每5分钟一次快照，最多丢5分钟数据）；
> - **大数据集 fork 耗时**：内存越大，fork 子进程越慢（可能阻塞主线程几百毫秒）。
>
> ------
>
> ## 二、AOF（Append-Only File）—— 日志持久化
>
> ### ✅ 原理：
>
> - 记录**每个写命令**到日志文件（如 `appendonly.aof`）；
> - Redis 重启时**重放（replay）** 这些命令重建数据。
>
> ### ⚙️ 同步策略（fsync 策略）：
>
> | 配置                   | 说明                    | 安全性                  | 性能                    |
> | ---------------------- | ----------------------- | ----------------------- | ----------------------- |
> | `appendfsync always`   | 每条命令都 fsync 到磁盘 | ✅ 最高（几乎不丢数据）  | ❌ 最差（磁盘 I/O 瓶颈） |
> | `appendfsync everysec` | 每秒 fsync 一次         | ✅ 高（最多丢1秒数据）   | ✅ 推荐（平衡）          |
> | `appendfsync no`       | 由操作系统决定刷盘      | ❌ 低（可能丢数秒~分钟） | ✅ 最高                  |
>
> ### ✅ 优点：
>
> - **数据更安全**：可配置为最多丢失 1 秒数据；
> - **日志可读**：AOF 是文本格式，可人工修复（如误删 key 可编辑删除 DEL 命令）。
>
> ### ❌ 缺点：
>
> - **文件体积大**：记录所有操作，可能远大于实际数据；
> - **恢复速度慢**：需重放大量命令；
> - **需要重写（rewrite）**：通过 `BGREWRITEAOF` 合并冗余命令，生成精简版 AOF（仍需 fork）。
>
> ------
>
> ## 三、混合持久化（Redis 4.0+）—— RDB + AOF 结合
>
> ### ✅ 原理：
>
> - AOF 文件不再纯命令日志，而是：
>   - **前半部分是 RDB 格式的全量快照**；
>   - **后半部分是增量的 AOF 命令**。
> - 重启时先加载 RDB 部分（快），再重放少量 AOF 命令（准）。
>
> ### ⚙️ 开启方式：
>
> ```
> aof-use-rdb-preamble yes  # 默认在 Redis 7.0+ 已启用
> ```
>
> ### ✅ 优势：
>
> - **兼顾速度与安全性**：恢复快 + 数据丢失少；
> - **AOF 文件更小**：避免纯 AOF 的冗长历史。
>
> > 💡 这是目前**最推荐的生产配置**。
>
> ------
>
> ## 四、如何选择？—— 场景化建议
>
> | 场景                                     | 推荐方案                          | 理由                                      |
> | ---------------------------------------- | --------------------------------- | ----------------------------------------- |
> | **缓存、可丢失数据**                     | 仅 RDB 或关闭持久化               | 性能优先，数据可重建                      |
> | **重要业务数据（如用户会话、订单状态）** | **AOF + everysec + 混合持久化**   | 平衡安全与性能                            |
> | **极高安全性要求（金融、支付）**         | AOF + `always`（谨慎！）          | 几乎不丢数据，但性能差；通常配合主从+哨兵 |
> | **快速恢复 + 备份需求**                  | RDB（定期备份） + AOF（实时保护） | RDB 用于冷备，AOF 用于热恢复              |
> | **开发/测试环境**                        | 关闭持久化 或 仅 RDB              | 简化配置，提升速度                        |
>
> ------
>
> ## 五、其他注意事项
>
> 1. **不要只依赖持久化做备份**
>     → 定期将 RDB/AOF 文件**复制到远程服务器**（如 S3、NAS）。
> 2. **监控 fork 时间**
>     → 内存 > 20GB 时，fork 可能导致毫秒级延迟，考虑使用 **Redis Cluster 分片降低单实例内存**。
> 3. **AOF 重写期间仍有风险**
>     → 重写时新命令会写入 **AOF 重写缓冲区**，若此时宕机且重写失败，可能不一致（极罕见）。
> 4. **云服务（如 AWS ElastiCache、阿里云 Redis）**
>     → 通常已优化持久化策略，建议使用其默认配置 + 自动备份。
>
> ------
>
> ## ✅ 最佳实践总结（生产环境）
>
> ```conf
> # 启用 AOF
> appendonly yes
> appendfilename "appendonly.aof"
> appendfsync everysec
> 
> # 启用混合持久化（Redis 4.0+）
> aof-use-rdb-preamble yes
> 
> # RDB 作为辅助备份（可选）
> save 3600 1
> save 300 100
> stop-writes-on-bgsave-error yes
> 
> # 设置合理的 dir
> dir /var/lib/redis
> ```
>
> > 🔄 同时配合 **主从复制 + 哨兵（Sentinel）或 Redis Cluster** 实现高可用，**持久化 ≠ 高可用**！

## redis持久化是先写日志还是先写文件

> # Redis 持久化：先写日志（AOF）还是先写内存（核心数据）详解
>
> 你想明确 Redis 持久化过程中 “写日志” 与 “写数据” 的顺序，核心结论先明确：Redis 始终是**先写内存（更新核心数据），后写持久化日志 / 文件**（无论 RDB 还是 AOF 持久化方式），不存在 “先写日志再写数据” 的情况，两种持久化方式的写入顺序细节如下：
>
> ## 一、AOF 持久化：先更内存 → 再写缓冲区 → 最后刷磁盘
>
> AOF 是 “日志式持久化”，其写入顺序严格遵循 “内存优先”，避免阻塞主线程，具体流程分 3 步：
>
> ### 1. 第一步：更新内存数据（核心操作，同步执行）
>
> 当 Redis 接收写命令（如`SET key value`）后，**首先在主线程中同步更新内存中的键值对数据**，完成后立即准备向客户端返回 “执行成功” 响应（非最终持久化成功）。
>
> - 这一步是同步的，确保客户端获取的是最新的内存数据状态；
> - 若此步骤失败，命令直接返回失败，不会进入后续日志流程。
>
> ### 2. 第二步：追加命令到 AOF 缓冲区（异步缓冲，非阻塞）
>
> 内存更新成功后，Redis 不会直接将命令写入磁盘 AOF 文件，而是**将该写命令以 RESP 协议格式追加到内存中的 “AOF 缓冲区”**（本质是一块内存缓存区域）。
>
> - 这一步是轻量级操作，仅内存内数据拷贝，无磁盘 I/O 开销，不会阻塞主线程；
> - 缓冲区的作用是聚合批量命令，减少频繁磁盘 I/O 带来的性能抖动。
>
> ### 3. 第三步：按策略刷盘到 AOF 文件（异步 / 同步，可选）
>
> AOF 缓冲区中的命令会根据`appendfsync`配置的刷盘策略，**异步或同步写入磁盘 AOF 文件**，这是持久化的最终步骤：
>
> - `appendfsync everysec`（默认）：由后台线程每秒执行一次刷盘，主线程不阻塞，最多丢失 1 秒数据；
> - `appendfsync always`：每次写命令都同步刷盘，阻塞主线程直到磁盘写入完成，数据零丢失，但性能开销大；
> - `appendfsync no`：由操作系统负责刷盘时机（默认 30 秒），主线程无阻塞，性能最优，数据丢失风险最高。
>
> ### AOF 写入顺序总结
>
> ```plaintext
> 接收写命令 → 同步更新内存数据 → 追加命令到AOF缓冲区 → 按策略异步/同步刷盘到AOF文件 → 返回客户端成功
> ```
>
> ## 二、RDB 持久化：先更内存 → 后异步生成快照文件
>
> RDB 是 “快照式持久化”，同样遵循 “先内存后文件” 的顺序，且快照生成完全不阻塞主线程，具体流程分 2 类场景：
>
> ### 1. 手动触发（BGSAVE 推荐）
>
> ```plaintext
> 接收写命令 → 同步更新内存数据 → 执行BGSAVE命令 → 创建子进程 → 子进程遍历内存生成RDB快照文件 → 快照写入完成后更新RDB文件标识 → 主线程全程不阻塞
> ```
>
> - 主线程仅负责创建子进程（轻量级操作），快照生成和文件写入由子进程完成，不影响主线程的读写操作；
> - 快照生成期间，主线程的新写操作会通过 “写时复制（COW）” 机制保留旧数据副本，确保 RDB 文件是某一时刻的完整内存快照。
>
> ### 2. 自动触发 / 手动 SAVE
>
> - 自动触发（如满足`save 900 1`规则）：流程与 BGSAVE 一致，由后台自动创建子进程生成 RDB 文件，不阻塞主线程；
> - 手动 SAVE（不推荐）：`SAVE`命令会让主线程直接遍历内存生成 RDB 文件，期间阻塞所有读写操作，仅适用于调试场景，生产环境禁止使用。
>
> ### RDB 写入顺序核心特点
>
> - 内存更新是同步优先的，RDB 文件仅是内存数据的 “事后快照”，而非实时同步；
> - 快照生成与主线程解耦，不会影响 Redis 的正常读写性能。
>
> ## 三、混合持久化（RDB+AOF）：继承 “先内存后文件” 的核心逻辑
>
> 混合持久化基于 AOF 重写实现，顺序与 AOF 一致，仅在 AOF 重写时增加 RDB 快照写入：
>
> ```plaintext
> 接收写命令 → 同步更新内存数据 → 追加命令到AOF缓冲区 → 按策略刷盘（增量AOF） → 触发AOF重写 → 创建子进程 → 子进程先生成RDB快照（写入新AOF文件头部） → 子进程追加重写期间的增量命令（写入新AOF文件尾部） → 替换旧AOF文件
> ```
>
> - 核心顺序不变：内存更新优先，文件写入（RDB 快照 + AOF 增量）异步进行；
> - 仅在 AOF 重写时整合 RDB 快照，本质是对 AOF 的优化，不改变 “先内存后文件” 的原则。
>
> ## 四、为什么 Redis 选择 “先内存后文件”？
>
> 1. **性能优先**：内存操作是纳秒级，磁盘 I/O 是毫秒级，先更新内存可快速响应客户端，避免磁盘 I/O 阻塞主线程，保障 Redis 的高并发性能；
> 2. **数据一致性保障**：先更新内存，再异步写持久化文件，确保内存数据始终是最新的，客户端读取的是实时状态；若先写文件再更内存，文件写入失败会导致内存与文件数据不一致；
> 3. **容错性更强**：持久化文件写入失败（如磁盘满），不会影响内存中的正常业务数据，仅丢失 “未持久化的增量数据”，可通过后续重试或主从同步恢复；若先写文件失败，会导致命令执行失败，影响业务可用性。
>
> ## 总结
>
> 1. Redis 所有持久化方式（RDB、AOF、混合持久化）均遵循**先更新内存数据，后写入持久化日志 / 文件**的顺序，不存在 “先写日志再写内存” 的情况；
> 2. AOF 是 “内存→AOF 缓冲区→磁盘文件”，通过刷盘策略平衡性能与可靠性；
> 3. RDB 是 “内存→子进程→RDB 快照文件”，快照生成不阻塞主线程；
> 4. 该顺序的核心目的是保障 Redis 的高并发性能，同时兼顾数据一致性与容错性。

## java一个进程中有两个子线程，其中一个线程OOM，另外一个线程会怎么样

>在 Java 中，**一个线程发生 OutOfMemoryError（OOM）通常不会直接终止其他线程**，但**整个 JVM 进程很可能变得不可用甚至崩溃**，具体行为取决于 OOM 的类型和系统资源状态。以下是详细分析：
>
>------
>
>### 一、Java 线程与内存模型基础
>
>- Java 中所有线程共享 **堆内存（Heap）** 和 **方法区（Metaspace/PermGen）**；
>- 每个线程有自己独立的 **栈（Stack）**；
>- `OutOfMemoryError` 是 `Error` 的子类，表示 JVM 无法分配所需内存。
>
>------
>
>### 二、不同类型的 OOM 对其他线程的影响
>
>#### ✅ 场景 1：**堆内存 OOM（`java.lang.OutOfMemoryError: Java heap space`）**
>
>- **原因**：某个线程不断创建对象（如死循环 new 对象），耗尽堆内存。
>
>- 对其他线程的影响
>
>  ：
>
>  - 抛出 OOM 的线程会**停止执行**（除非捕获了 Error 并处理，但不推荐）；
>  - **其他线程仍可运行**，**但几乎无法再分配新对象**；
>  - 后续任何尝试分配内存的操作（包括 GC）都可能立即抛出 OOM；
>  - **JVM 处于“瘫痪”状态**：虽然进程没退出，但应用基本不可用。
>
>> 📌 示例：
>>
>> ```java
>> // 线程A：疯狂分配内存
>> new Thread(() -> {
>>     List<byte[]> list = new ArrayList<>();
>>     while (true) list.add(new byte[1024 * 1024]); // OOM
>> }).start();
>> 
>> // 线程B：看似正常，但很快也会失败
>> new Thread(() -> {
>>     try { Thread.sleep(1000); } catch (InterruptedException e) {}
>>     System.out.println("B: " + new Date()); // 可能因 new Date() 分配内存而 OOM
>> }).start();
>> ```
>
>#### ✅ 场景 2：**Metaspace OOM（`java.lang.OutOfMemoryError: Metaspace`）**
>
>- **原因**：动态生成大量类（如反射、CGLib、Groovy 脚本）；
>- **影响**：所有需要加载新类的线程都会失败，但已有类的执行可能继续；
>- 其他线程若不涉及类加载，**可能暂时正常**，但系统整体不稳定。
>
>#### ✅ 场景 3：**栈溢出（`StackOverflowError`）或线程栈 OOM（`Unable to create native thread`）**
>
>- **`StackOverflowError`**：仅影响当前线程，其他线程不受影响；
>- **`Unable to create new native thread`**：表示 OS 无法创建新线程（线程数达上限），**不影响已有线程**，但无法启动新线程。
>
>------
>
>### 三、关键结论
>
>| 问题                           | 回答                                                         |
>| ------------------------------ | ------------------------------------------------------------ |
>| **另一个线程会被杀死吗？**     | ❌ 不会直接被杀死；JVM 不会因为一个线程 OOM 而 kill 其他线程。 |
>| **另一个线程还能正常工作吗？** | ⚠️ **理论上可以，实际上很难**。因为堆内存已耗尽，任何对象分配都会失败。 |
>| **JVM 会退出吗？**             | ❌ 默认不会自动退出（除非所有非守护线程结束）；但应用已“假死”。 |
>| **能捕获 OOM 并恢复吗？**      | ⚠️ 技术上可以 `try-catch (OutOfMemoryError)`，但**强烈不推荐**——此时 JVM 状态不可预测，GC 可能失效，继续运行风险极高。 |
>
>------
>
>### 四、生产环境建议
>
>1. **不要捕获 `OutOfMemoryError`**
>    → 应让 JVM 崩溃，触发监控告警和自动重启。
>
>2. **配置 JVM 崩溃时生成 Heap Dump**
>
>   ```
>   -XX:+HeapDumpOnOutOfMemoryError
>   -XX:HeapDumpPath=/logs/heap.hprof
>   ```
>
>3. **使用容器或进程管理工具（如 systemd、K8s）自动重启**
>    → 避免“僵尸进程”长期占用资源。
>
>4. **监控堆内存使用率**
>    → 在 OOM 发生前预警（如 Prometheus + Grafana）。
>
>------
>
>### 五、附加说明：守护线程 vs 非守护线程
>
>- 如果 OOM 发生后，**所有非守护线程都已终止**，JVM 会自动退出；
>- 但如果还有非守护线程存活（即使卡住），JVM 会一直运行。
>
>------
>
>### ✅ 总结
>
>> **一个线程 OOM 不会直接杀死其他线程，但由于内存是全局共享资源，整个 JVM 进程将陷入不可用状态。其他线程即使未 crash，也无法正常执行涉及内存分配的操作。因此，在实践中，一个线程 OOM ≈ 整个应用崩溃。**
>
>最佳策略：**快速失败 + 自动恢复**，而不是试图在 OOM 后继续运行。

## 算法题

### 1、k个一组反转年表；让换了一道

### 2、滑动窗口最大值；

